{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph\n",
    "# %pip install langchain-huggingface\n",
    "# %pip install -qU langchain-qdrant\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun  9 18:59:27 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-SXM2-16GB           On  |   00000000:18:00.0 Off |                    0 |\n",
      "| N/A   39C    P0             43W /  300W |       4MiB /  16384MiB |      0%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2-16GB           On  |   00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   37C    P0             44W /  300W |       0MiB /  16384MiB |      0%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2-16GB           On  |   00000000:86:00.0 Off |                    0 |\n",
      "| N/A   36C    P0             43W /  300W |       0MiB /  16384MiB |      0%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2-16GB           On  |   00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   41C    P0             44W /  300W |       0MiB /  16384MiB |      0%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projectnb/vkolagrp/avarshn/.conda/envs/rag_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import io\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# from colpali_engine.models import ColQwen2, ColQwen2Processor\n",
    "# from transformers.utils.import_utils import is_flash_attn_2_available\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=1000,  # chunk size (characters)\n",
    "#     chunk_overlap=200,  # chunk overlap (characters)\n",
    "#     add_start_index=True,  # track index in original document\n",
    "# )\n",
    "# # all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     # Set a really small chunk size, just to show.\n",
    "#     chunk_size=100,\n",
    "#     chunk_overlap=20,\n",
    "#     length_function=len,\n",
    "#     is_separator_regex=False,\n",
    "# )\n",
    "# # texts = text_splitter.create_documents([state_of_the_union])\n",
    "\n",
    "\n",
    "# print(texts[0])\n",
    "# print(texts[1])\n",
    "# print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "#     is_separator_regex=False,\n",
    "#     add_start_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='123456789123456789gj'),\n",
       " Document(metadata={}, page_content='23456789gjdfddsasjgh'),\n",
       " Document(metadata={}, page_content='dfddsasjghj34567'),\n",
       " Document(metadata={}, page_content='Kitty'),\n",
       " Document(metadata={}, page_content='cat')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.create_documents([\"123456789123456789gjdfddsasjghj34567 Kitty \\n\\ncat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'file1.txt', 'start_index': 0}, page_content='1256 7 Kitty \\n\\ncat'),\n",
       " Document(metadata={'source': 'file2.txt', 'start_index': 0}, page_content='Hi H ow are you doing?')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metas = [{\"source\": \"file1.txt\"}, {\"source\": \"file2.txt\"}]\n",
    "all_splits = text_splitter.create_documents([\"1256 7 Kitty \\n\\ncat\", \"Hi H ow are you doing?\"], metadatas = metas)\n",
    "all_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dictionary update sequence element #0 has length 1; 2 is required",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m all_splits:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m split.metadata:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         \u001b[43msplit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msource\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m         \u001b[38;5;28;01mdel\u001b[39;00m split.metadata[\u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: dictionary update sequence element #0 has length 1; 2 is required"
     ]
    }
   ],
   "source": [
    "for split in all_splits:\n",
    "    if \"source\" in split.metadata:\n",
    "        split.metadata.update(split.metadata[\"source\"])\n",
    "        del split.metadata[\"source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='This is a demo'),\n",
       " Document(metadata={}, page_content='is a demo sentence.'),\n",
       " Document(metadata={}, page_content='sentence. Here is'),\n",
       " Document(metadata={}, page_content='Here is another.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text_splitter.create_documents([\"This is a demo sentence. Here is another.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Meow'),\n",
       " Document(metadata={}, page_content='Moneowhjgjdfdfifgfg'),\n",
       " Document(metadata={}, page_content='jdfdfifgfgfgfgdfdfdf'),\n",
       " Document(metadata={}, page_content='fgfgdfdfdfhgasasasas'),\n",
       " Document(metadata={}, page_content='hgasasasasasjghj'),\n",
       " Document(metadata={}, page_content='Kitty cat')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter.create_documents([\"Meow Moneowhjgjdfdfifgfgfgfgdfdfdfhgasasasasasjghj Kitty cat\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Model - For generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# if not os.environ.get(\"PPLX_API_KEY\"):\n",
    "#   os.environ[\"PPLX_API_KEY\"] = getpass.getpass(\"Enter API key for Perplexity: \")\n",
    "\n",
    "# from langchain.chat_models import init_chat_model\n",
    "\n",
    "# llm = init_chat_model(\"llama-3.1-sonar-small-128k-online\", model_provider=\"perplexity\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_checkpoint = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name = model_checkpoint)\n",
    "\n",
    "# If there is some way to get from embedding model object itself \n",
    "embedding_dim = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name='sentence-transformers/all-mpnet-base-v2' cache_folder=None model_kwargs={} encode_kwargs={} query_encode_kwargs={} multi_process=False show_progress=False\n"
     ]
    }
   ],
   "source": [
    "print(embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Store / Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_qdrant import QdrantVectorStore\n",
    "# from qdrant_client import QdrantClient\n",
    "\n",
    "# client = QdrantClient(\":memory:\")\n",
    "# vector_store = QdrantVectorStore(\n",
    "#     client=client,\n",
    "#     collection_name=\"test\",\n",
    "#     embedding=embeddings,\n",
    "# )\n",
    "\n",
    "# ValueError: Collection test not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from qdrant_client import QdrantClient\n",
    "# from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "# # from langchain.embeddings import FakeEmbeddings  # or your own embedding model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Step 2: Create collection manually\n",
    "# client.recreate_collection(\n",
    "#     collection_name=\"test\",\n",
    "#     vectors_config=VectorParams(size=embedding_dim, distance=Distance.COSINE),\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "# # Initialize your client\n",
    "# client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "# Step 1: Create Qdrant client in memory\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "# Step 2: Define collection name and configuration\n",
    "collection_name = \"test\"\n",
    "vector_config = VectorParams(size=embedding_dim, distance=Distance.COSINE)\n",
    "\n",
    "# Check if collection exists, create it if not\n",
    "if not client.collection_exists(collection_name):\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=vector_config\n",
    "    )\n",
    "\n",
    "# Step 3: Use it with LangChain\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=client,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       doc_name          domain passage_id                      image_path  \\\n",
      "0  2310.05634v2  Academic paper          0  page_images/2310.05634v2_0.jpg   \n",
      "1  2310.05634v2  Academic paper          1  page_images/2310.05634v2_1.jpg   \n",
      "2  2310.05634v2  Academic paper          2  page_images/2310.05634v2_2.jpg   \n",
      "3  2310.05634v2  Academic paper          3  page_images/2310.05634v2_3.jpg   \n",
      "4  2310.05634v2  Academic paper          4  page_images/2310.05634v2_4.jpg   \n",
      "\n",
      "                                        image_binary  \\\n",
      "0  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
      "1  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
      "2  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
      "3  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
      "4  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
      "\n",
      "                                            ocr_text  \\\n",
      "0  Towards Verifiable Generation: A Benchmark for...   \n",
      "1  for attribution? Many real-world applications ...   \n",
      "2  be partially verified by the knowledge graph G...   \n",
      "3  evance on general and specific settings are 73...   \n",
      "4  Model Output: Mininum\\nSentence’ [k1][k2]. Kno...   \n",
      "\n",
      "                                            vlm_text  \n",
      "0  Towards Verifiable Generation: A Benchmark for...  \n",
      "1  \\nTo address the first challenge, we utilize k...  \n",
      "2  \\n2.2 Dataset Construction \\nEach entry of dat...  \n",
      "3  \\nIn practice, it is difficult to define a pre...  \n",
      "4  The image shows a diagram explaining how to ev...  \n"
     ]
    }
   ],
   "source": [
    "# Read a Parquet file\n",
    "ds = pd.read_parquet(\"../data_task1/MMDocIR_pages.parquet\")\n",
    "\n",
    "# Show first few rows\n",
    "print(ds.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open Parquet file\n",
    "# parquet_file = pq.ParquetFile(\"../data_task1/MMDocIR_pages.parquet\")\n",
    "\n",
    "# # Check number of row groups\n",
    "# print(\"Row Groups:\", parquet_file.num_row_groups)\n",
    "\n",
    "# # Iterate over row groups\n",
    "# for i in range(parquet_file.num_row_groups):\n",
    "#     batch = parquet_file.read_row_group(i)\n",
    "#     df_chunk = batch.to_pandas()\n",
    "#     print(df_chunk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataset, doc_name_filter):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: Parquet dataset as Pandas.DataFrame object\n",
    "            doc_name_filter: Only process text with this `doc_name`\n",
    "        \"\"\"\n",
    "        filtered_dataset = dataset[dataset[\"doc_name\"] == doc_name_filter]\n",
    "        self.data = filtered_dataset.reset_index(drop = True)\n",
    "        self.doc_name = doc_name_filter\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        passage_id = self.data.iloc[idx][\"passage_id\"]\n",
    "        text = self.data.iloc[idx][\"vlm_text\"]     # \"ocr_text\"\n",
    "        return passage_id, text\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Contains all the pages of this doc_id : {self.doc_name}\"\n",
    "\n",
    "\n",
    "def text_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to ensure DataLoader returns:\n",
    "    - A list of passage_ids (strings)\n",
    "    - A list of OCR text (either from OCR or VLM-based models)\n",
    "    \"\"\"\n",
    "    passage_ids, texts = zip(*batch)\n",
    "\n",
    "    [{\"page_source\": \"file1.txt\"}, {\"source\": \"file2.txt\"}]\n",
    "\n",
    "    return list(passage_ids), list(texts)\n",
    "\n",
    "def process_document_texts(ds, doc_name):\n",
    "    \"\"\"\n",
    "    Process texts only from the document that matches `doc_name`.\n",
    "    \"\"\"\n",
    "    text_dataset = TextDataset(ds, doc_name)\n",
    "    text_dataloader = DataLoader(\n",
    "        text_dataset, \n",
    "        batch_size=4, \n",
    "        shuffle=False,\n",
    "        collate_fn=text_collate_fn\n",
    "    )\n",
    "\n",
    "    for passage_ids, texts in tqdm(text_dataloader, desc=\"Processing document texts\"):\n",
    "\n",
    "        metas = [{\"page_source\" : passage_id, \"doc_source\" : doc_name} for passage_id in passage_ids]\n",
    "\n",
    "        all_splits = text_splitter.create_documents(texts, metadatas = metas)\n",
    "\n",
    "        document_ids = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_splits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mall_splits\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'all_splits' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_relevant_passage_topk(query_text, embedding_store, passage_store, top_k=5):\n",
    "#     \"\"\"\n",
    "#     Retrieves the top-k most relevant passages for a given query.\n",
    "    \n",
    "#     Args:\n",
    "#         query_text (str): Query `question`\n",
    "#         top_k (int): Number of top passages to retrieve.\n",
    "    \n",
    "#     Returns:\n",
    "#         list: List of passage IDs for the top-k matches.\n",
    "#     \"\"\"\n",
    "\n",
    "#     batch_queries = processor.process_queries([query_text]).to(model.device)\n",
    "#     with torch.no_grad():\n",
    "#         query_embeddings = model(**batch_queries)\n",
    "#     scores = processor.score_multi_vector(query_embeddings, embedding_store)\n",
    "\n",
    "#     min_k = min(top_k, scores[0].numel())      # To avoid out of range errors if documents has less than top_k pages\n",
    "#     top_k_scores, top_k_indices = scores[0].topk(min_k)   \n",
    "#     top_k_passages = [passage_store[idx] for idx in top_k_indices.tolist()]\n",
    "\n",
    "#     while len(top_k_passages) < top_k:\n",
    "#         top_k_passages.append(top_k_passages[-1])\n",
    "\n",
    "#     return top_k_passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Questions: 20\n"
     ]
    }
   ],
   "source": [
    "script_testing = False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--start_idx\", type=int, required=True, help=\"Start index for dataset slicing.\")\n",
    "    parser.add_argument(\"--end_idx\", type=int, required=True, help=\"End index for dataset slicing.\")\n",
    "    parser.add_argument(\"--model_checkpoint\", type = str, required = True, help = \"Hugging Face model checkpoints\")\n",
    "    if not script_testing:\n",
    "        test_args = [\"--start_idx\", \"0\", \"--end_idx\", \"20\", \"--model_checkpoint\", \"all-mpnet-base-v2\"]\n",
    "        args = parser.parse_args(test_args)\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "\n",
    "    # Load dataset - Read a Parquet file\n",
    "    ds = pd.read_parquet(\"../data_task1/MMDocIR_pages.parquet\")\n",
    "\n",
    "\n",
    "    # Use this for querying purpose\n",
    "    # Process JSONL data and update passage_id field with relevant passages\n",
    "    data_json = []\n",
    "    with open(\"../data_task1/MMDocIR_gt_remove.jsonl\", 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data_json.append(json.loads(line.strip()))\n",
    "\n",
    "    data_json = data_json[args.start_idx:args.end_idx]\n",
    "\n",
    "    print(f\"Total Questions: {len(data_json)}\")\n",
    "\n",
    "    data_df = pd.DataFrame(data_json)\n",
    "\n",
    "    doc_list = list(data_df[\"doc_name\"].unique())\n",
    "\n",
    "    res_passage_ids = []     # page numbers stating with zero index\n",
    "    res_question_ids = []\n",
    "    res_questions = []\n",
    "\n",
    "    # Text- Splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,  # chunk size (characters)\n",
    "        chunk_overlap=200,  # chunk overlap (characters)\n",
    "        add_start_index=True,  # track index in original document\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset = TextDataset(ds, doc_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contains all the pages of this doc_id : 2310.05634v2\n"
     ]
    }
   ],
   "source": [
    "print(text_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doc_name                                             2310.05634v2\n",
       "domain                                             Academic paper\n",
       "passage_id                                                      0\n",
       "image_path                         page_images/2310.05634v2_0.jpg\n",
       "image_binary    b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...\n",
       "ocr_text        Towards Verifiable Generation: A Benchmark for...\n",
       "vlm_text        Towards Verifiable Generation: A Benchmark for...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset.data.loc[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To view the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='000344aa775c480698f942573587e221' payload={'page_content': '5.4 Human Evaluation \\nWe conduct human evaluation to verify the correla- tion between automatic evaluation and human judg- ment. We randomly sample 100 sentence-citation pairs from each of the three baselines: ChatGPT (temperature 0.5), LLaMA-7B, and Vicuna-13B. We request two proficient English annotators for each baseline to determine if the citation aligns to the sentence and provides support for it. The reason we choose metric alignment here is in appendix  C , with instruction to annotators and IAA. \\nThe comparison between automatically calcu- lated Alignment and human evaluation results is shown in Table  6 . For all three baselines, the auto- matic and human scores are close with a gap within 2.5, despite the significant differences among the baselines. This indicates a strong correlation be- tween the automatically calculated alignment and human judgments. The experiment results demon- strate that the automatic evaluation serves as a reli-', 'metadata': {'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 4136}} vector=None shard_key=None order_value=None\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "points, _ = client.scroll(\n",
    "    collection_name=\"test\",\n",
    "    scroll_filter=None,\n",
    "    limit=200,\n",
    "    with_vectors=False  # must be True to get vectors\n",
    ")\n",
    "\n",
    "documents = [\n",
    "    Document(page_content=point.payload.get(\"page_content\", \"\"), metadata=point.payload)\n",
    "    for point in points\n",
    "]\n",
    "\n",
    "print(points[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Record(id='000344aa775c480698f942573587e221', payload={'page_content': '5.4 Human Evaluation \\nWe conduct human evaluation to verify the correla- tion between automatic evaluation and human judg- ment. We randomly sample 100 sentence-citation pairs from each of the three baselines: ChatGPT (temperature 0.5), LLaMA-7B, and Vicuna-13B. We request two proficient English annotators for each baseline to determine if the citation aligns to the sentence and provides support for it. The reason we choose metric alignment here is in appendix  C , with instruction to annotators and IAA. \\nThe comparison between automatically calcu- lated Alignment and human evaluation results is shown in Table  6 . For all three baselines, the auto- matic and human scores are close with a gap within 2.5, despite the significant differences among the baselines. This indicates a strong correlation be- tween the automatically calculated alignment and human judgments. The experiment results demon- strate that the automatic evaluation serves as a reli-', 'metadata': {'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 4136}}, vector=None, shard_key=None, order_value=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Doc Name: 2310.05634v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing document texts: 100%|██████████| 6/6 [00:00<00:00,  7.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_id                                                 10000\n",
      "question        In figure 1, which relation arrows do not poin...\n",
      "doc_name                                             2310.05634v2\n",
      "domain                                             Academic paper\n",
      "num_of_pages                                                   24\n",
      "passage_id                                                     []\n",
      "Name: 0, dtype: object\n",
      "question_id                                                 10001\n",
      "question        In figure 5, what is the color of the line tha...\n",
      "doc_name                                             2310.05634v2\n",
      "domain                                             Academic paper\n",
      "num_of_pages                                                   24\n",
      "passage_id                                                     []\n",
      "Name: 1, dtype: object\n",
      "question_id                                         10002\n",
      "question        How many tables include \"F1\" as a metric?\n",
      "doc_name                                     2310.05634v2\n",
      "domain                                     Academic paper\n",
      "num_of_pages                                           24\n",
      "passage_id                                             []\n",
      "Name: 2, dtype: object\n",
      "question_id                                                 10003\n",
      "question        From the paper, which temperature gives ChatGP...\n",
      "doc_name                                             2310.05634v2\n",
      "domain                                             Academic paper\n",
      "num_of_pages                                                   24\n",
      "passage_id                                                     []\n",
      "Name: 3, dtype: object\n",
      "question_id                                                 10004\n",
      "question        For dataset construction, which step takes the...\n",
      "doc_name                                             2310.05634v2\n",
      "domain                                             Academic paper\n",
      "num_of_pages                                                   24\n",
      "passage_id                                                     []\n",
      "Name: 4, dtype: object\n",
      "question_id                                                 10005\n",
      "question        According to author's definition on conscious ...\n",
      "doc_name                                             2310.05634v2\n",
      "domain                                             Academic paper\n",
      "num_of_pages                                                   24\n",
      "passage_id                                                     []\n",
      "Name: 5, dtype: object\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Doc Name: 2401.18059v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing document texts: 100%|██████████| 6/6 [00:00<00:00,  7.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_id                                                 10006\n",
      "question        In figure 4, which nodes are retrieved by RAPT...\n",
      "doc_name                                             2401.18059v1\n",
      "domain                                             Academic paper\n",
      "num_of_pages                                                   23\n",
      "passage_id                                                     []\n",
      "Name: 6, dtype: object\n",
      "question_id                                                 10007\n",
      "question        What are the horizontal and vertical axis of F...\n",
      "doc_name                                             2401.18059v1\n",
      "domain                                             Academic paper\n",
      "num_of_pages                                                   23\n",
      "passage_id                                                     []\n",
      "Name: 7, dtype: object\n",
      "question_id                                                 10008\n",
      "question        Based on \"PSEUDOCODE FOR RETRIEVAL METHODS\", w...\n",
      "doc_name                                             2401.18059v1\n",
      "domain                                             Academic paper\n",
      "num_of_pages                                                   23\n",
      "passage_id                                                     []\n",
      "Name: 8, dtype: object\n",
      "question_id                                                 10009\n",
      "question        In Figure 1's demonstration, what are the colo...\n",
      "doc_name                                             2401.18059v1\n",
      "domain                                             Academic paper\n",
      "num_of_pages                                                   23\n",
      "passage_id                                                     []\n",
      "Name: 9, dtype: object\n",
      "question_id                                                 10010\n",
      "question        What model is the clustering algorithm of this...\n",
      "doc_name                                             2401.18059v1\n",
      "domain                                             Academic paper\n",
      "num_of_pages                                                   23\n",
      "passage_id                                                     []\n",
      "Name: 10, dtype: object\n",
      "question_id                                                 10011\n",
      "question        Write down the pseudo code from appendix that ...\n",
      "doc_name                                             2401.18059v1\n",
      "domain                                             Academic paper\n",
      "num_of_pages                                                   23\n",
      "passage_id                                                     []\n",
      "Name: 11, dtype: object\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Doc Name: 2312.10997v5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing document texts: 100%|██████████| 6/6 [00:00<00:00,  7.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_id                                                 10012\n",
      "question        In the figure that has a tree shape, what is t...\n",
      "doc_name                                             2312.10997v5\n",
      "domain                                             Academic paper\n",
      "num_of_pages                                                   21\n",
      "passage_id                                                     []\n",
      "Name: 12, dtype: object\n",
      "question_id                                              10013\n",
      "question        In figure 3, how many distinct icons are used?\n",
      "doc_name                                          2312.10997v5\n",
      "domain                                          Academic paper\n",
      "num_of_pages                                                21\n",
      "passage_id                                                  []\n",
      "Name: 13, dtype: object\n",
      "question_id                                                 10014\n",
      "question        What is the paper's full title that proposes t...\n",
      "doc_name                                             2312.10997v5\n",
      "domain                                             Academic paper\n",
      "num_of_pages                                                   21\n",
      "passage_id                                                     []\n",
      "Name: 14, dtype: object\n",
      "question_id                                                 10015\n",
      "question        According to table II, which are the datasets ...\n",
      "doc_name                                             2312.10997v5\n",
      "domain                                             Academic paper\n",
      "num_of_pages                                                   21\n",
      "passage_id                                                     []\n",
      "Name: 15, dtype: object\n",
      "question_id                                                 10016\n",
      "question        Which subsection does the section \"AUGMENTATIO...\n",
      "doc_name                                             2312.10997v5\n",
      "domain                                             Academic paper\n",
      "num_of_pages                                                   21\n",
      "passage_id                                                     []\n",
      "Name: 16, dtype: object\n",
      "question_id                                                 10017\n",
      "question        Which method integrates knowledge into white-b...\n",
      "doc_name                                             2312.10997v5\n",
      "domain                                             Academic paper\n",
      "num_of_pages                                                   21\n",
      "passage_id                                                     []\n",
      "Name: 17, dtype: object\n",
      "*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-\n",
      "Doc Name: fd76bbefe469561966e5387aa709c482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing document texts: 100%|██████████| 5/5 [00:00<00:00, 18.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_id                                10018\n",
      "question           What degree does LEBOUR have?\n",
      "doc_name        fd76bbefe469561966e5387aa709c482\n",
      "domain                            Academic paper\n",
      "num_of_pages                                  17\n",
      "passage_id                                    []\n",
      "Name: 18, dtype: object\n",
      "question_id                                         10019\n",
      "question        What is the title of the of the Figure 2?\n",
      "doc_name                 fd76bbefe469561966e5387aa709c482\n",
      "domain                                     Academic paper\n",
      "num_of_pages                                           17\n",
      "passage_id                                             []\n",
      "Name: 19, dtype: object\n",
      "Saved dataset with passage_ids\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for doc_name in doc_list:\n",
    "    print(\"*-\"*20)\n",
    "    print(f\"Doc Name: {doc_name}\")\n",
    "\n",
    "    # Indexing Phase\n",
    "    process_document_texts(ds, doc_name)\n",
    "\n",
    "    # Querying Phase\n",
    "    filtered_df = data_df[data_df[\"doc_name\"] == doc_name]\n",
    "    \n",
    "    top_k = 15\n",
    "    for row_idx, row_data in filtered_df.iterrows():\n",
    "\n",
    "        top_k_matches = vector_store.similarity_search(\n",
    "            query = row_data[\"question\"],\n",
    "            k=top_k,\n",
    "            # Filter on metadata key \"doc_source\"\n",
    "            filter=models.Filter(\n",
    "                must=[\n",
    "                    models.FieldCondition(\n",
    "                        key=\"metadata.doc_source\",\n",
    "                        match=models.MatchValue(value=doc_name)\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(row_data)\n",
    "\n",
    "        while len(top_k_matches) < top_k:\n",
    "            top_k_matches.append(top_k_matches[-1])\n",
    "\n",
    "        res_passage_ids.append(top_k_matches)\n",
    "\n",
    "        res_question_ids.append(row_data['question_id'])\n",
    "        res_questions.append(row_data['question'])\n",
    "\n",
    "\n",
    "# Save the results\n",
    "\n",
    "res_df = pd.DataFrame({\"question_id\": res_question_ids, \"question\" : res_questions, \"passage_id\": res_passage_ids })\n",
    "\n",
    "result_dir = f\"../results_task1/{model_checkpoint}/\"\n",
    "\n",
    "os.makedirs(result_dir, exist_ok = True)\n",
    "\n",
    "res_df.to_csv(f\"{result_dir}/task1_{args.start_idx}.csv\", index = False)\n",
    "print(\"Saved dataset with passage_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page_source': '2', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 703, '_id': '7ae63d8c27dc471f8b08c7ef87a5ffea', '_collection_name': 'test'}, page_content='The image is a diagram displaying \"Breccia Gashes,\" which are formations within a cliff. The top part of these gashes is shown as being eroded or \"denuded,\" while the lower portion is visible within the cliff face. \"Breccia\" refers to a rock composed of broken fragments of minerals or rock cemented together by a fine-grained matrix, which is depicted in the diagram as a pattern of angular fragments, representing the texture of the breccia within these formations.\\nDia g rum of Breccia Gash in Cli fr with top denuded of fund Bottom concealed by the beach. \\nThe image is a line drawing labeled \"FiG. 3,\" depicting a geological cross-section with features that resemble vertical geological formations or structures. These structures could represent elements like veins or intrusions within layered rock formations. The drawing uses different textures and lines to distinguish between various layers and features, suggesting a focus on geological processes or rock formations.'),\n",
       " Document(metadata={'page_source': '2', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 703, '_id': 'ea756a38564642c2800dfc4958134130', '_collection_name': 'test'}, page_content='The image is a diagram displaying \"Breccia Gashes,\" which are formations within a cliff. The top part of these gashes is shown as being eroded or \"denuded,\" while the lower portion is visible within the cliff face. \"Breccia\" refers to a rock composed of broken fragments of minerals or rock cemented together by a fine-grained matrix, which is depicted in the diagram as a pattern of angular fragments, representing the texture of the breccia within these formations.\\nDia g rum of Breccia Gash in Cli fr with top denuded of fund Bottom concealed by the beach. \\nThe image is a line drawing labeled \"FiG. 3,\" depicting a geological cross-section with features that resemble vertical geological formations or structures. These structures could represent elements like veins or intrusions within layered rock formations. The drawing uses different textures and lines to distinguish between various layers and features, suggesting a focus on geological processes or rock formations.'),\n",
       " Document(metadata={'page_source': '2', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 1681, '_id': 'a540bf910ba94ca19ea25e0234a929be', '_collection_name': 'test'}, page_content='The image appears to be a geological diagram illustrating a specific formation process. It depicts layers of sedimentary rock with a visible deformation feature, which resembles a fault or crack filled with fragmented material, indicative of a fracture or a fault zone in the rock layers. This may represent the formation process of a particular geological structure, potentially related to a gas formation, although the text accompanying the image contains typographical errors and is difficult to interpret accurately.\\nN.B. In.the above sketches the Cementing matter is represented by diagonal shading.'),\n",
       " Document(metadata={'page_source': '2', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 1681, '_id': 'ad284ff5cb594a51915816b2271111d2', '_collection_name': 'test'}, page_content='The image appears to be a geological diagram illustrating a specific formation process. It depicts layers of sedimentary rock with a visible deformation feature, which resembles a fault or crack filled with fragmented material, indicative of a fracture or a fault zone in the rock layers. This may represent the formation process of a particular geological structure, potentially related to a gas formation, although the text accompanying the image contains typographical errors and is difficult to interpret accurately.\\nN.B. In.the above sketches the Cementing matter is represented by diagonal shading.'),\n",
       " Document(metadata={'page_source': '2', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 0, '_id': 'afd8cc791c2c498f963bbc9314a5dea7', '_collection_name': 'test'}, page_content='To illustrate Prt. Lcbour\\'s paper \"On the Breccia Gushes of Durham? \\nThe image appears to be a drawing or illustration depicting a stone wall with a section in the middle showing rubble or irregularly shaped stones. The caption text mentions \"FiG. I,\" indicating that this might be Figure 1 in a larger set of illustrations. The words \"evil & cypress\" are written at the top left corner of the image, but without additional context, their relevance to the image is unclear. The style suggests it may be from an older publication on construction, archaeology, or geology, possibly illustrating a type of masonry or wall construction method involving rough-hewn stones used for the inner layer of a wall.'),\n",
       " Document(metadata={'page_source': '2', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 0, '_id': '6193213492714233aafe28cbd3e1db64', '_collection_name': 'test'}, page_content='To illustrate Prt. Lcbour\\'s paper \"On the Breccia Gushes of Durham? \\nThe image appears to be a drawing or illustration depicting a stone wall with a section in the middle showing rubble or irregularly shaped stones. The caption text mentions \"FiG. I,\" indicating that this might be Figure 1 in a larger set of illustrations. The words \"evil & cypress\" are written at the top left corner of the image, but without additional context, their relevance to the image is unclear. The style suggests it may be from an older publication on construction, archaeology, or geology, possibly illustrating a type of masonry or wall construction method involving rough-hewn stones used for the inner layer of a wall.'),\n",
       " Document(metadata={'page_source': '4', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 0, '_id': '58d9b81171cb4bc19933d23c18e9f779', '_collection_name': 'test'}, page_content='The image appears to be a sketch or drawing depicting a textured, rocky landscape or a cliffside. It uses a monochromatic palette with detailed shading to create depth and texture.'),\n",
       " Document(metadata={'page_source': '4', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 0, '_id': 'fffb2f0c7c0648aeb80bab75221d44dc', '_collection_name': 'test'}, page_content='The image appears to be a sketch or drawing depicting a textured, rocky landscape or a cliffside. It uses a monochromatic palette with detailed shading to create depth and texture.'),\n",
       " Document(metadata={'page_source': '0', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 969, '_id': '33362d23e72948f5aafc5a675879ec4f', '_collection_name': 'test'}, page_content='in structure, and so striking by reason of the variety of their forms, that they have been described in many raluable papers, by the late Professor Sedgwick, Mr. R. Howse, Mr. J. W. Kirkby, and others.\\\\* It is not intended in the present paper to repeat what has been so well and so often·said before, but simply to draw special atten- tion to one of the strangest and most striking of the developments of the Magnesian Iimestone as displayed in Marsden Bay.'),\n",
       " Document(metadata={'page_source': '0', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 969, '_id': '29830190cfc74c9eaf68be00844f4755', '_collection_name': 'test'}, page_content='in structure, and so striking by reason of the variety of their forms, that they have been described in many raluable papers, by the late Professor Sedgwick, Mr. R. Howse, Mr. J. W. Kirkby, and others.\\\\* It is not intended in the present paper to repeat what has been so well and so often·said before, but simply to draw special atten- tion to one of the strangest and most striking of the developments of the Magnesian Iimestone as displayed in Marsden Bay.'),\n",
       " Document(metadata={'page_source': '1', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 1901, '_id': '25691d9d50f44364a9d2af8cd16f433a', '_collection_name': 'test'}, page_content='The varieties of breccia-gashes\\\\*enumerated above are illustrated by diagrammatic sketches in Plate XII., Figs..1, 2, 3, and 4, whilst the nature of the breccia itself is shown in Plate XI lI.,which has been drawn from B photograph of one of the largest gashes near the north end of Marsden Bay, kindly taken for the writer by Mr. W. G. Laws, jun., A.Sc. \\n\\\\* The word gash is a convenient one used oca sion ally by lead-miners to express a fissure unaccompanied by dislocation. See N. Winch\\'s “ Geology of Northumberland and Durham,\" Trans. Geol. Soc., Vol. IV., p. 30, (1816).'),\n",
       " Document(metadata={'page_source': '1', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 1901, '_id': '532dcea68b854fff8ac61ee1445563c0', '_collection_name': 'test'}, page_content='The varieties of breccia-gashes\\\\*enumerated above are illustrated by diagrammatic sketches in Plate XII., Figs..1, 2, 3, and 4, whilst the nature of the breccia itself is shown in Plate XI lI.,which has been drawn from B photograph of one of the largest gashes near the north end of Marsden Bay, kindly taken for the writer by Mr. W. G. Laws, jun., A.Sc. \\n\\\\* The word gash is a convenient one used oca sion ally by lead-miners to express a fissure unaccompanied by dislocation. See N. Winch\\'s “ Geology of Northumberland and Durham,\" Trans. Geol. Soc., Vol. IV., p. 30, (1816).'),\n",
       " Document(metadata={'page_source': '6', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 0, '_id': '5dad70ec92fd43739af40439e49071c2', '_collection_name': 'test'}, page_content='The fragments constituting thebreccia are of all shapes and sizes. from blocks 8 yard or more in diameter to the smallest grains,butallare angular. \\n11.'),\n",
       " Document(metadata={'page_source': '6', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 0, '_id': '80014ca8518d4a888307fb6002f39b9c', '_collection_name': 'test'}, page_content='The fragments constituting thebreccia are of all shapes and sizes. from blocks 8 yard or more in diameter to the smallest grains,butallare angular. \\n11.'),\n",
       " Document(metadata={'page_source': '12', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 696, '_id': 'c9e4b6b6afce4a5d9c8b92b971d87027', '_collection_name': 'test'}, page_content='Here then are the conditions to which it is desired that attention should be directed:——A mass of stone, mostly hard and compact, but cellular in places and earthy and friable in others; often cavernous on a large scale; full of water, and through its action continually parting with its substance, and thus enlarging the cavities within it.')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_parquet(\"../data_task1/MMDocIR_pages.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_name</th>\n",
       "      <th>domain</th>\n",
       "      <th>passage_id</th>\n",
       "      <th>image_path</th>\n",
       "      <th>image_binary</th>\n",
       "      <th>ocr_text</th>\n",
       "      <th>vlm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2310.05634v2</td>\n",
       "      <td>Academic paper</td>\n",
       "      <td>0</td>\n",
       "      <td>page_images/2310.05634v2_0.jpg</td>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>Towards Verifiable Generation: A Benchmark for...</td>\n",
       "      <td>Towards Verifiable Generation: A Benchmark for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2310.05634v2</td>\n",
       "      <td>Academic paper</td>\n",
       "      <td>1</td>\n",
       "      <td>page_images/2310.05634v2_1.jpg</td>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>for attribution? Many real-world applications ...</td>\n",
       "      <td>\\nTo address the first challenge, we utilize k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2310.05634v2</td>\n",
       "      <td>Academic paper</td>\n",
       "      <td>2</td>\n",
       "      <td>page_images/2310.05634v2_2.jpg</td>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>be partially verified by the knowledge graph G...</td>\n",
       "      <td>\\n2.2 Dataset Construction \\nEach entry of dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2310.05634v2</td>\n",
       "      <td>Academic paper</td>\n",
       "      <td>3</td>\n",
       "      <td>page_images/2310.05634v2_3.jpg</td>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>evance on general and specific settings are 73...</td>\n",
       "      <td>\\nIn practice, it is difficult to define a pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2310.05634v2</td>\n",
       "      <td>Academic paper</td>\n",
       "      <td>4</td>\n",
       "      <td>page_images/2310.05634v2_4.jpg</td>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>Model Output: Mininum\\nSentence’ [k1][k2]. Kno...</td>\n",
       "      <td>The image shows a diagram explaining how to ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20390</th>\n",
       "      <td>disciplined-agile-business-analysis-1602180127...</td>\n",
       "      <td>Tutorial/Workshop</td>\n",
       "      <td>36</td>\n",
       "      <td>page_images/disciplined-agile-business-analysi...</td>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>My parting advice....\\n\\nRecognize that Agile ...</td>\n",
       "      <td>My parting advice. \\nRecognize that Agile and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20391</th>\n",
       "      <td>disciplined-agile-business-analysis-1602180127...</td>\n",
       "      <td>Tutorial/Workshop</td>\n",
       "      <td>37</td>\n",
       "      <td>page_images/disciplined-agile-business-analysi...</td>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>Thank You!\\n\\nscott [at] scottambler.com\\nwW @...</td>\n",
       "      <td>Thank You! \\nscott[at]scott ambler.com @scott ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20392</th>\n",
       "      <td>disciplined-agile-business-analysis-1602180127...</td>\n",
       "      <td>Tutorial/Workshop</td>\n",
       "      <td>38</td>\n",
       "      <td>page_images/disciplined-agile-business-analysi...</td>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>Shuhari and Disciplined Agile Certification\\n\\...</td>\n",
       "      <td>Shuhariand Disciplined Agile Certification \\nT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20393</th>\n",
       "      <td>disciplined-agile-business-analysis-1602180127...</td>\n",
       "      <td>Tutorial/Workshop</td>\n",
       "      <td>39</td>\n",
       "      <td>page_images/disciplined-agile-business-analysi...</td>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>Would You Like This Presented\\nto Your Organiz...</td>\n",
       "      <td>Would You Like This Presented to Your Organiza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20394</th>\n",
       "      <td>disciplined-agile-business-analysis-1602180127...</td>\n",
       "      <td>Tutorial/Workshop</td>\n",
       "      <td>40</td>\n",
       "      <td>page_images/disciplined-agile-business-analysi...</td>\n",
       "      <td>b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...</td>\n",
       "      <td>SCOTT AMBLER\\nF Associates\\n\\nScott Ambler + A...</td>\n",
       "      <td>SCOTT AMBLER 十Associates\\nScott Ambler  $^+$  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20395 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                doc_name             domain  \\\n",
       "0                                           2310.05634v2     Academic paper   \n",
       "1                                           2310.05634v2     Academic paper   \n",
       "2                                           2310.05634v2     Academic paper   \n",
       "3                                           2310.05634v2     Academic paper   \n",
       "4                                           2310.05634v2     Academic paper   \n",
       "...                                                  ...                ...   \n",
       "20390  disciplined-agile-business-analysis-1602180127...  Tutorial/Workshop   \n",
       "20391  disciplined-agile-business-analysis-1602180127...  Tutorial/Workshop   \n",
       "20392  disciplined-agile-business-analysis-1602180127...  Tutorial/Workshop   \n",
       "20393  disciplined-agile-business-analysis-1602180127...  Tutorial/Workshop   \n",
       "20394  disciplined-agile-business-analysis-1602180127...  Tutorial/Workshop   \n",
       "\n",
       "      passage_id                                         image_path  \\\n",
       "0              0                     page_images/2310.05634v2_0.jpg   \n",
       "1              1                     page_images/2310.05634v2_1.jpg   \n",
       "2              2                     page_images/2310.05634v2_2.jpg   \n",
       "3              3                     page_images/2310.05634v2_3.jpg   \n",
       "4              4                     page_images/2310.05634v2_4.jpg   \n",
       "...          ...                                                ...   \n",
       "20390         36  page_images/disciplined-agile-business-analysi...   \n",
       "20391         37  page_images/disciplined-agile-business-analysi...   \n",
       "20392         38  page_images/disciplined-agile-business-analysi...   \n",
       "20393         39  page_images/disciplined-agile-business-analysi...   \n",
       "20394         40  page_images/disciplined-agile-business-analysi...   \n",
       "\n",
       "                                            image_binary  \\\n",
       "0      b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
       "1      b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
       "2      b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
       "3      b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
       "4      b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
       "...                                                  ...   \n",
       "20390  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
       "20391  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
       "20392  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
       "20393  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
       "20394  b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00...   \n",
       "\n",
       "                                                ocr_text  \\\n",
       "0      Towards Verifiable Generation: A Benchmark for...   \n",
       "1      for attribution? Many real-world applications ...   \n",
       "2      be partially verified by the knowledge graph G...   \n",
       "3      evance on general and specific settings are 73...   \n",
       "4      Model Output: Mininum\\nSentence’ [k1][k2]. Kno...   \n",
       "...                                                  ...   \n",
       "20390  My parting advice....\\n\\nRecognize that Agile ...   \n",
       "20391  Thank You!\\n\\nscott [at] scottambler.com\\nwW @...   \n",
       "20392  Shuhari and Disciplined Agile Certification\\n\\...   \n",
       "20393  Would You Like This Presented\\nto Your Organiz...   \n",
       "20394  SCOTT AMBLER\\nF Associates\\n\\nScott Ambler + A...   \n",
       "\n",
       "                                                vlm_text  \n",
       "0      Towards Verifiable Generation: A Benchmark for...  \n",
       "1      \\nTo address the first challenge, we utilize k...  \n",
       "2      \\n2.2 Dataset Construction \\nEach entry of dat...  \n",
       "3      \\nIn practice, it is difficult to define a pre...  \n",
       "4      The image shows a diagram explaining how to ev...  \n",
       "...                                                  ...  \n",
       "20390  My parting advice. \\nRecognize that Agile and ...  \n",
       "20391  Thank You! \\nscott[at]scott ambler.com @scott ...  \n",
       "20392  Shuhariand Disciplined Agile Certification \\nT...  \n",
       "20393  Would You Like This Presented to Your Organiza...  \n",
       "20394  SCOTT AMBLER 十Associates\\nScott Ambler  $^+$  ...  \n",
       "\n",
       "[20395 rows x 7 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
