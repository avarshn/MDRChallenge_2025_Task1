question_id,question,passage_id
10000,"In figure 1, which relation arrows do not point to specific leaf nodes?","[Document(metadata={'page_source': '0', 'doc_source': '2310.05634v2', 'start_index': 2751, '_id': '5b933b37abfb4cdd88aae58eece53758', '_collection_name': 'test'}, page_content='The knowledge graph connects these figures with concepts like occupation, style, place of birth, and their citizen status. The output suggests information about Artemisia’s birth and her status as an Italian painter.\ntries that require precision and factual knowledge like finance, law, and medical treatment.'), Document(metadata={'page_source': '0', 'doc_source': '2310.05634v2', 'start_index': 2751, '_id': '51fafc5459e748608d1bb962bf8ff93c', '_collection_name': 'test'}, page_content='The knowledge graph connects these figures with concepts like occupation, style, place of birth, and their citizen status. The output suggests information about Artemisia’s birth and her status as an Italian painter.\ntries that require precision and factual knowledge like finance, law, and medical treatment.'), Document(metadata={'page_source': '0', 'doc_source': '2310.05634v2', 'start_index': 2751, '_id': '78f232bae3e64c2cad8e96450131f42d', '_collection_name': 'test'}, page_content='The knowledge graph connects these figures with concepts like occupation, style, place of birth, and their citizen status. The output suggests information about Artemisia’s birth and her status as an Italian painter.\ntries that require precision and factual knowledge like finance, law, and medical treatment.'), Document(metadata={'page_source': '0', 'doc_source': '2310.05634v2', 'start_index': 2751, '_id': '478a7f94a7ee40709637e32745b101bb', '_collection_name': 'test'}, page_content='The knowledge graph connects these figures with concepts like occupation, style, place of birth, and their citizen status. The output suggests information about Artemisia’s birth and her status as an Italian painter.\ntries that require precision and factual knowledge like finance, law, and medical treatment.'), Document(metadata={'page_source': '0', 'doc_source': '2310.05634v2', 'start_index': 2751, '_id': '1ada8077eb0c47a88c6b9307dea7b085', '_collection_name': 'test'}, page_content='The knowledge graph connects these figures with concepts like occupation, style, place of birth, and their citizen status. The output suggests information about Artemisia’s birth and her status as an Italian painter.\ntries that require precision and factual knowledge like finance, law, and medical treatment.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '18cc6bf5b1824e29ae65cdaaee677cc0', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '0c0a742e33b44860915b53d79b0d1f67', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '3052e2970a974ba6adf9e018f6e0710c', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '708b8bfda47042bcacfcb11fd496b91e', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': 'ebb3238aeedc41fe8e360783679cdb7e', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '2', 'doc_source': '2310.05634v2', 'start_index': 3564, '_id': 'd113de7ab8bb4e1fa947b5db2f5eb85e', '_collection_name': 'test'}, page_content='To our best knowledge, there is no perfect au- tomatic evaluation for these metrics. Naturalness and significance are subjective. Hence, we apply human evaluation to ensure the dataset quality. \nWe randomly sample 50 data entries from BioKaLMA and ask human annotators to evalu- ate the data entries based on the four metrics. The general and specific questions are evaluated sepa- rately. More details are given in Appendix  C .'), Document(metadata={'page_source': '2', 'doc_source': '2310.05634v2', 'start_index': 3564, '_id': '1cde433a54a04645856409ce08402bf1', '_collection_name': 'test'}, page_content='To our best knowledge, there is no perfect au- tomatic evaluation for these metrics. Naturalness and significance are subjective. Hence, we apply human evaluation to ensure the dataset quality. \nWe randomly sample 50 data entries from BioKaLMA and ask human annotators to evalu- ate the data entries based on the four metrics. The general and specific questions are evaluated sepa- rately. More details are given in Appendix  C .'), Document(metadata={'page_source': '2', 'doc_source': '2310.05634v2', 'start_index': 3564, '_id': '3b612a4bba7546bba90489486f3fbd63', '_collection_name': 'test'}, page_content='To our best knowledge, there is no perfect au- tomatic evaluation for these metrics. Naturalness and significance are subjective. Hence, we apply human evaluation to ensure the dataset quality. \nWe randomly sample 50 data entries from BioKaLMA and ask human annotators to evalu- ate the data entries based on the four metrics. The general and specific questions are evaluated sepa- rately. More details are given in Appendix  C .'), Document(metadata={'page_source': '2', 'doc_source': '2310.05634v2', 'start_index': 3564, '_id': 'd7fb405454054706b0cafd460d9854a4', '_collection_name': 'test'}, page_content='To our best knowledge, there is no perfect au- tomatic evaluation for these metrics. Naturalness and significance are subjective. Hence, we apply human evaluation to ensure the dataset quality. \nWe randomly sample 50 data entries from BioKaLMA and ask human annotators to evalu- ate the data entries based on the four metrics. The general and specific questions are evaluated sepa- rately. More details are given in Appendix  C .'), Document(metadata={'page_source': '2', 'doc_source': '2310.05634v2', 'start_index': 3564, '_id': '7c9624c68c324354992e295410af2f36', '_collection_name': 'test'}, page_content='To our best knowledge, there is no perfect au- tomatic evaluation for these metrics. Naturalness and significance are subjective. Hence, we apply human evaluation to ensure the dataset quality. \nWe randomly sample 50 data entries from BioKaLMA and ask human annotators to evalu- ate the data entries based on the four metrics. The general and specific questions are evaluated sepa- rately. More details are given in Appendix  C .')]"
10001,"In figure 5, what is the color of the line that has no intersection with any other line?","[Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '3052e2970a974ba6adf9e018f6e0710c', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '18cc6bf5b1824e29ae65cdaaee677cc0', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '0c0a742e33b44860915b53d79b0d1f67', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '708b8bfda47042bcacfcb11fd496b91e', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': 'ebb3238aeedc41fe8e360783679cdb7e', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 4155, '_id': '7913b604c52f40aa9cfc72d34f585809', '_collection_name': 'test'}, page_content='- The row labeled ""0 (gold)"" has values 95.5 (Corr.), 30.1 (Prec.), 57.1 (Rec.), and 39.4 (F1.).\n- The row labeled ""1"" has values 94.1 (Corr.), 26.1 (Prec.), 42.5 (Rec.), and 32.3 (F1.).\n- The row labeled ""2"" has values 94.0 (Corr.), 21.0 (Prec.), 31.4 (Rec.), and 25.2 (F1.).\n- The row labeled ""3"" has values 93.9 (Corr.), 16.3 (Prec.), 20.4 (Rec.), and 18.1 (F1.).'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 4155, '_id': '8ccbb4b3a39a4e0f8440747d28b8350b', '_collection_name': 'test'}, page_content='- The row labeled ""0 (gold)"" has values 95.5 (Corr.), 30.1 (Prec.), 57.1 (Rec.), and 39.4 (F1.).\n- The row labeled ""1"" has values 94.1 (Corr.), 26.1 (Prec.), 42.5 (Rec.), and 32.3 (F1.).\n- The row labeled ""2"" has values 94.0 (Corr.), 21.0 (Prec.), 31.4 (Rec.), and 25.2 (F1.).\n- The row labeled ""3"" has values 93.9 (Corr.), 16.3 (Prec.), 20.4 (Rec.), and 18.1 (F1.).'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 4155, '_id': '9e777ed8044d4b449a63e45cdffd8e56', '_collection_name': 'test'}, page_content='- The row labeled ""0 (gold)"" has values 95.5 (Corr.), 30.1 (Prec.), 57.1 (Rec.), and 39.4 (F1.).\n- The row labeled ""1"" has values 94.1 (Corr.), 26.1 (Prec.), 42.5 (Rec.), and 32.3 (F1.).\n- The row labeled ""2"" has values 94.0 (Corr.), 21.0 (Prec.), 31.4 (Rec.), and 25.2 (F1.).\n- The row labeled ""3"" has values 93.9 (Corr.), 16.3 (Prec.), 20.4 (Rec.), and 18.1 (F1.).'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 4155, '_id': '7f6f809d6bfd4933bee1ab611842c294', '_collection_name': 'test'}, page_content='- The row labeled ""0 (gold)"" has values 95.5 (Corr.), 30.1 (Prec.), 57.1 (Rec.), and 39.4 (F1.).\n- The row labeled ""1"" has values 94.1 (Corr.), 26.1 (Prec.), 42.5 (Rec.), and 32.3 (F1.).\n- The row labeled ""2"" has values 94.0 (Corr.), 21.0 (Prec.), 31.4 (Rec.), and 25.2 (F1.).\n- The row labeled ""3"" has values 93.9 (Corr.), 16.3 (Prec.), 20.4 (Rec.), and 18.1 (F1.).'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 4155, '_id': 'dba35a2413b3404d8216a02cb937929f', '_collection_name': 'test'}, page_content='- The row labeled ""0 (gold)"" has values 95.5 (Corr.), 30.1 (Prec.), 57.1 (Rec.), and 39.4 (F1.).\n- The row labeled ""1"" has values 94.1 (Corr.), 26.1 (Prec.), 42.5 (Rec.), and 32.3 (F1.).\n- The row labeled ""2"" has values 94.0 (Corr.), 21.0 (Prec.), 31.4 (Rec.), and 25.2 (F1.).\n- The row labeled ""3"" has values 93.9 (Corr.), 16.3 (Prec.), 20.4 (Rec.), and 18.1 (F1.).'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': 'a2ff907be4384f49b3e7114ffcf90875', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': '5077b28c8d8d41eabf3b8e77a954949d', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': 'cbf9c1fe83e24a248ba2021bedcc9cdb', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': 'f83c90f322654fedaaa45697ac763aa3', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': '80f4128715084ef8b6c1c30e3de503e8', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.')]"
10002,"How many tables include ""F1"" as a metric?","[Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': 'a2ff907be4384f49b3e7114ffcf90875', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': '5077b28c8d8d41eabf3b8e77a954949d', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': 'cbf9c1fe83e24a248ba2021bedcc9cdb', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': '80f4128715084ef8b6c1c30e3de503e8', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': 'f83c90f322654fedaaa45697ac763aa3', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': 'a499b22f3e9a4afe94b238c844892287', '_collection_name': 'test'}, page_content='The table presents a comparison of different models, including GPT-4, ChatGPT, Alpaca-7B, LLaMA-7B, LLaMA-13B, and Vicuna-13B. It compares metrics such as Alignment, Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1-score (F1.) under both ""Micro"" and ""Macro"" settings. Each metric is accompanied by a smaller value in parentheses, indicating some form of sub-measurement or statistical variation. \n\nHere are the columns explained:\n\n1. **Align.** - Alignment score of the models.\n2. **Corr.** - Correctness score.\n3. **Micro:** \n   - **Prec.** - Precision under micro averaging.\n   - **Rec.** - Recall under micro averaging.\n   - **F1.** - F1-score under micro averaging.\n4. **Macro:** \n   - **Prec.** - Precision under macro averaging.\n   - **Rec.** - Recall under macro averaging.\n   - **F1.** - F1-score under macro averaging.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': 'b3ec090246da4b6b830e76f04e702468', '_collection_name': 'test'}, page_content='The table presents a comparison of different models, including GPT-4, ChatGPT, Alpaca-7B, LLaMA-7B, LLaMA-13B, and Vicuna-13B. It compares metrics such as Alignment, Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1-score (F1.) under both ""Micro"" and ""Macro"" settings. Each metric is accompanied by a smaller value in parentheses, indicating some form of sub-measurement or statistical variation. \n\nHere are the columns explained:\n\n1. **Align.** - Alignment score of the models.\n2. **Corr.** - Correctness score.\n3. **Micro:** \n   - **Prec.** - Precision under micro averaging.\n   - **Rec.** - Recall under micro averaging.\n   - **F1.** - F1-score under micro averaging.\n4. **Macro:** \n   - **Prec.** - Precision under macro averaging.\n   - **Rec.** - Recall under macro averaging.\n   - **F1.** - F1-score under macro averaging.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '586ca4cef5b24633b8e6cf92435af0ec', '_collection_name': 'test'}, page_content='The table presents a comparison of different models, including GPT-4, ChatGPT, Alpaca-7B, LLaMA-7B, LLaMA-13B, and Vicuna-13B. It compares metrics such as Alignment, Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1-score (F1.) under both ""Micro"" and ""Macro"" settings. Each metric is accompanied by a smaller value in parentheses, indicating some form of sub-measurement or statistical variation. \n\nHere are the columns explained:\n\n1. **Align.** - Alignment score of the models.\n2. **Corr.** - Correctness score.\n3. **Micro:** \n   - **Prec.** - Precision under micro averaging.\n   - **Rec.** - Recall under micro averaging.\n   - **F1.** - F1-score under micro averaging.\n4. **Macro:** \n   - **Prec.** - Precision under macro averaging.\n   - **Rec.** - Recall under macro averaging.\n   - **F1.** - F1-score under macro averaging.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '14025d0926e14806a63db374a6157f1f', '_collection_name': 'test'}, page_content='The table presents a comparison of different models, including GPT-4, ChatGPT, Alpaca-7B, LLaMA-7B, LLaMA-13B, and Vicuna-13B. It compares metrics such as Alignment, Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1-score (F1.) under both ""Micro"" and ""Macro"" settings. Each metric is accompanied by a smaller value in parentheses, indicating some form of sub-measurement or statistical variation. \n\nHere are the columns explained:\n\n1. **Align.** - Alignment score of the models.\n2. **Corr.** - Correctness score.\n3. **Micro:** \n   - **Prec.** - Precision under micro averaging.\n   - **Rec.** - Recall under micro averaging.\n   - **F1.** - F1-score under micro averaging.\n4. **Macro:** \n   - **Prec.** - Precision under macro averaging.\n   - **Rec.** - Recall under macro averaging.\n   - **F1.** - F1-score under macro averaging.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': 'a935f1e31a86496fa8e430efbaef8c6a', '_collection_name': 'test'}, page_content='The table presents a comparison of different models, including GPT-4, ChatGPT, Alpaca-7B, LLaMA-7B, LLaMA-13B, and Vicuna-13B. It compares metrics such as Alignment, Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1-score (F1.) under both ""Micro"" and ""Macro"" settings. Each metric is accompanied by a smaller value in parentheses, indicating some form of sub-measurement or statistical variation. \n\nHere are the columns explained:\n\n1. **Align.** - Alignment score of the models.\n2. **Corr.** - Correctness score.\n3. **Micro:** \n   - **Prec.** - Precision under micro averaging.\n   - **Rec.** - Recall under micro averaging.\n   - **F1.** - F1-score under micro averaging.\n4. **Macro:** \n   - **Prec.** - Precision under macro averaging.\n   - **Rec.** - Recall under macro averaging.\n   - **F1.** - F1-score under macro averaging.'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 3490, '_id': 'e958901076894f9c8167a313af088cd5', '_collection_name': 'test'}, page_content='The image uses precise color-coding for clarity: green check marks for correct matches, red crosses for incorrect ones, and arrows to represent the calculation of precision and recall.\nknowledge triplet from minimum knowledge set of the question. (See Figure  2 .) \nRecall We calculate citation recall for each knowledge (0 or 1) in minimum knowledge set, and average over all knowledge to get micro recall. Recall  $=1$   if and only if the knowledge if hit by a correct citation. (See Figure  2 .) \nWe average over all citations/knowledge in an an- swer, and average all answer-level precision/recall to get macro precision and recall. we calculate micro and macro F1-Score from corresponding pre- cision and recall. \n4.3 Text-Citation Alignment'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 3490, '_id': 'c0847333b75b4f86b5167a6d4a55182d', '_collection_name': 'test'}, page_content='The image uses precise color-coding for clarity: green check marks for correct matches, red crosses for incorrect ones, and arrows to represent the calculation of precision and recall.\nknowledge triplet from minimum knowledge set of the question. (See Figure  2 .) \nRecall We calculate citation recall for each knowledge (0 or 1) in minimum knowledge set, and average over all knowledge to get micro recall. Recall  $=1$   if and only if the knowledge if hit by a correct citation. (See Figure  2 .) \nWe average over all citations/knowledge in an an- swer, and average all answer-level precision/recall to get macro precision and recall. we calculate micro and macro F1-Score from corresponding pre- cision and recall. \n4.3 Text-Citation Alignment'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 3490, '_id': '3912f91812314f439e76cd08f79d2baf', '_collection_name': 'test'}, page_content='The image uses precise color-coding for clarity: green check marks for correct matches, red crosses for incorrect ones, and arrows to represent the calculation of precision and recall.\nknowledge triplet from minimum knowledge set of the question. (See Figure  2 .) \nRecall We calculate citation recall for each knowledge (0 or 1) in minimum knowledge set, and average over all knowledge to get micro recall. Recall  $=1$   if and only if the knowledge if hit by a correct citation. (See Figure  2 .) \nWe average over all citations/knowledge in an an- swer, and average all answer-level precision/recall to get macro precision and recall. we calculate micro and macro F1-Score from corresponding pre- cision and recall. \n4.3 Text-Citation Alignment'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 3490, '_id': '554a47df83064870b03d08d8bd3683e5', '_collection_name': 'test'}, page_content='The image uses precise color-coding for clarity: green check marks for correct matches, red crosses for incorrect ones, and arrows to represent the calculation of precision and recall.\nknowledge triplet from minimum knowledge set of the question. (See Figure  2 .) \nRecall We calculate citation recall for each knowledge (0 or 1) in minimum knowledge set, and average over all knowledge to get micro recall. Recall  $=1$   if and only if the knowledge if hit by a correct citation. (See Figure  2 .) \nWe average over all citations/knowledge in an an- swer, and average all answer-level precision/recall to get macro precision and recall. we calculate micro and macro F1-Score from corresponding pre- cision and recall. \n4.3 Text-Citation Alignment'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 3490, '_id': '3ed2f34ae561440b9c6503751921ba30', '_collection_name': 'test'}, page_content='The image uses precise color-coding for clarity: green check marks for correct matches, red crosses for incorrect ones, and arrows to represent the calculation of precision and recall.\nknowledge triplet from minimum knowledge set of the question. (See Figure  2 .) \nRecall We calculate citation recall for each knowledge (0 or 1) in minimum knowledge set, and average over all knowledge to get micro recall. Recall  $=1$   if and only if the knowledge if hit by a correct citation. (See Figure  2 .) \nWe average over all citations/knowledge in an an- swer, and average all answer-level precision/recall to get macro precision and recall. we calculate micro and macro F1-Score from corresponding pre- cision and recall. \n4.3 Text-Citation Alignment')]"
10003,"From the paper, which temperature gives ChatGPT the highest alignment score?","[Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': '5077b28c8d8d41eabf3b8e77a954949d', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': 'a2ff907be4384f49b3e7114ffcf90875', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': '80f4128715084ef8b6c1c30e3de503e8', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': 'cbf9c1fe83e24a248ba2021bedcc9cdb', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': 'f83c90f322654fedaaa45697ac763aa3', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '586ca4cef5b24633b8e6cf92435af0ec', '_collection_name': 'test'}, page_content='The table presents a comparison of different models, including GPT-4, ChatGPT, Alpaca-7B, LLaMA-7B, LLaMA-13B, and Vicuna-13B. It compares metrics such as Alignment, Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1-score (F1.) under both ""Micro"" and ""Macro"" settings. Each metric is accompanied by a smaller value in parentheses, indicating some form of sub-measurement or statistical variation. \n\nHere are the columns explained:\n\n1. **Align.** - Alignment score of the models.\n2. **Corr.** - Correctness score.\n3. **Micro:** \n   - **Prec.** - Precision under micro averaging.\n   - **Rec.** - Recall under micro averaging.\n   - **F1.** - F1-score under micro averaging.\n4. **Macro:** \n   - **Prec.** - Precision under macro averaging.\n   - **Rec.** - Recall under macro averaging.\n   - **F1.** - F1-score under macro averaging.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': 'b3ec090246da4b6b830e76f04e702468', '_collection_name': 'test'}, page_content='The table presents a comparison of different models, including GPT-4, ChatGPT, Alpaca-7B, LLaMA-7B, LLaMA-13B, and Vicuna-13B. It compares metrics such as Alignment, Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1-score (F1.) under both ""Micro"" and ""Macro"" settings. Each metric is accompanied by a smaller value in parentheses, indicating some form of sub-measurement or statistical variation. \n\nHere are the columns explained:\n\n1. **Align.** - Alignment score of the models.\n2. **Corr.** - Correctness score.\n3. **Micro:** \n   - **Prec.** - Precision under micro averaging.\n   - **Rec.** - Recall under micro averaging.\n   - **F1.** - F1-score under micro averaging.\n4. **Macro:** \n   - **Prec.** - Precision under macro averaging.\n   - **Rec.** - Recall under macro averaging.\n   - **F1.** - F1-score under macro averaging.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '14025d0926e14806a63db374a6157f1f', '_collection_name': 'test'}, page_content='The table presents a comparison of different models, including GPT-4, ChatGPT, Alpaca-7B, LLaMA-7B, LLaMA-13B, and Vicuna-13B. It compares metrics such as Alignment, Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1-score (F1.) under both ""Micro"" and ""Macro"" settings. Each metric is accompanied by a smaller value in parentheses, indicating some form of sub-measurement or statistical variation. \n\nHere are the columns explained:\n\n1. **Align.** - Alignment score of the models.\n2. **Corr.** - Correctness score.\n3. **Micro:** \n   - **Prec.** - Precision under micro averaging.\n   - **Rec.** - Recall under micro averaging.\n   - **F1.** - F1-score under micro averaging.\n4. **Macro:** \n   - **Prec.** - Precision under macro averaging.\n   - **Rec.** - Recall under macro averaging.\n   - **F1.** - F1-score under macro averaging.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': 'a935f1e31a86496fa8e430efbaef8c6a', '_collection_name': 'test'}, page_content='The table presents a comparison of different models, including GPT-4, ChatGPT, Alpaca-7B, LLaMA-7B, LLaMA-13B, and Vicuna-13B. It compares metrics such as Alignment, Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1-score (F1.) under both ""Micro"" and ""Macro"" settings. Each metric is accompanied by a smaller value in parentheses, indicating some form of sub-measurement or statistical variation. \n\nHere are the columns explained:\n\n1. **Align.** - Alignment score of the models.\n2. **Corr.** - Correctness score.\n3. **Micro:** \n   - **Prec.** - Precision under micro averaging.\n   - **Rec.** - Recall under micro averaging.\n   - **F1.** - F1-score under micro averaging.\n4. **Macro:** \n   - **Prec.** - Precision under macro averaging.\n   - **Rec.** - Recall under macro averaging.\n   - **F1.** - F1-score under macro averaging.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': 'a499b22f3e9a4afe94b238c844892287', '_collection_name': 'test'}, page_content='The table presents a comparison of different models, including GPT-4, ChatGPT, Alpaca-7B, LLaMA-7B, LLaMA-13B, and Vicuna-13B. It compares metrics such as Alignment, Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1-score (F1.) under both ""Micro"" and ""Macro"" settings. Each metric is accompanied by a smaller value in parentheses, indicating some form of sub-measurement or statistical variation. \n\nHere are the columns explained:\n\n1. **Align.** - Alignment score of the models.\n2. **Corr.** - Correctness score.\n3. **Micro:** \n   - **Prec.** - Precision under micro averaging.\n   - **Rec.** - Recall under micro averaging.\n   - **F1.** - F1-score under micro averaging.\n4. **Macro:** \n   - **Prec.** - Precision under macro averaging.\n   - **Rec.** - Recall under macro averaging.\n   - **F1.** - F1-score under macro averaging.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 3225, '_id': 'e13836a21ada41ad912e755510851073', '_collection_name': 'test'}, page_content='1. **GPT-4 (0.5)**\n   - Coh.: 4.48\n   - Con.: 4.89\n   - Flu.: 4.64\n   - Rel.: 4.72\n\n2. **ChatGPT (0.1)**\n   - Coh.: 4.57\n   - Con.: 4.94\n   - Flu.: 4.69\n   - Rel.: 4.83\n\n3. **ChatGPT (0.5)**\n   - Coh.: 4.57\n   - Con.: 4.94\n   - Flu.: 4.71\n   - Rel.: 4.81\n\n4. **ChatGPT (0.9)**\n   - Coh.: 4.52\n   - Con.: 4.91\n   - Flu.: 4.67\n   - Rel.: 4.79\n\n5. **Alpaca-7B**\n   - Coh.: 4.10\n   - Con.: 4.46\n   - Flu.: 4.23\n   - Rel.: 3.76\n\n6. **LLaMa-7B**\n   - Coh.: 3.06\n   - Con.: 3.79\n   - Flu.: 3.62\n   - Rel.: 2.96\n\n7. **LLaMa-13B**\n   - Coh.: 3.60\n   - Con.: 4.23\n   - Flu.: 3.94\n   - Rel.: 3.56\n\n8. **Vicuna-13B**\n   - Coh.: 3.67\n   - Con.: 4.50\n   - Flu.: 3.96\n   - Rel.: 3.64\n\nOverall, the ChatGPT variants, particularly ChatGPT (0.1) and ChatGPT (0.5), tend to have higher scores across all metrics compared to the other models.\nThe table displays the following data across five columns: ""Removed"", ""Corr."", ""Prec."", ""Rec."", and ""F1.""'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 3225, '_id': '699cca522ed34a418297a88ee31aff8c', '_collection_name': 'test'}, page_content='1. **GPT-4 (0.5)**\n   - Coh.: 4.48\n   - Con.: 4.89\n   - Flu.: 4.64\n   - Rel.: 4.72\n\n2. **ChatGPT (0.1)**\n   - Coh.: 4.57\n   - Con.: 4.94\n   - Flu.: 4.69\n   - Rel.: 4.83\n\n3. **ChatGPT (0.5)**\n   - Coh.: 4.57\n   - Con.: 4.94\n   - Flu.: 4.71\n   - Rel.: 4.81\n\n4. **ChatGPT (0.9)**\n   - Coh.: 4.52\n   - Con.: 4.91\n   - Flu.: 4.67\n   - Rel.: 4.79\n\n5. **Alpaca-7B**\n   - Coh.: 4.10\n   - Con.: 4.46\n   - Flu.: 4.23\n   - Rel.: 3.76\n\n6. **LLaMa-7B**\n   - Coh.: 3.06\n   - Con.: 3.79\n   - Flu.: 3.62\n   - Rel.: 2.96\n\n7. **LLaMa-13B**\n   - Coh.: 3.60\n   - Con.: 4.23\n   - Flu.: 3.94\n   - Rel.: 3.56\n\n8. **Vicuna-13B**\n   - Coh.: 3.67\n   - Con.: 4.50\n   - Flu.: 3.96\n   - Rel.: 3.64\n\nOverall, the ChatGPT variants, particularly ChatGPT (0.1) and ChatGPT (0.5), tend to have higher scores across all metrics compared to the other models.\nThe table displays the following data across five columns: ""Removed"", ""Corr."", ""Prec."", ""Rec."", and ""F1.""'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 3225, '_id': '6822661cfd0e403e8e4800761df67e72', '_collection_name': 'test'}, page_content='1. **GPT-4 (0.5)**\n   - Coh.: 4.48\n   - Con.: 4.89\n   - Flu.: 4.64\n   - Rel.: 4.72\n\n2. **ChatGPT (0.1)**\n   - Coh.: 4.57\n   - Con.: 4.94\n   - Flu.: 4.69\n   - Rel.: 4.83\n\n3. **ChatGPT (0.5)**\n   - Coh.: 4.57\n   - Con.: 4.94\n   - Flu.: 4.71\n   - Rel.: 4.81\n\n4. **ChatGPT (0.9)**\n   - Coh.: 4.52\n   - Con.: 4.91\n   - Flu.: 4.67\n   - Rel.: 4.79\n\n5. **Alpaca-7B**\n   - Coh.: 4.10\n   - Con.: 4.46\n   - Flu.: 4.23\n   - Rel.: 3.76\n\n6. **LLaMa-7B**\n   - Coh.: 3.06\n   - Con.: 3.79\n   - Flu.: 3.62\n   - Rel.: 2.96\n\n7. **LLaMa-13B**\n   - Coh.: 3.60\n   - Con.: 4.23\n   - Flu.: 3.94\n   - Rel.: 3.56\n\n8. **Vicuna-13B**\n   - Coh.: 3.67\n   - Con.: 4.50\n   - Flu.: 3.96\n   - Rel.: 3.64\n\nOverall, the ChatGPT variants, particularly ChatGPT (0.1) and ChatGPT (0.5), tend to have higher scores across all metrics compared to the other models.\nThe table displays the following data across five columns: ""Removed"", ""Corr."", ""Prec."", ""Rec."", and ""F1.""'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 3225, '_id': 'aeb2a4f8ae8f4780876ddca48a6368a3', '_collection_name': 'test'}, page_content='1. **GPT-4 (0.5)**\n   - Coh.: 4.48\n   - Con.: 4.89\n   - Flu.: 4.64\n   - Rel.: 4.72\n\n2. **ChatGPT (0.1)**\n   - Coh.: 4.57\n   - Con.: 4.94\n   - Flu.: 4.69\n   - Rel.: 4.83\n\n3. **ChatGPT (0.5)**\n   - Coh.: 4.57\n   - Con.: 4.94\n   - Flu.: 4.71\n   - Rel.: 4.81\n\n4. **ChatGPT (0.9)**\n   - Coh.: 4.52\n   - Con.: 4.91\n   - Flu.: 4.67\n   - Rel.: 4.79\n\n5. **Alpaca-7B**\n   - Coh.: 4.10\n   - Con.: 4.46\n   - Flu.: 4.23\n   - Rel.: 3.76\n\n6. **LLaMa-7B**\n   - Coh.: 3.06\n   - Con.: 3.79\n   - Flu.: 3.62\n   - Rel.: 2.96\n\n7. **LLaMa-13B**\n   - Coh.: 3.60\n   - Con.: 4.23\n   - Flu.: 3.94\n   - Rel.: 3.56\n\n8. **Vicuna-13B**\n   - Coh.: 3.67\n   - Con.: 4.50\n   - Flu.: 3.96\n   - Rel.: 3.64\n\nOverall, the ChatGPT variants, particularly ChatGPT (0.1) and ChatGPT (0.5), tend to have higher scores across all metrics compared to the other models.\nThe table displays the following data across five columns: ""Removed"", ""Corr."", ""Prec."", ""Rec."", and ""F1.""'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 3225, '_id': 'bdc6ad4eda1f4ed6b57c22b8f65819e4', '_collection_name': 'test'}, page_content='1. **GPT-4 (0.5)**\n   - Coh.: 4.48\n   - Con.: 4.89\n   - Flu.: 4.64\n   - Rel.: 4.72\n\n2. **ChatGPT (0.1)**\n   - Coh.: 4.57\n   - Con.: 4.94\n   - Flu.: 4.69\n   - Rel.: 4.83\n\n3. **ChatGPT (0.5)**\n   - Coh.: 4.57\n   - Con.: 4.94\n   - Flu.: 4.71\n   - Rel.: 4.81\n\n4. **ChatGPT (0.9)**\n   - Coh.: 4.52\n   - Con.: 4.91\n   - Flu.: 4.67\n   - Rel.: 4.79\n\n5. **Alpaca-7B**\n   - Coh.: 4.10\n   - Con.: 4.46\n   - Flu.: 4.23\n   - Rel.: 3.76\n\n6. **LLaMa-7B**\n   - Coh.: 3.06\n   - Con.: 3.79\n   - Flu.: 3.62\n   - Rel.: 2.96\n\n7. **LLaMa-13B**\n   - Coh.: 3.60\n   - Con.: 4.23\n   - Flu.: 3.94\n   - Rel.: 3.56\n\n8. **Vicuna-13B**\n   - Coh.: 3.67\n   - Con.: 4.50\n   - Flu.: 3.96\n   - Rel.: 3.64\n\nOverall, the ChatGPT variants, particularly ChatGPT (0.1) and ChatGPT (0.5), tend to have higher scores across all metrics compared to the other models.\nThe table displays the following data across five columns: ""Removed"", ""Corr."", ""Prec."", ""Rec."", and ""F1.""')]"
10004,"For dataset construction, which step takes the most word to describe than the others. ","[Document(metadata={'page_source': '11', 'doc_source': '2310.05634v2', 'start_index': 1262, '_id': '2cad5ba400d64a3e8fbbadf91d44594d', '_collection_name': 'test'}, page_content='A Dataset Construction \nIn this section, we will explain the detailed process and algorithms for the automatic dataset construc- tion pipeline. Using this pipeline, we are able to construct datasets with a greater scale or in other domains. \nA.1 Person Selection'), Document(metadata={'page_source': '11', 'doc_source': '2310.05634v2', 'start_index': 1262, '_id': '8aa02167a62a43ca898c089444dfdcc9', '_collection_name': 'test'}, page_content='A Dataset Construction \nIn this section, we will explain the detailed process and algorithms for the automatic dataset construc- tion pipeline. Using this pipeline, we are able to construct datasets with a greater scale or in other domains. \nA.1 Person Selection'), Document(metadata={'page_source': '11', 'doc_source': '2310.05634v2', 'start_index': 1262, '_id': '17813dc0a8434dae8710436ab6b3a56f', '_collection_name': 'test'}, page_content='A Dataset Construction \nIn this section, we will explain the detailed process and algorithms for the automatic dataset construc- tion pipeline. Using this pipeline, we are able to construct datasets with a greater scale or in other domains. \nA.1 Person Selection'), Document(metadata={'page_source': '11', 'doc_source': '2310.05634v2', 'start_index': 1262, '_id': '417f0d18ce48464abf6cbb2ebc761016', '_collection_name': 'test'}, page_content='A Dataset Construction \nIn this section, we will explain the detailed process and algorithms for the automatic dataset construc- tion pipeline. Using this pipeline, we are able to construct datasets with a greater scale or in other domains. \nA.1 Person Selection'), Document(metadata={'page_source': '11', 'doc_source': '2310.05634v2', 'start_index': 1262, '_id': '680cf72f24ee423f924f71de2737d634', '_collection_name': 'test'}, page_content='A Dataset Construction \nIn this section, we will explain the detailed process and algorithms for the automatic dataset construc- tion pipeline. Using this pipeline, we are able to construct datasets with a greater scale or in other domains. \nA.1 Person Selection'), Document(metadata={'page_source': '13', 'doc_source': '2310.05634v2', 'start_index': 776, '_id': '61744bbca1cb4b3f92b092300726d697', '_collection_name': 'test'}, page_content='C.1 Dataset Evaluation \nTo evaluate the dataset quality, we have two indi- vidual annotators who are proficient in the English language. Below are the exact method for evaluat- ing each metric: \n•  Authenticity . We ask the annotators to check from WikiPedia and understand the back- ground stories of the mentioned people, and decide if the generated question matches the background story. Each question is assigned score 1 if it matches the background story, and score 0 if there is contradiction. \n•  Relevance . After understanding the back- ground stories, we ask the annotators to label each piece of knowledge from the minimum knowledge set. A piece of knowledge is la- beled 1 if the annotator thinks it is necessary to answer the question, and 0 if it is redun- dant. The relevance score of a question is the ratio of number of necessary knowledge to the number of knowledge in the minimum set.'), Document(metadata={'page_source': '13', 'doc_source': '2310.05634v2', 'start_index': 776, '_id': 'a15cd9b40ed143f9a4b71fd55ae92e6d', '_collection_name': 'test'}, page_content='C.1 Dataset Evaluation \nTo evaluate the dataset quality, we have two indi- vidual annotators who are proficient in the English language. Below are the exact method for evaluat- ing each metric: \n•  Authenticity . We ask the annotators to check from WikiPedia and understand the back- ground stories of the mentioned people, and decide if the generated question matches the background story. Each question is assigned score 1 if it matches the background story, and score 0 if there is contradiction. \n•  Relevance . After understanding the back- ground stories, we ask the annotators to label each piece of knowledge from the minimum knowledge set. A piece of knowledge is la- beled 1 if the annotator thinks it is necessary to answer the question, and 0 if it is redun- dant. The relevance score of a question is the ratio of number of necessary knowledge to the number of knowledge in the minimum set.'), Document(metadata={'page_source': '13', 'doc_source': '2310.05634v2', 'start_index': 776, '_id': '36a51d3d8a3b44a8b04fdfdab73d5ac5', '_collection_name': 'test'}, page_content='C.1 Dataset Evaluation \nTo evaluate the dataset quality, we have two indi- vidual annotators who are proficient in the English language. Below are the exact method for evaluat- ing each metric: \n•  Authenticity . We ask the annotators to check from WikiPedia and understand the back- ground stories of the mentioned people, and decide if the generated question matches the background story. Each question is assigned score 1 if it matches the background story, and score 0 if there is contradiction. \n•  Relevance . After understanding the back- ground stories, we ask the annotators to label each piece of knowledge from the minimum knowledge set. A piece of knowledge is la- beled 1 if the annotator thinks it is necessary to answer the question, and 0 if it is redun- dant. The relevance score of a question is the ratio of number of necessary knowledge to the number of knowledge in the minimum set.'), Document(metadata={'page_source': '13', 'doc_source': '2310.05634v2', 'start_index': 776, '_id': '38978546810645fda41519bb2bb9d788', '_collection_name': 'test'}, page_content='C.1 Dataset Evaluation \nTo evaluate the dataset quality, we have two indi- vidual annotators who are proficient in the English language. Below are the exact method for evaluat- ing each metric: \n•  Authenticity . We ask the annotators to check from WikiPedia and understand the back- ground stories of the mentioned people, and decide if the generated question matches the background story. Each question is assigned score 1 if it matches the background story, and score 0 if there is contradiction. \n•  Relevance . After understanding the back- ground stories, we ask the annotators to label each piece of knowledge from the minimum knowledge set. A piece of knowledge is la- beled 1 if the annotator thinks it is necessary to answer the question, and 0 if it is redun- dant. The relevance score of a question is the ratio of number of necessary knowledge to the number of knowledge in the minimum set.'), Document(metadata={'page_source': '13', 'doc_source': '2310.05634v2', 'start_index': 776, '_id': 'f36440d2efd74eecafdead344e2a8bc8', '_collection_name': 'test'}, page_content='C.1 Dataset Evaluation \nTo evaluate the dataset quality, we have two indi- vidual annotators who are proficient in the English language. Below are the exact method for evaluat- ing each metric: \n•  Authenticity . We ask the annotators to check from WikiPedia and understand the back- ground stories of the mentioned people, and decide if the generated question matches the background story. Each question is assigned score 1 if it matches the background story, and score 0 if there is contradiction. \n•  Relevance . After understanding the back- ground stories, we ask the annotators to label each piece of knowledge from the minimum knowledge set. A piece of knowledge is la- beled 1 if the annotator thinks it is necessary to answer the question, and 0 if it is redun- dant. The relevance score of a question is the ratio of number of necessary knowledge to the number of knowledge in the minimum set.'), Document(metadata={'page_source': '2', 'doc_source': '2310.05634v2', 'start_index': 2839, '_id': 'd5113a83e9214bc5bdadf0820b969e32', '_collection_name': 'test'}, page_content='Each metric shows how both General and Specific categories are scored, with full scores given in parentheses next to each metric name for reference.\nEvaluation of Dataset Quality We evaluate the BioKaLMA dataset on the following four metrics to ensure the quality of the dataset: 1)  Authen- ticity : The generated questions should accurately reflect the objective facts. 2)  Relevance : Each min- imum knowledge set should provide support to the corresponding question. Each piece of knowledge from the minimum knowledge set is not redundant. 3)  Naturalness : The generated question should be concise and understandable by human readers. 4) Significance : The generated question should be meaningful and helpful to users. \nTo our best knowledge, there is no perfect au- tomatic evaluation for these metrics. Naturalness and significance are subjective. Hence, we apply human evaluation to ensure the dataset quality.'), Document(metadata={'page_source': '2', 'doc_source': '2310.05634v2', 'start_index': 2839, '_id': '4eff0849f76544219970decce979854e', '_collection_name': 'test'}, page_content='Each metric shows how both General and Specific categories are scored, with full scores given in parentheses next to each metric name for reference.\nEvaluation of Dataset Quality We evaluate the BioKaLMA dataset on the following four metrics to ensure the quality of the dataset: 1)  Authen- ticity : The generated questions should accurately reflect the objective facts. 2)  Relevance : Each min- imum knowledge set should provide support to the corresponding question. Each piece of knowledge from the minimum knowledge set is not redundant. 3)  Naturalness : The generated question should be concise and understandable by human readers. 4) Significance : The generated question should be meaningful and helpful to users. \nTo our best knowledge, there is no perfect au- tomatic evaluation for these metrics. Naturalness and significance are subjective. Hence, we apply human evaluation to ensure the dataset quality.'), Document(metadata={'page_source': '2', 'doc_source': '2310.05634v2', 'start_index': 2839, '_id': 'd41e32efeb464c1e9ee3f4376e2e76ca', '_collection_name': 'test'}, page_content='Each metric shows how both General and Specific categories are scored, with full scores given in parentheses next to each metric name for reference.\nEvaluation of Dataset Quality We evaluate the BioKaLMA dataset on the following four metrics to ensure the quality of the dataset: 1)  Authen- ticity : The generated questions should accurately reflect the objective facts. 2)  Relevance : Each min- imum knowledge set should provide support to the corresponding question. Each piece of knowledge from the minimum knowledge set is not redundant. 3)  Naturalness : The generated question should be concise and understandable by human readers. 4) Significance : The generated question should be meaningful and helpful to users. \nTo our best knowledge, there is no perfect au- tomatic evaluation for these metrics. Naturalness and significance are subjective. Hence, we apply human evaluation to ensure the dataset quality.'), Document(metadata={'page_source': '2', 'doc_source': '2310.05634v2', 'start_index': 2839, '_id': '1767508e50244e8495f0c9b24c3dd3d2', '_collection_name': 'test'}, page_content='Each metric shows how both General and Specific categories are scored, with full scores given in parentheses next to each metric name for reference.\nEvaluation of Dataset Quality We evaluate the BioKaLMA dataset on the following four metrics to ensure the quality of the dataset: 1)  Authen- ticity : The generated questions should accurately reflect the objective facts. 2)  Relevance : Each min- imum knowledge set should provide support to the corresponding question. Each piece of knowledge from the minimum knowledge set is not redundant. 3)  Naturalness : The generated question should be concise and understandable by human readers. 4) Significance : The generated question should be meaningful and helpful to users. \nTo our best knowledge, there is no perfect au- tomatic evaluation for these metrics. Naturalness and significance are subjective. Hence, we apply human evaluation to ensure the dataset quality.'), Document(metadata={'page_source': '2', 'doc_source': '2310.05634v2', 'start_index': 2839, '_id': 'aeb5423a01cf459f8c873b6605c498ef', '_collection_name': 'test'}, page_content='Each metric shows how both General and Specific categories are scored, with full scores given in parentheses next to each metric name for reference.\nEvaluation of Dataset Quality We evaluate the BioKaLMA dataset on the following four metrics to ensure the quality of the dataset: 1)  Authen- ticity : The generated questions should accurately reflect the objective facts. 2)  Relevance : Each min- imum knowledge set should provide support to the corresponding question. Each piece of knowledge from the minimum knowledge set is not redundant. 3)  Naturalness : The generated question should be concise and understandable by human readers. 4) Significance : The generated question should be meaningful and helpful to users. \nTo our best knowledge, there is no perfect au- tomatic evaluation for these metrics. Naturalness and significance are subjective. Hence, we apply human evaluation to ensure the dataset quality.')]"
10005,"According to author's definition on conscious incompetence, when can a sentence map to both [NA] and a list of sub-graph knowledge?","[Document(metadata={'page_source': '1', 'doc_source': '2310.05634v2', 'start_index': 3851, '_id': 'ce74b6cda36247a0a0811f9358dc200e', '_collection_name': 'test'}, page_content='Setting of Conscious Incompetence We extend this task setting to include conscious incompetence. Given the same input, each sentence    $s$   in the output text  $t$   can map to a Not Applicable Citation (we use [NA] to represent it) if it includes some knowledge to be verified, but the knowledge is absent in the knowledge graph    $G$  . A sentence can map to both [NA] and a list of sub-graph knowledge if it can be partially verified by the knowledge graph    $G$  . [NA] is not a citation on conventional means, but a indicator of knowledge gap.'), Document(metadata={'page_source': '1', 'doc_source': '2310.05634v2', 'start_index': 3851, '_id': '7f61014dce754fba9f1cd42fc935a42d', '_collection_name': 'test'}, page_content='Setting of Conscious Incompetence We extend this task setting to include conscious incompetence. Given the same input, each sentence    $s$   in the output text  $t$   can map to a Not Applicable Citation (we use [NA] to represent it) if it includes some knowledge to be verified, but the knowledge is absent in the knowledge graph    $G$  . A sentence can map to both [NA] and a list of sub-graph knowledge if it can be partially verified by the knowledge graph    $G$  . [NA] is not a citation on conventional means, but a indicator of knowledge gap.'), Document(metadata={'page_source': '1', 'doc_source': '2310.05634v2', 'start_index': 3851, '_id': '6e78c59f68e447779a6d5deb18c7b4a3', '_collection_name': 'test'}, page_content='Setting of Conscious Incompetence We extend this task setting to include conscious incompetence. Given the same input, each sentence    $s$   in the output text  $t$   can map to a Not Applicable Citation (we use [NA] to represent it) if it includes some knowledge to be verified, but the knowledge is absent in the knowledge graph    $G$  . A sentence can map to both [NA] and a list of sub-graph knowledge if it can be partially verified by the knowledge graph    $G$  . [NA] is not a citation on conventional means, but a indicator of knowledge gap.'), Document(metadata={'page_source': '1', 'doc_source': '2310.05634v2', 'start_index': 3851, '_id': 'eed8b3b2d535406ab949e3202da6efda', '_collection_name': 'test'}, page_content='Setting of Conscious Incompetence We extend this task setting to include conscious incompetence. Given the same input, each sentence    $s$   in the output text  $t$   can map to a Not Applicable Citation (we use [NA] to represent it) if it includes some knowledge to be verified, but the knowledge is absent in the knowledge graph    $G$  . A sentence can map to both [NA] and a list of sub-graph knowledge if it can be partially verified by the knowledge graph    $G$  . [NA] is not a citation on conventional means, but a indicator of knowledge gap.'), Document(metadata={'page_source': '1', 'doc_source': '2310.05634v2', 'start_index': 3851, '_id': '18a946b4609f403f986b399b8a7feaaa', '_collection_name': 'test'}, page_content='Setting of Conscious Incompetence We extend this task setting to include conscious incompetence. Given the same input, each sentence    $s$   in the output text  $t$   can map to a Not Applicable Citation (we use [NA] to represent it) if it includes some knowledge to be verified, but the knowledge is absent in the knowledge graph    $G$  . A sentence can map to both [NA] and a list of sub-graph knowledge if it can be partially verified by the knowledge graph    $G$  . [NA] is not a citation on conventional means, but a indicator of knowledge gap.'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 5134, '_id': '6750e5505fca47c8aa71258cbd8417e9', '_collection_name': 'test'}, page_content='4.4 Conscious Incompetence Evaluation \nTheoretically, each [NA] mark should map to a piece of knowledge absent from the retrieved knowledge graph. However, it is difficult to identify if sentence requires any absent knowledge since there is no ground truth. Therefore, we conduct a three-round experiment to manually create ground truth for absent knowledge. In round 1, we select one knowledge from the minimum knowledge set, and remove it from the ground-truth knowledge graph. We let the LLMs attribute to this incomplete knowledge graph to generate answers, whereby the removed knowledge forms the “absent knowledge ground truth”. In subsequent rounds, we each re- move one additional knowledge from the minimum knowledge set, simulating a knowledge graph with more serious coverage problem.'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 5134, '_id': '10bcc67976e84b30a08108d38be1b3e2', '_collection_name': 'test'}, page_content='4.4 Conscious Incompetence Evaluation \nTheoretically, each [NA] mark should map to a piece of knowledge absent from the retrieved knowledge graph. However, it is difficult to identify if sentence requires any absent knowledge since there is no ground truth. Therefore, we conduct a three-round experiment to manually create ground truth for absent knowledge. In round 1, we select one knowledge from the minimum knowledge set, and remove it from the ground-truth knowledge graph. We let the LLMs attribute to this incomplete knowledge graph to generate answers, whereby the removed knowledge forms the “absent knowledge ground truth”. In subsequent rounds, we each re- move one additional knowledge from the minimum knowledge set, simulating a knowledge graph with more serious coverage problem.'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 5134, '_id': 'bfceef1e09414638af1d31ee520216c5', '_collection_name': 'test'}, page_content='4.4 Conscious Incompetence Evaluation \nTheoretically, each [NA] mark should map to a piece of knowledge absent from the retrieved knowledge graph. However, it is difficult to identify if sentence requires any absent knowledge since there is no ground truth. Therefore, we conduct a three-round experiment to manually create ground truth for absent knowledge. In round 1, we select one knowledge from the minimum knowledge set, and remove it from the ground-truth knowledge graph. We let the LLMs attribute to this incomplete knowledge graph to generate answers, whereby the removed knowledge forms the “absent knowledge ground truth”. In subsequent rounds, we each re- move one additional knowledge from the minimum knowledge set, simulating a knowledge graph with more serious coverage problem.'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 5134, '_id': '133b12bfa37e45a4b6c313d265d22416', '_collection_name': 'test'}, page_content='4.4 Conscious Incompetence Evaluation \nTheoretically, each [NA] mark should map to a piece of knowledge absent from the retrieved knowledge graph. However, it is difficult to identify if sentence requires any absent knowledge since there is no ground truth. Therefore, we conduct a three-round experiment to manually create ground truth for absent knowledge. In round 1, we select one knowledge from the minimum knowledge set, and remove it from the ground-truth knowledge graph. We let the LLMs attribute to this incomplete knowledge graph to generate answers, whereby the removed knowledge forms the “absent knowledge ground truth”. In subsequent rounds, we each re- move one additional knowledge from the minimum knowledge set, simulating a knowledge graph with more serious coverage problem.'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 5134, '_id': '690f1f6a73e74f31abeffb94556d67aa', '_collection_name': 'test'}, page_content='4.4 Conscious Incompetence Evaluation \nTheoretically, each [NA] mark should map to a piece of knowledge absent from the retrieved knowledge graph. However, it is difficult to identify if sentence requires any absent knowledge since there is no ground truth. Therefore, we conduct a three-round experiment to manually create ground truth for absent knowledge. In round 1, we select one knowledge from the minimum knowledge set, and remove it from the ground-truth knowledge graph. We let the LLMs attribute to this incomplete knowledge graph to generate answers, whereby the removed knowledge forms the “absent knowledge ground truth”. In subsequent rounds, we each re- move one additional knowledge from the minimum knowledge set, simulating a knowledge graph with more serious coverage problem.'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 2547, '_id': '2a20479f05ca416aad95c82d69e97a86', '_collection_name': 'test'}, page_content='- **Model Output**: It shows two sentences with various knowledge elements marked by k1, k3, k6, etc., and tagged with [NA] for those marked as ""conscious incompetence.""\n  - Sentence1 has k1 tagged with [NA].\n  - Sentence2 has k3 and k6 tagged with [NA].\n  - Sentence3, absent of [NA], has elements k6 and k9.\n\n- **Absent Knowledge Set**: It includes knowledge elements k2, k4, and k5 that are associated with the absence of knowledge.\n\n- **NLI (Natural Language Inference) Evaluation**:\n  - **[NA] Precision**: Evaluates the accuracy of [NA] tagging in sentences when compared to the absent knowledge set.\n    - Sentence1\'s correct identification of absent knowledge (k2) achieves 1 out of 2 attempted, resulting in a precision of 0.5.\n  - **[NA] Recall**: Measures how effectively the model identified all items in the absent knowledge set as [NA].\n    - Sentence2\'s hit of k2 against an all of 3 absence marks results in a recall of 0.33.'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 2547, '_id': 'f1b826cfd235417687bfe8e662814273', '_collection_name': 'test'}, page_content='- **Model Output**: It shows two sentences with various knowledge elements marked by k1, k3, k6, etc., and tagged with [NA] for those marked as ""conscious incompetence.""\n  - Sentence1 has k1 tagged with [NA].\n  - Sentence2 has k3 and k6 tagged with [NA].\n  - Sentence3, absent of [NA], has elements k6 and k9.\n\n- **Absent Knowledge Set**: It includes knowledge elements k2, k4, and k5 that are associated with the absence of knowledge.\n\n- **NLI (Natural Language Inference) Evaluation**:\n  - **[NA] Precision**: Evaluates the accuracy of [NA] tagging in sentences when compared to the absent knowledge set.\n    - Sentence1\'s correct identification of absent knowledge (k2) achieves 1 out of 2 attempted, resulting in a precision of 0.5.\n  - **[NA] Recall**: Measures how effectively the model identified all items in the absent knowledge set as [NA].\n    - Sentence2\'s hit of k2 against an all of 3 absence marks results in a recall of 0.33.'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 2547, '_id': '5fd62e217a9840ac94fd19b1bdc1d767', '_collection_name': 'test'}, page_content='- **Model Output**: It shows two sentences with various knowledge elements marked by k1, k3, k6, etc., and tagged with [NA] for those marked as ""conscious incompetence.""\n  - Sentence1 has k1 tagged with [NA].\n  - Sentence2 has k3 and k6 tagged with [NA].\n  - Sentence3, absent of [NA], has elements k6 and k9.\n\n- **Absent Knowledge Set**: It includes knowledge elements k2, k4, and k5 that are associated with the absence of knowledge.\n\n- **NLI (Natural Language Inference) Evaluation**:\n  - **[NA] Precision**: Evaluates the accuracy of [NA] tagging in sentences when compared to the absent knowledge set.\n    - Sentence1\'s correct identification of absent knowledge (k2) achieves 1 out of 2 attempted, resulting in a precision of 0.5.\n  - **[NA] Recall**: Measures how effectively the model identified all items in the absent knowledge set as [NA].\n    - Sentence2\'s hit of k2 against an all of 3 absence marks results in a recall of 0.33.'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 2547, '_id': '47d7d52d9cfc44e5a18d97c635c64efe', '_collection_name': 'test'}, page_content='- **Model Output**: It shows two sentences with various knowledge elements marked by k1, k3, k6, etc., and tagged with [NA] for those marked as ""conscious incompetence.""\n  - Sentence1 has k1 tagged with [NA].\n  - Sentence2 has k3 and k6 tagged with [NA].\n  - Sentence3, absent of [NA], has elements k6 and k9.\n\n- **Absent Knowledge Set**: It includes knowledge elements k2, k4, and k5 that are associated with the absence of knowledge.\n\n- **NLI (Natural Language Inference) Evaluation**:\n  - **[NA] Precision**: Evaluates the accuracy of [NA] tagging in sentences when compared to the absent knowledge set.\n    - Sentence1\'s correct identification of absent knowledge (k2) achieves 1 out of 2 attempted, resulting in a precision of 0.5.\n  - **[NA] Recall**: Measures how effectively the model identified all items in the absent knowledge set as [NA].\n    - Sentence2\'s hit of k2 against an all of 3 absence marks results in a recall of 0.33.'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 2547, '_id': '00ed04d796ec43ecb480b0894b79200c', '_collection_name': 'test'}, page_content='- **Model Output**: It shows two sentences with various knowledge elements marked by k1, k3, k6, etc., and tagged with [NA] for those marked as ""conscious incompetence.""\n  - Sentence1 has k1 tagged with [NA].\n  - Sentence2 has k3 and k6 tagged with [NA].\n  - Sentence3, absent of [NA], has elements k6 and k9.\n\n- **Absent Knowledge Set**: It includes knowledge elements k2, k4, and k5 that are associated with the absence of knowledge.\n\n- **NLI (Natural Language Inference) Evaluation**:\n  - **[NA] Precision**: Evaluates the accuracy of [NA] tagging in sentences when compared to the absent knowledge set.\n    - Sentence1\'s correct identification of absent knowledge (k2) achieves 1 out of 2 attempted, resulting in a precision of 0.5.\n  - **[NA] Recall**: Measures how effectively the model identified all items in the absent knowledge set as [NA].\n    - Sentence2\'s hit of k2 against an all of 3 absence marks results in a recall of 0.33.')]"
10006,"In figure 4, which nodes are retrieved by RAPTOR for both questions?","[Document(metadata={'page_source': '6', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': 'a384f3b42699438ab68b978baab4791b', '_collection_name': 'test'}, page_content='The image is an illustration of the querying process by RAPTOR, a system for retrieving information. It shows how RAPTOR retrieves information for two questions about the Cinderella story. The diagram features nodes and arrows with different colors representing selections by RAPTOR and DPR (Dense Passage Retrieval). The nodes are arranged in a hierarchical structure with numbers, and the highlighted nodes indicate RAPTOR\'s selections, differentiated for two distinct questions: ""What is the central theme of the story?"" and ""How did Cinderella find a happy ending?"".\n\nKey components:\n\n- Orange and purple highlighted nodes: Indicate RAPTOR\'s selections for Question 1 and Question 2.\n- Arrows: Point to DPR’s leaf nodes for each question, with orange arrows for Question 1 and purple arrows for Question 2.\n- RAPTOR\'s context is shown to often encompass the information retrieved by DPR.'), Document(metadata={'page_source': '1', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '8cb93675a3fa4506b1d10fd773541bce', '_collection_name': 'test'}, page_content=""The image illustrates the tree construction process used by RAPTOR to cluster and summarize text. Here's a breakdown of its components:\n\n1. **RAPTOR Tree (Left Panel):**\n   - **Root Layer:** Contains nodes (9 and 10).\n   - **Leaf Layer:** Contains nodes (1 to 5).\n   - Clustering of nodes forms a hierarchical structure from the leaf layer up to the root.\n\n2. **Formation of One Tree Layer (Middle Panel):**\n   - **Step 1: Clustering** - Text chunks (1 through 5) are clustered based on their embeddings.\n   - **Step 2: Summarization by LLM** - Clusters (such as those formed by nodes 2 and 3) are summarized to create another layer in the tree (nodes 6, 7, 8).\n\n3. **Contents of a Node (Right Panel):**\n   - **Index:** Identifies the node (e.g., #8).\n   - **Child Nodes:** Lists nodes contained within a cluster (e.g., 2, 3).\n   - **Text Summary:** Provides a summary of the cluster's contents.\n   - **Text Embedding:** Displays the vector representation used for clustering.""), Document(metadata={'page_source': '18', 'doc_source': '2401.18059v1', 'start_index': 2109, '_id': '1f1240f54ebb4119bca65926342f9eb6', '_collection_name': 'test'}, page_content='2. **Question: How does Cinderella find a happy ending?**\n   - **RAPTOR**: Details the transformation magic, Cinderella impressing the Prince, her need to leave by eleven, and the Prince finding her through the glass slipper. It mentions her forgiveness.\n   - **DPR**: Focuses on the Prince searching for Cinderella, the Fairy transforming her rags back into a gown, and the creation of the glass slippers. It emphasizes the necessity to leave by eleven to maintain the magic.\nquestion at hand. Further, the information that would be retrieved by DPR is more often than not included in the context retrieved by RAPTOR, either directly as a leaf node or indirectly as part of a summary from a higher layer.'), Document(metadata={'page_source': '20', 'doc_source': '2401.18059v1', 'start_index': 2127, '_id': '2788f6b3d7fe4f9d8401b96cb8c582a6', '_collection_name': 'test'}, page_content='Table 14: Performance of RAPTOR when querying different layers of the tree for Story 2. \nThe table presents data on a multi-layered structure with three rows and four columns. The columns represent ""Layers Queried / Start Layer,"" ""Layer 0 (Leaf Nodes),"" ""Layer 1,"" and ""Layer 2."" The rows are labeled as ""1 layer,"" ""2 layers,"" and ""3 layers,"" likely indicating the number of layers queried or starting layers in some hierarchy or system.'), Document(metadata={'page_source': '3', 'doc_source': '2401.18059v1', 'start_index': 4727, '_id': 'a0b5ca33bbfb44158471dbee115ac13e', '_collection_name': 'test'}, page_content='1. Start at the root layer of the RAPTOR tree. Compute the cosine similarity between the query embedding and the embeddings of all nodes present at this initial layer. 2. Choose the top-  $\\cdot k$   nodes based on the highest cosine similarity scores, forming the set  $S_{1}$  .'), Document(metadata={'page_source': '3', 'doc_source': '2401.18059v1', 'start_index': 3826, '_id': '97acd1da24a24ee8a786b079fa0c91ab', '_collection_name': 'test'}, page_content='Querying In this section, we elaborate on the two querying mechanisms employed by RAPTOR: tree traversal and collapsed tree. These methods offer unique ways of traversing the multi-layered RAPTOR tree to retrieve relevant information, each with its own advantages and trade-offs. We provide the pseudocode of both methods in Appendix  F . Note that we embed all nodes using SBERT. \nThe  tree traversal  method first selects the top-k most relevant root nodes based on their cosine similarity to the query embedding. The children of these selected nodes are considered at the next layer and the top  $\\cdot\\mathrm{k}$   nodes are selected from this pool again based on their cosine similarity to the query vector. This process is repeated until we reach the leaf nodes. Finally, the text from all selected nodes is concatenated to form the retrieved context. The algorithm’s steps are outlined below:'), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 2606, '_id': 'ce88e64b67e44615a0c33c7735773cd6', '_collection_name': 'test'}, page_content='- For ""1 layer"":\n  - Layer 0 (Leaf Nodes): 57.9\n  - Layer 1: 47.3\n  \n- For ""2 layers"":\n  - Layer 0 (Leaf Nodes): Not applicable or missing\n  - Layer 1: 68.4 (in bold)\nI.2 W HICH  L AYERS DO  R ETRIEVED  N ODES COME FROM  ? \nWe further conduct an ablation study across all three datasets and across three different retrievers with RAPTOR with the collapsed tree retrieval to examine the layers from which the retrieved nodes originate. We observe that between  $18.5\\%$   to  $57\\%$   of the retrieved nodes come from non-leaf nodes. As illustrated in Figure  7 , the retrieval pattern across layers reveals the importance of RAPTOR’s multi-layered tree structure. Notably, a significant percentage of the nodes retrieved by RAPTOR using the DPR retriever for the Narrative QA dataset come from the first and second layers of the tree, as opposed to the leaf nodes. This pattern is consistent across the other datasets and retrievers, albeit with varying percentages.'), Document(metadata={'page_source': '17', 'doc_source': '2401.18059v1', 'start_index': 2959, '_id': 'c927121369f24122970224b9ff662d77', '_collection_name': 'test'}, page_content='This algorithm likely performs a top-k selection of nodes at each layer of the tree according to some scoring system derived from the `dot_product` of nodes and a query, effectively filtering the tree traversed by prioritizing nodes with the highest scores.\nG Q UALITATIVE  A NALYSIS \nTo qualitatively examine RAPTOR’s retrieval process, we test it on thematic, multi-hop questions about a 1500-word version of the fairytale Cinderella. We compare the context retrieved by RAP- TOR with the context retrieved by Dense Passage Retrieval (DPR). Figure  4  in the main paper details the retrieval process within RAPTOR’s tree structure for two questions. The nodes that RAPTOR selects for each question are highlighted, while the leaf nodes that DPR selects for the same question are indicated with arrows. This comparison illustrates the advantage of RAPTOR’s tree structure. RAPTOR selects nodes from different layers depending on the level of granularity required by the'), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 924, '_id': 'e3f6a37c00f84138a18b530acf4c2c2f', '_collection_name': 'test'}, page_content=""The table presents data relating to three different queries or layers with corresponding values across three different columns labeled as Layer 0 (Leaf Nodes), Layer 1, and Layer 2. Here's a breakdown of the table contents:""), Document(metadata={'page_source': '18', 'doc_source': '2401.18059v1', 'start_index': 706, '_id': '4264e246f74246fe9cb2591dcce06d99', '_collection_name': 'test'}, page_content=""4. **Initializing Result and Token Count**: The algorithm initializes an empty list `result` and sets `total_tokens` to zero.\n\n5. **Iterating Over Top Nodes**: It then iterates over each node in the sorted `top_nodes` list and checks if adding the current node's token size to `total_tokens` would exceed `max_tokens`.\n\n   - If not, the node is appended to the `result` list.\n   - The `total_tokens` is updated by adding the current node's token size.\n\n6. **Return Result**: The function finally returns the `result` list, which presumably contains a subset of nodes from the original tree that maximize some criteria while keeping within a token limit.\n\nThe algorithm essentially collapses a tree into a prioritized list of nodes based on their relevance to a query, constrained by a token limit.\nThe table compares RAPTOR and DPR responses to two questions about the story of Cinderella:""), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '53ff9f7bafd94c41a8359a6aa2589abb', '_collection_name': 'test'}, page_content='The image displays three bar charts comparing the performance of three different models (SBERT, BM25, and DPR) across various tasks: NarrativeQA, Quality, and Qasper. Each chart shows the percentage of some metric across different layers (Layer 0, Layer 1, etc.). The colors represent different models: light blue for SBERT, green for BM25, and red for DPR. The percentage values decrease as the layer number increases across all tasks.\nFigure 7: Histogram showing the percentage of nodes retrieved from different layers of the RAPTOR tree across three datasets (Narrative QA, Quality, and Qasper) using three retrievers (SBERT, BM25, and DPR). The data indicate that a substantial portion of the nodes contributing to the final retrieval comes from non-leaf layers, with a notable percentage from the first and second layers, highlighting the importance of RAPTOR’s hierarchical sum mari z ation in the retrieval process.'), Document(metadata={'page_source': '8', 'doc_source': '2401.18059v1', 'start_index': 1422, '_id': '758016051d5d41689db20e902f9688ca', '_collection_name': 'test'}, page_content='3. CoLISA (DeBERTaV3-large) (Dong et al., 2023a)\n   - Test Set Accuracy: 62.3\n   - Hard Subset Accuracy: 54.7\n\n4. RAPTOR + GPT-4\n   - Test Set Accuracy: 82.6\n   - Hard Subset Accuracy: 76.2\n\nThe RAPTOR + GPT-4 model achieves the highest accuracy on both the Test Set and the Hard Subset compared to the other models listed.\nThe table presents data on different layers and their corresponding numeric values under various conditions. It consists of three main columns after the initial descriptive column:\n\n1. **Layers Queried / Start Layer**: This column lists the number of layers queried or the start layer for each row.\n   - 1 layer\n   - 2 layers\n   - 3 layers\n\n2. **Layer 0 (Leaf Nodes)**: This column provides the values associated with Layer 0 for different queries:\n   - For 1 layer: 57.9\n   - No data is provided for 2 layers and 3 layers scenarios for Layer 0.'), Document(metadata={'page_source': '2', 'doc_source': '2401.18059v1', 'start_index': 1394, '_id': '754914658a72485b8edc74d9abf635ac', '_collection_name': 'test'}, page_content='nodes thus storing varying levels of detail, keeping granular details. However, both methods, due to their reliance on adjacency for grouping or summarizing adjacent nodes, may still overlook distant interdependencies within the text, which we can find and group with RAPTOR.'), Document(metadata={'page_source': '8', 'doc_source': '2401.18059v1', 'start_index': 2692, '_id': '1cf7ac3e98b24fdf83aaf67afb761fac', '_collection_name': 'test'}, page_content='There is no additional caption or description provided with the table to give context to what these layers specifically refer to, but the format suggests a progression or calculation regarding multiple layers or stages within a system or model.\nWe validated this hypothesis both quantitatively and qualitatively. We present qualitative analysis in appendix  G . To quantitatively understand the contribution of the upper-level nodes, we used stories from the QuALITY dataset. The RAPTOR tree is built for each of these stories, as described in Section  3 . However, during retrieval, we limit the search to different subsets of layers. For example, we exclusively retrieve from the leaf nodes and each upper layer, as well as from different contiguous subsets of the layers. We show findings specific to one story in Table  8 , revealing that a full-tree search, utilizing all layers, outperformed retrieval strategies that focused only on specific layers.'), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 1149, '_id': '5498fd44698440e09d35d94c8a3e8ba4', '_collection_name': 'test'}, page_content='1. The first column is labeled ""Layers Queried / Start Layer"" and lists the different scenarios of the layers queried:\n   - 1 layer\n   - 2 layers\n   - 3 layers\n\n2. The second column, ""Layer 0 (Leaf Nodes),"" provides values for each of the scenarios:\n   - For ""1 layer,"" the value is 66.6.\n   - There is no value (indicated by a dash) for ""2 layers"" and ""3 layers.""\n\n3. The third column, ""Layer 1,"" presents values as follows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - There is no value (indicated by a dash) for ""3 layers.""\n\n4. The final column, ""Layer 2,"" shows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - For ""3 layers,"" the value is 83.3, which is also bolded for emphasis.')]"
10007,What are the horizontal and vertical axis of Figure 3 respectively?,"[Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 1149, '_id': '5498fd44698440e09d35d94c8a3e8ba4', '_collection_name': 'test'}, page_content='1. The first column is labeled ""Layers Queried / Start Layer"" and lists the different scenarios of the layers queried:\n   - 1 layer\n   - 2 layers\n   - 3 layers\n\n2. The second column, ""Layer 0 (Leaf Nodes),"" provides values for each of the scenarios:\n   - For ""1 layer,"" the value is 66.6.\n   - There is no value (indicated by a dash) for ""2 layers"" and ""3 layers.""\n\n3. The third column, ""Layer 1,"" presents values as follows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - There is no value (indicated by a dash) for ""3 layers.""\n\n4. The final column, ""Layer 2,"" shows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - For ""3 layers,"" the value is 83.3, which is also bolded for emphasis.'), Document(metadata={'page_source': '8', 'doc_source': '2401.18059v1', 'start_index': 2293, '_id': '33558f02c544404fa1b2042cc5f87eda', '_collection_name': 'test'}, page_content='3. **Layer 1**: This column provides the values associated with Layer 1:\n   - For 1 layer: 57.8\n   - For 2 layers: 52.6\n   - No data is provided for 3 layers scenario for Layer 1.\n\n4. **Layer 2**: This column provides the values associated with Layer 2:\n   - For 1 layer: 57.9\n   - For 2 layers: 63.15\n   - For 3 layers: 73.68 (which is in bold, possibly indicating a significant value or result).'), Document(metadata={'page_source': '14', 'doc_source': '2401.18059v1', 'start_index': 2556, '_id': '3c95baaaa06a45ba88f24e140e699e62', '_collection_name': 'test'}, page_content='1. **QASPER** (left plot):\n   - The plot uses red color to represent data.\n   - The horizontal axis shows document length in tokens, ranging from 0 to 30,000.\n   - The vertical axis shows the total number of tokens for both the prompt and completion, ranging from 0 to 40,000.\n   - The graph illustrates a linear increase in token cost as document length increases.\n\n2. **Narrative QA** (middle plot):\n   - The plot uses blue color to represent data.\n   - The horizontal axis shows document length, ranging from 0 to 400,000 tokens.\n   - The vertical axis shows the total number of tokens for both the prompt and completion, ranging from 0 to 600,000.\n   - The graph depicts a linear relationship, with token cost steadily increasing with document length.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '3052e2970a974ba6adf9e018f6e0710c', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '0c0a742e33b44860915b53d79b0d1f67', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '18cc6bf5b1824e29ae65cdaaee677cc0', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': 'ebb3238aeedc41fe8e360783679cdb7e', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '708b8bfda47042bcacfcb11fd496b91e', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '20', 'doc_source': '2401.18059v1', 'start_index': 2566, '_id': '1567924ffdb647df8cae4e0f2d0a7448', '_collection_name': 'test'}, page_content='- For ""1 layer,"" the values are 58.8 for Layer 0, 47.1 for Layer 1, and 41.1 for Layer 2.\n- For ""2 layers,"" no value is given for Layer 0, but Layer 1 has a value of 64.7, and Layer 2 is at 52.9.\n- For ""3 layers,"" no values are given for Layer 0 and Layer 1, but Layer 2 is at 47.1.\n\nThis table seems to depict some kind of measurement or metric across different layers of a structure, where Layer 0 is referred to as ""Leaf Nodes."" The values could represent percentages, scores, or any other relevant metric, but the table does not provide further context on what these numbers specifically represent.'), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 924, '_id': 'e3f6a37c00f84138a18b530acf4c2c2f', '_collection_name': 'test'}, page_content=""The table presents data relating to three different queries or layers with corresponding values across three different columns labeled as Layer 0 (Leaf Nodes), Layer 1, and Layer 2. Here's a breakdown of the table contents:""), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 1716, '_id': 'cf2e8e2fc4254465be0fc9bdbf6157f0', '_collection_name': 'test'}, page_content='4. The final column, ""Layer 2,"" shows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - For ""3 layers,"" the value is 83.3, which is also bolded for emphasis.\n\nThe table seems to depict performance metrics or results specific to each layer query scenario across the three layers mentioned.\nThe table shows data related to layers queried or start layers across two main columns: ""Layer 0 (Leaf Nodes)"" and ""Layer 1"". It contains two rows that describe:\n\n1. For ""1 layer"" queried or started:\n   - The value in ""Layer 0 (Leaf Nodes)"" is 94.7.\n   - The value in ""Layer 1"" is 84.2.\n\n2. For ""2 layers"" queried or started:\n   - There is no value for ""Layer 0 (Leaf Nodes)"" (indicated by \'-\').\n   - The value in ""Layer 1"" is 89.4.\nThe table shows data for different layers queried or start layers, with respective values for ""Layer 0 (Leaf Nodes)"" and ""Layer 1"":'), Document(metadata={'page_source': '16', 'doc_source': '2401.18059v1', 'start_index': 1901, '_id': '5f94b11d14264cdeb3a90977269b7e96', '_collection_name': 'test'}, page_content='5. **Avg. Compression Ratio (%)**: Represents the average compression ratio in percentage for each dataset. The ratios are:\n   - All Datasets: 28%\n   - QuALITY: 28%\n   - NarrativeQA: 27%\n   - QASPER: 35%\nD S UM MARI Z ATION  P ROMPT \nTable  11  shows the prompt used for sum mari z ation. \nThe table has two columns: ""Role"" and ""Content."" It has two rows:'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': 'a2ff907be4384f49b3e7114ffcf90875', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': '80f4128715084ef8b6c1c30e3de503e8', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': '5077b28c8d8d41eabf3b8e77a954949d', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.')]"
10008,"Based on ""PSEUDOCODE FOR RETRIEVAL METHODS"", which algorithm has more number of lines? (Give the algorithm name)","[Document(metadata={'page_source': '18', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '5773eb452b0843af8d5bfc7e17b5aed1', '_collection_name': 'test'}, page_content='The table contains pseudocode for an algorithm labeled as ""Algorithm 2: Collapsed Tree Algorithm"". The function `COLLAPSEDTREE` is designed to process a data structure, presumably a tree, based on several parameters: `tree`, `query`, `k`, and `max_tokens`. Here\'s a breakdown of the algorithm:\n\n1. **Flatten the Tree**: The tree is first flattened into a one-dimensional structure using a function `flatten(tree)`.\n\n2. **Calculate Dot Product**: For each node in the flattened tree, a dot product between the `query` and the `node` is computed and stored in a list `top_nodes` along with the node itself.\n\n3. **Sorting**: The `top_nodes` list is sorted, although it is not specified what it is sorted by.\n\n4. **Initializing Result and Token Count**: The algorithm initializes an empty list `result` and sets `total_tokens` to zero.'), Document(metadata={'page_source': '15', 'doc_source': '2401.18059v1', 'start_index': 2203, '_id': '751183f846544e5e853d7be1ecbb3ae9', '_collection_name': 'test'}, page_content='B.2 R ESULTS  & D ISCUSSION \nThe results of the ablation study are presented in table  9 . The results from this ablation study clearly indicate an improvement in accuracy when employing RAPTOR’s clustering mechanism over the recency-based tree approach. This finding substantiates our hypothesis that the clustering strategy in RAPTOR is more effective in capturing homogeneous content for sum mari z ation, thereby enhancing the overall retrieval performance.'), Document(metadata={'page_source': '17', 'doc_source': '2401.18059v1', 'start_index': 2959, '_id': 'c927121369f24122970224b9ff662d77', '_collection_name': 'test'}, page_content='This algorithm likely performs a top-k selection of nodes at each layer of the tree according to some scoring system derived from the `dot_product` of nodes and a query, effectively filtering the tree traversed by prioritizing nodes with the highest scores.\nG Q UALITATIVE  A NALYSIS \nTo qualitatively examine RAPTOR’s retrieval process, we test it on thematic, multi-hop questions about a 1500-word version of the fairytale Cinderella. We compare the context retrieved by RAP- TOR with the context retrieved by Dense Passage Retrieval (DPR). Figure  4  in the main paper details the retrieval process within RAPTOR’s tree structure for two questions. The nodes that RAPTOR selects for each question are highlighted, while the leaf nodes that DPR selects for the same question are indicated with arrows. This comparison illustrates the advantage of RAPTOR’s tree structure. RAPTOR selects nodes from different layers depending on the level of granularity required by the'), Document(metadata={'page_source': '8', 'doc_source': '2401.18059v1', 'start_index': 784, '_id': '9bd51463aaa449cd84b0b9dfe11cbf21', '_collection_name': 'test'}, page_content='4. **Retriever + Reader (Izacard & Grave, 2022)**\n   - ROUGE-L: 32.0\n   - BLEU-1: 35.3\n   - BLEU-4: 7.5\n   - METEOR: 11.1\n\n5. **RAPTOR + UnifiedQA**\n   - ROUGE-L: 30.8\n   - BLEU-1: 23.5\n   - BLEU-4: 6.4\n   - METEOR: 19.1\n\nThe best scores for each metric are bolded in the table.\nThe table presents the accuracy of different models on two datasets: the ""Test Set"" and the ""Hard Subset"". The models compared in the table are:\n\n1. Longformer-base (Beltagy et al., 2020)\n   - Test Set Accuracy: 39.5\n   - Hard Subset Accuracy: 35.3\n\n2. DPR and DeBERTaV3-large (Pang et al., 2022)\n   - Test Set Accuracy: 55.4\n   - Hard Subset Accuracy: 46.1\n\n3. CoLISA (DeBERTaV3-large) (Dong et al., 2023a)\n   - Test Set Accuracy: 62.3\n   - Hard Subset Accuracy: 54.7\n\n4. RAPTOR + GPT-4\n   - Test Set Accuracy: 82.6\n   - Hard Subset Accuracy: 76.2'), Document(metadata={'page_source': '4', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '58b166c194b7456f93d6ff256ce170fc', '_collection_name': 'test'}, page_content='The image illustrates two mechanisms for retrieving information using tree structures: Tree Traversal Retrieval and Collapsed Tree Retrieval.\n\n1. **Tree Traversal Retrieval (A)**:\n   - Starts with a query that is encoded into a vector.\n   - The tree structure is traversed starting from the root.\n   - At each level, it retrieves the top nodes based on cosine similarity to the query vector.\n   - Retrieves context from selected nodes which is then combined with the query to generate an answer using a language model (LLM).\n\n2. **Collapsed Tree Retrieval (B)**:\n   - Also starts with a query that is encoded.\n   - The tree is collapsed into a single layer.\n   - Nodes are retrieved based on cosine similarity until reaching a threshold number of tokens.\n   - The retrieved context is combined with the query to produce an answer using an LLM.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': 'f83c90f322654fedaaa45697ac763aa3', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': 'a2ff907be4384f49b3e7114ffcf90875', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': 'cbf9c1fe83e24a248ba2021bedcc9cdb', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': '5077b28c8d8d41eabf3b8e77a954949d', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': '80f4128715084ef8b6c1c30e3de503e8', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2401.18059v1', 'start_index': 3511, '_id': '5332ab1e2b1e4df7aa03feb38b9ad099', '_collection_name': 'test'}, page_content='The table appears to be comparing the performance of these models based on the F-1 Match metric.\na new state-of-the-art METEOR score. When compared to the recursively summarizing model by Wu et al.  ( 2021 ), which also employs UnifiedQA, RAPTOR outperforms it on all metrics. While Wu et al.  ( 2021 ) rely solely on the summary in the top root node of the tree structure, RAPTOR benefits from its intermediate layers and clustering approaches, which allows it to capture a range of information, from general themes to specific details, contributing to its overall strong performance. \n4.1 C ON TRI BUT ION OF THE TREE STRUCTURE \nWe examine the contribution of each layer of nodes to RAPTOR’s retrieval capabilities. We hy- pothesized that upper nodes play a crucial role in handling thematic or multi-hop queries requiring a broader understanding of the text.'), Document(metadata={'page_source': '6', 'doc_source': '2401.18059v1', 'start_index': 893, '_id': '510e5c6db21f42858a5af52b18552e2c', '_collection_name': 'test'}, page_content=""This diagram visualizes the differing layers of retrieval and how RAPTOR's higher-layer summaries sometimes include DPR's specific retrievals directly or through summarization.\nour results demonstrate that RAPTOR, when combined with any retriever, consistently outperforms the respective retriever across all datasets.   2""), Document(metadata={'page_source': '18', 'doc_source': '2401.18059v1', 'start_index': 706, '_id': '4264e246f74246fe9cb2591dcce06d99', '_collection_name': 'test'}, page_content=""4. **Initializing Result and Token Count**: The algorithm initializes an empty list `result` and sets `total_tokens` to zero.\n\n5. **Iterating Over Top Nodes**: It then iterates over each node in the sorted `top_nodes` list and checks if adding the current node's token size to `total_tokens` would exceed `max_tokens`.\n\n   - If not, the node is appended to the `result` list.\n   - The `total_tokens` is updated by adding the current node's token size.\n\n6. **Return Result**: The function finally returns the `result` list, which presumably contains a subset of nodes from the original tree that maximize some criteria while keeping within a token limit.\n\nThe algorithm essentially collapses a tree into a prioritized list of nodes based on their relevance to a query, constrained by a token limit.\nThe table compares RAPTOR and DPR responses to two questions about the story of Cinderella:""), Document(metadata={'page_source': '7', 'doc_source': '2401.18059v1', 'start_index': 470, '_id': 'aa55f5d31b554cddaae107882b917fda', '_collection_name': 'test'}, page_content='- **DPR with RAPTOR**\n  - Accuracy (QuALITY): 54.7%\n  - Answer F1 (QASPER): 32.23%\n\n- **DPR without RAPTOR**\n  - Accuracy (QuALITY): 53.1%\n  - Answer F1 (QASPER): 31.70%\n\nThe models are assessed with and without the RAPTOR component, showing differences in performance across the metrics.\nTable 3: Controlled comparison of F-1 scores on the QASPER dataset, using three different lan- guage models (GPT-3, GPT-4, UnifiedQA 3B) and various retrieval methods. The column ”Title  $^+$  Abstract” reflects performance when only the title and abstract of the papers are used for context. RAPTOR outperforms the established baselines BM25 and DPR across all tested language models. Specifically, RAPTOR’s F-1 scores are at least   $1.8\\%$   points higher than DPR and at least  $5.3\\%$   points higher than BM25. \nThe table shows F-1 Match scores for different retrievers when combined with different models: GPT-3, GPT-4, and UnifiedQA.'), Document(metadata={'page_source': '15', 'doc_source': '2401.18059v1', 'start_index': 1608, '_id': '64da18109ada4a1fa52be4891502adc7', '_collection_name': 'test'}, page_content='B.1 M ETHODOLOGY \nBoth configurations in this ablation study utilized SBERT embeddings and UnifiedQA to maintain consistency in retrieval. For RAPTOR, we employed our typical clustering and sum mari z ation process. In contrast, the alternative setup involved creating a balanced tree by recursively encoding and summarizing contiguous text chunks. We determined the window size for this setup based on the average cluster size observed in RAPTOR, which is approximately 6.7 nodes. Hence, we chose a window size of 7 nodes. The collapsed tree approach was applied for retrieval in both models. \nB.2 R ESULTS  & D ISCUSSION')]"
10009,"In Figure 1's demonstration, what are the color of the nodes that appear in more than one clusters?","[Document(metadata={'page_source': '1', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '8cb93675a3fa4506b1d10fd773541bce', '_collection_name': 'test'}, page_content=""The image illustrates the tree construction process used by RAPTOR to cluster and summarize text. Here's a breakdown of its components:\n\n1. **RAPTOR Tree (Left Panel):**\n   - **Root Layer:** Contains nodes (9 and 10).\n   - **Leaf Layer:** Contains nodes (1 to 5).\n   - Clustering of nodes forms a hierarchical structure from the leaf layer up to the root.\n\n2. **Formation of One Tree Layer (Middle Panel):**\n   - **Step 1: Clustering** - Text chunks (1 through 5) are clustered based on their embeddings.\n   - **Step 2: Summarization by LLM** - Clusters (such as those formed by nodes 2 and 3) are summarized to create another layer in the tree (nodes 6, 7, 8).\n\n3. **Contents of a Node (Right Panel):**\n   - **Index:** Identifies the node (e.g., #8).\n   - **Child Nodes:** Lists nodes contained within a cluster (e.g., 2, 3).\n   - **Text Summary:** Provides a summary of the cluster's contents.\n   - **Text Embedding:** Displays the vector representation used for clustering.""), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 924, '_id': 'e3f6a37c00f84138a18b530acf4c2c2f', '_collection_name': 'test'}, page_content=""The table presents data relating to three different queries or layers with corresponding values across three different columns labeled as Layer 0 (Leaf Nodes), Layer 1, and Layer 2. Here's a breakdown of the table contents:""), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 1149, '_id': '5498fd44698440e09d35d94c8a3e8ba4', '_collection_name': 'test'}, page_content='1. The first column is labeled ""Layers Queried / Start Layer"" and lists the different scenarios of the layers queried:\n   - 1 layer\n   - 2 layers\n   - 3 layers\n\n2. The second column, ""Layer 0 (Leaf Nodes),"" provides values for each of the scenarios:\n   - For ""1 layer,"" the value is 66.6.\n   - There is no value (indicated by a dash) for ""2 layers"" and ""3 layers.""\n\n3. The third column, ""Layer 1,"" presents values as follows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - There is no value (indicated by a dash) for ""3 layers.""\n\n4. The final column, ""Layer 2,"" shows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - For ""3 layers,"" the value is 83.3, which is also bolded for emphasis.'), Document(metadata={'page_source': '16', 'doc_source': '2401.18059v1', 'start_index': 1042, '_id': '141522788ab343069bde8771e33522cc', '_collection_name': 'test'}, page_content='1. **Dataset**: Lists the datasets analyzed, including ""All Datasets,"" ""QuALITY,"" ""NarrativeQA,"" and ""QASPER.""\n\n2. **Avg. Summary Length (tokens)**: Displays the average summary length in terms of tokens for each dataset. The average summary lengths are:\n   - All Datasets: 131 tokens\n   - QuALITY: 124.4 tokens\n   - NarrativeQA: 129.7 tokens\n   - QASPER: 145.9 tokens\n\n3. **Avg. Child Node Text Length (tokens)**: Shows the average text length in tokens for child nodes in each dataset. The average child node text lengths are:\n   - All Datasets: 85.6 tokens\n   - QuALITY: 87.9 tokens\n   - NarrativeQA: 85.5 tokens\n   - QASPER: 86.2 tokens\n\n4. **Avg. # of Child Nodes Per Parent**: Indicates the average number of child nodes per parent node in each dataset. The averages are:\n   - All Datasets: 6.7\n   - QuALITY: 5.7\n   - NarrativeQA: 6.8\n   - QASPER: 5.7'), Document(metadata={'page_source': '20', 'doc_source': '2401.18059v1', 'start_index': 2566, '_id': '1567924ffdb647df8cae4e0f2d0a7448', '_collection_name': 'test'}, page_content='- For ""1 layer,"" the values are 58.8 for Layer 0, 47.1 for Layer 1, and 41.1 for Layer 2.\n- For ""2 layers,"" no value is given for Layer 0, but Layer 1 has a value of 64.7, and Layer 2 is at 52.9.\n- For ""3 layers,"" no values are given for Layer 0 and Layer 1, but Layer 2 is at 47.1.\n\nThis table seems to depict some kind of measurement or metric across different layers of a structure, where Layer 0 is referred to as ""Leaf Nodes."" The values could represent percentages, scores, or any other relevant metric, but the table does not provide further context on what these numbers specifically represent.'), Document(metadata={'page_source': '3', 'doc_source': '2401.18059v1', 'start_index': 2837, '_id': '6102f63fcbe04f91b7633ef8897e3b13', '_collection_name': 'test'}, page_content='Model-Based Sum mari z ation After clustering the nodes using Gaussian Mixture Models, the nodes in each cluster are sent to a language model for sum mari z ation. This step allows the model to transform large chunks of text into concise, coherent summaries of the selected nodes. For our experiments, we use    $\\mathtt{g p t\\!-\\!3.5\\!-\\!t u r b o}$   to generate the summaries. The sum mari z ation step con- denses the potentially large volume of retrieved information into a manageable size. We provide statistics on the compression due to the sum mari z ation in Appendix  C  and the prompt used for sum mari z ation in Appendix  D . \nWhile the sum mari z ation model generally produces reliable summaries, a focused annotation study revealed that about   $4\\%$   of the summaries contained minor hallucinations. These did not propagate to parent nodes and had no discernible impact on question-answering tasks. For an in-depth analysis of hallucinations, refer to the appendix  E .'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 4155, '_id': '7f6f809d6bfd4933bee1ab611842c294', '_collection_name': 'test'}, page_content='- The row labeled ""0 (gold)"" has values 95.5 (Corr.), 30.1 (Prec.), 57.1 (Rec.), and 39.4 (F1.).\n- The row labeled ""1"" has values 94.1 (Corr.), 26.1 (Prec.), 42.5 (Rec.), and 32.3 (F1.).\n- The row labeled ""2"" has values 94.0 (Corr.), 21.0 (Prec.), 31.4 (Rec.), and 25.2 (F1.).\n- The row labeled ""3"" has values 93.9 (Corr.), 16.3 (Prec.), 20.4 (Rec.), and 18.1 (F1.).'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 4155, '_id': '9e777ed8044d4b449a63e45cdffd8e56', '_collection_name': 'test'}, page_content='- The row labeled ""0 (gold)"" has values 95.5 (Corr.), 30.1 (Prec.), 57.1 (Rec.), and 39.4 (F1.).\n- The row labeled ""1"" has values 94.1 (Corr.), 26.1 (Prec.), 42.5 (Rec.), and 32.3 (F1.).\n- The row labeled ""2"" has values 94.0 (Corr.), 21.0 (Prec.), 31.4 (Rec.), and 25.2 (F1.).\n- The row labeled ""3"" has values 93.9 (Corr.), 16.3 (Prec.), 20.4 (Rec.), and 18.1 (F1.).'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 4155, '_id': '8ccbb4b3a39a4e0f8440747d28b8350b', '_collection_name': 'test'}, page_content='- The row labeled ""0 (gold)"" has values 95.5 (Corr.), 30.1 (Prec.), 57.1 (Rec.), and 39.4 (F1.).\n- The row labeled ""1"" has values 94.1 (Corr.), 26.1 (Prec.), 42.5 (Rec.), and 32.3 (F1.).\n- The row labeled ""2"" has values 94.0 (Corr.), 21.0 (Prec.), 31.4 (Rec.), and 25.2 (F1.).\n- The row labeled ""3"" has values 93.9 (Corr.), 16.3 (Prec.), 20.4 (Rec.), and 18.1 (F1.).'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 4155, '_id': 'dba35a2413b3404d8216a02cb937929f', '_collection_name': 'test'}, page_content='- The row labeled ""0 (gold)"" has values 95.5 (Corr.), 30.1 (Prec.), 57.1 (Rec.), and 39.4 (F1.).\n- The row labeled ""1"" has values 94.1 (Corr.), 26.1 (Prec.), 42.5 (Rec.), and 32.3 (F1.).\n- The row labeled ""2"" has values 94.0 (Corr.), 21.0 (Prec.), 31.4 (Rec.), and 25.2 (F1.).\n- The row labeled ""3"" has values 93.9 (Corr.), 16.3 (Prec.), 20.4 (Rec.), and 18.1 (F1.).'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 4155, '_id': '7913b604c52f40aa9cfc72d34f585809', '_collection_name': 'test'}, page_content='- The row labeled ""0 (gold)"" has values 95.5 (Corr.), 30.1 (Prec.), 57.1 (Rec.), and 39.4 (F1.).\n- The row labeled ""1"" has values 94.1 (Corr.), 26.1 (Prec.), 42.5 (Rec.), and 32.3 (F1.).\n- The row labeled ""2"" has values 94.0 (Corr.), 21.0 (Prec.), 31.4 (Rec.), and 25.2 (F1.).\n- The row labeled ""3"" has values 93.9 (Corr.), 16.3 (Prec.), 20.4 (Rec.), and 18.1 (F1.).'), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 1716, '_id': 'cf2e8e2fc4254465be0fc9bdbf6157f0', '_collection_name': 'test'}, page_content='4. The final column, ""Layer 2,"" shows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - For ""3 layers,"" the value is 83.3, which is also bolded for emphasis.\n\nThe table seems to depict performance metrics or results specific to each layer query scenario across the three layers mentioned.\nThe table shows data related to layers queried or start layers across two main columns: ""Layer 0 (Leaf Nodes)"" and ""Layer 1"". It contains two rows that describe:\n\n1. For ""1 layer"" queried or started:\n   - The value in ""Layer 0 (Leaf Nodes)"" is 94.7.\n   - The value in ""Layer 1"" is 84.2.\n\n2. For ""2 layers"" queried or started:\n   - There is no value for ""Layer 0 (Leaf Nodes)"" (indicated by \'-\').\n   - The value in ""Layer 1"" is 89.4.\nThe table shows data for different layers queried or start layers, with respective values for ""Layer 0 (Leaf Nodes)"" and ""Layer 1"":'), Document(metadata={'page_source': '17', 'doc_source': '2401.18059v1', 'start_index': 2482, '_id': 'ae0343d74d364b44ba2900ff483d30f2', '_collection_name': 'test'}, page_content='5. **Select Top-k Nodes**:\n   - `S_layer` is assigned the top-k nodes from `sorted(top_k)[:k].nodes`, where `top_k` is sorted based on scores.\n\n6. **Update Current Layer**:\n   - `S_current` is updated to be the current set of selected top-k nodes, `S_layer`.\n\n7. **Return Statement**:\n   - The function returns the union of sets from all the layers traversed (`S_0 ∪ S_1 ∪ S_2 ... ∪ S_k`).\n\n8. **End of Function**: The function is concluded with an `end function` statement.'), Document(metadata={'page_source': '2', 'doc_source': '2401.18059v1', 'start_index': 4309, '_id': '8ccb0429f4d44047a5db6fa55f2e7096', '_collection_name': 'test'}, page_content='Our clustering algorithm is based on Gaussian Mixture Models (GMMs), an approach that offers both flexibility and a probabilistic framework. GMMs assume that data points are generated from a mixture of several Gaussian distributions.'), Document(metadata={'page_source': '8', 'doc_source': '2401.18059v1', 'start_index': 1422, '_id': '758016051d5d41689db20e902f9688ca', '_collection_name': 'test'}, page_content='3. CoLISA (DeBERTaV3-large) (Dong et al., 2023a)\n   - Test Set Accuracy: 62.3\n   - Hard Subset Accuracy: 54.7\n\n4. RAPTOR + GPT-4\n   - Test Set Accuracy: 82.6\n   - Hard Subset Accuracy: 76.2\n\nThe RAPTOR + GPT-4 model achieves the highest accuracy on both the Test Set and the Hard Subset compared to the other models listed.\nThe table presents data on different layers and their corresponding numeric values under various conditions. It consists of three main columns after the initial descriptive column:\n\n1. **Layers Queried / Start Layer**: This column lists the number of layers queried or the start layer for each row.\n   - 1 layer\n   - 2 layers\n   - 3 layers\n\n2. **Layer 0 (Leaf Nodes)**: This column provides the values associated with Layer 0 for different queries:\n   - For 1 layer: 57.9\n   - No data is provided for 2 layers and 3 layers scenarios for Layer 0.')]"
10010,"What model is the clustering algorithm of this paper based on, and what presents a challenge to it?","[Document(metadata={'page_source': '3', 'doc_source': '2401.18059v1', 'start_index': 1463, '_id': '2038410930494ad28582bde58da2a050', '_collection_name': 'test'}, page_content='Should a local cluster’s combined context ever exceed the sum mari z ation model’s token threshold, our algorithm recursively applies clustering within the cluster, ensuring that the context remains within the token threshold. \nTo determine the optimal number of clusters, we employ the Bayesian Information Criterion (BIC) for model selection. BIC not only penalizes model complexity but also rewards goodness of fit ( Schwarz ,  1978 ). The BIC for a given GMM is    $B I C=\\ln(N)k-2\\ln(\\hat{L})$  , where    $N$   is the number of text segments (or data points),    $k$   is the number of model parameters, and  $\\hat{L}$   is the maximized value of the likelihood function of the model. In the context of GMM, the number of parameters  $k$  is a function of the dimensionality of the input vectors and the number of clusters.'), Document(metadata={'page_source': '2', 'doc_source': '2401.18059v1', 'start_index': 4309, '_id': '8ccb0429f4d44047a5db6fa55f2e7096', '_collection_name': 'test'}, page_content='Our clustering algorithm is based on Gaussian Mixture Models (GMMs), an approach that offers both flexibility and a probabilistic framework. GMMs assume that data points are generated from a mixture of several Gaussian distributions.'), Document(metadata={'page_source': '3', 'doc_source': '2401.18059v1', 'start_index': 2294, '_id': '26f0effccc0a47a5afc15119836c9bd0', '_collection_name': 'test'}, page_content='With the optimal number of clusters determined by BIC, the Expectation-Maximization algorithm is then used to estimate the GMM parameters, namely the means, co variances, and mixture weights. \nWhile the Gaussian assumption in GMMs may not perfectly align with the nature of text data, which often exhibits a sparse and skewed distribution, our empirical observations suggest that it offers an effective model for our purpose. We run an ablation comparing GMM Clustering with summarizing contiguous chunks and provide details in Appendix  B .'), Document(metadata={'page_source': '15', 'doc_source': '2401.18059v1', 'start_index': 2203, '_id': '751183f846544e5e853d7be1ecbb3ae9', '_collection_name': 'test'}, page_content='B.2 R ESULTS  & D ISCUSSION \nThe results of the ablation study are presented in table  9 . The results from this ablation study clearly indicate an improvement in accuracy when employing RAPTOR’s clustering mechanism over the recency-based tree approach. This finding substantiates our hypothesis that the clustering strategy in RAPTOR is more effective in capturing homogeneous content for sum mari z ation, thereby enhancing the overall retrieval performance.'), Document(metadata={'page_source': '1', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '8cb93675a3fa4506b1d10fd773541bce', '_collection_name': 'test'}, page_content=""The image illustrates the tree construction process used by RAPTOR to cluster and summarize text. Here's a breakdown of its components:\n\n1. **RAPTOR Tree (Left Panel):**\n   - **Root Layer:** Contains nodes (9 and 10).\n   - **Leaf Layer:** Contains nodes (1 to 5).\n   - Clustering of nodes forms a hierarchical structure from the leaf layer up to the root.\n\n2. **Formation of One Tree Layer (Middle Panel):**\n   - **Step 1: Clustering** - Text chunks (1 through 5) are clustered based on their embeddings.\n   - **Step 2: Summarization by LLM** - Clusters (such as those formed by nodes 2 and 3) are summarized to create another layer in the tree (nodes 6, 7, 8).\n\n3. **Contents of a Node (Right Panel):**\n   - **Index:** Identifies the node (e.g., #8).\n   - **Child Nodes:** Lists nodes contained within a cluster (e.g., 2, 3).\n   - **Text Summary:** Provides a summary of the cluster's contents.\n   - **Text Embedding:** Displays the vector representation used for clustering.""), Document(metadata={'page_source': '18', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '5773eb452b0843af8d5bfc7e17b5aed1', '_collection_name': 'test'}, page_content='The table contains pseudocode for an algorithm labeled as ""Algorithm 2: Collapsed Tree Algorithm"". The function `COLLAPSEDTREE` is designed to process a data structure, presumably a tree, based on several parameters: `tree`, `query`, `k`, and `max_tokens`. Here\'s a breakdown of the algorithm:\n\n1. **Flatten the Tree**: The tree is first flattened into a one-dimensional structure using a function `flatten(tree)`.\n\n2. **Calculate Dot Product**: For each node in the flattened tree, a dot product between the `query` and the `node` is computed and stored in a list `top_nodes` along with the node itself.\n\n3. **Sorting**: The `top_nodes` list is sorted, although it is not specified what it is sorted by.\n\n4. **Initializing Result and Token Count**: The algorithm initializes an empty list `result` and sets `total_tokens` to zero.'), Document(metadata={'page_source': '2', 'doc_source': '2401.18059v1', 'start_index': 2727, '_id': 'db0f0328f7b04e3ab299885072ef8c60', '_collection_name': 'test'}, page_content='To group similar text chunks, we employ a clustering algorithm. Once clustered, a Language Model is used to summarize the grouped texts. These summarized texts are then re-embedded, and the cycle of embedding, clustering, and sum mari z ation continues until further clustering becomes infeasible, resulting in a structured, multi-layered tree representation of the original documents. An important aspect of RAPTOR is its computational efficiency. The system scales linearly in terms of both build time and token expenditure, making it suitable for processing large and complex corpora. For a comprehensive discussion on RAPTOR’s s cal ability, please refer to the Appendix  A .'), Document(metadata={'page_source': '3', 'doc_source': '2401.18059v1', 'start_index': 608, '_id': '48279b9e9dcd42a28920fca6c4716760', '_collection_name': 'test'}, page_content='The high dimensionality of vector embeddings presents a challenge for traditional GMMs, as dis- tance metrics may behave poorly when used to measure similarity in high-dimensional spaces ( Ag- garwal et al. ,  2001 ). To mitigate this, we employ Uniform Manifold Approximation and Projection (UMAP), a manifold learning technique for dimensionality reduction ( McInnes et al. ,  2018 ). The number of nearest neighbors parameter,  n neighbors , in UMAP determines the balance between the preservation of local and global structures. Our algorithm varies  n neighbors  to create a hierar- chical clustering structure: it first identifies global clusters and then performs local clustering within these global clusters. This two-step clustering process captures a broad spectrum of relationships among the text data, from broad themes to specific details.'), Document(metadata={'page_source': '2', 'doc_source': '2401.18059v1', 'start_index': 3408, '_id': '4b9f0856bfca4b1c83d8c3719a636d86', '_collection_name': 'test'}, page_content='For querying within this tree, we introduce two distinct strategies: tree traversal and collapsed tree. The tree traversal method traverses the tree layer-by-layer, pruning and selecting the most relevant nodes at each level. The collapsed tree method evaluates nodes collectively across all layers to find the most relevant ones. \nClustering Algorithm Clustering plays a key role in building the RAPTOR tree, organizing text segments into cohesive groups. This step groups related content together, which helps the subse- quent retrieval process. \nOne of the unique aspects of our clustering approach is the use of soft clustering, where nodes can belong to multiple clusters without requiring a fixed number of clusters. This flexibility is essen- tial because individual text segments often contain information relevant to various topics, thereby warranting their inclusion in multiple summaries.'), Document(metadata={'page_source': '3', 'doc_source': '2401.18059v1', 'start_index': 2837, '_id': '6102f63fcbe04f91b7633ef8897e3b13', '_collection_name': 'test'}, page_content='Model-Based Sum mari z ation After clustering the nodes using Gaussian Mixture Models, the nodes in each cluster are sent to a language model for sum mari z ation. This step allows the model to transform large chunks of text into concise, coherent summaries of the selected nodes. For our experiments, we use    $\\mathtt{g p t\\!-\\!3.5\\!-\\!t u r b o}$   to generate the summaries. The sum mari z ation step con- denses the potentially large volume of retrieved information into a manageable size. We provide statistics on the compression due to the sum mari z ation in Appendix  C  and the prompt used for sum mari z ation in Appendix  D . \nWhile the sum mari z ation model generally produces reliable summaries, a focused annotation study revealed that about   $4\\%$   of the summaries contained minor hallucinations. These did not propagate to parent nodes and had no discernible impact on question-answering tasks. For an in-depth analysis of hallucinations, refer to the appendix  E .'), Document(metadata={'page_source': '18', 'doc_source': '2401.18059v1', 'start_index': 706, '_id': '4264e246f74246fe9cb2591dcce06d99', '_collection_name': 'test'}, page_content=""4. **Initializing Result and Token Count**: The algorithm initializes an empty list `result` and sets `total_tokens` to zero.\n\n5. **Iterating Over Top Nodes**: It then iterates over each node in the sorted `top_nodes` list and checks if adding the current node's token size to `total_tokens` would exceed `max_tokens`.\n\n   - If not, the node is appended to the `result` list.\n   - The `total_tokens` is updated by adding the current node's token size.\n\n6. **Return Result**: The function finally returns the `result` list, which presumably contains a subset of nodes from the original tree that maximize some criteria while keeping within a token limit.\n\nThe algorithm essentially collapses a tree into a prioritized list of nodes based on their relevance to a query, constrained by a token limit.\nThe table compares RAPTOR and DPR responses to two questions about the story of Cinderella:""), Document(metadata={'page_source': '17', 'doc_source': '2401.18059v1', 'start_index': 2482, '_id': 'ae0343d74d364b44ba2900ff483d30f2', '_collection_name': 'test'}, page_content='5. **Select Top-k Nodes**:\n   - `S_layer` is assigned the top-k nodes from `sorted(top_k)[:k].nodes`, where `top_k` is sorted based on scores.\n\n6. **Update Current Layer**:\n   - `S_current` is updated to be the current set of selected top-k nodes, `S_layer`.\n\n7. **Return Statement**:\n   - The function returns the union of sets from all the layers traversed (`S_0 ∪ S_1 ∪ S_2 ... ∪ S_k`).\n\n8. **End of Function**: The function is concluded with an `end function` statement.'), Document(metadata={'page_source': '4', 'doc_source': '2401.18059v1', 'start_index': 845, '_id': 'a8c32df27bf24acda07ac12f19cb07d2', '_collection_name': 'test'}, page_content='Nodes on which cosine similarity is performed are highlighted in both mechanisms.\n3. Proceed to the child nodes of the elements in set  $S_{1}$  . Compute the cosine similarity between the query vector and the vector embeddings of these child nodes. 4. Select the top  $k$   child nodes with the highest cosine similarity scores to the query, forming the set    $S_{2}$  . 5. Continue this process recursively for  $d$   layers, producing sets  $S_{1},S_{2},.\\,.\\,.\\,,S_{d}$  . 6. Concatenate sets    $S_{1}$   through  $S_{d}$   to assemble the relevant context to the query. \nBy adjusting the depth    $d$   and the number of nodes  $k$   selected at each layer, the tree traversal method offers control over the specificity and breadth of the information retrieved. The algorithm starts with a broad outlook by considering the top layers of the tree and progressively focuses on finer details as it descends through the lower layers.'), Document(metadata={'page_source': '2', 'doc_source': '2401.18059v1', 'start_index': 1394, '_id': '754914658a72485b8edc74d9abf635ac', '_collection_name': 'test'}, page_content='nodes thus storing varying levels of detail, keeping granular details. However, both methods, due to their reliance on adjacency for grouping or summarizing adjacent nodes, may still overlook distant interdependencies within the text, which we can find and group with RAPTOR.'), Document(metadata={'page_source': '17', 'doc_source': '2401.18059v1', 'start_index': 905, '_id': '4f74f284dead4c3fa8d836bcce39f3bb', '_collection_name': 'test'}, page_content='The hallucination here is that the summary states that Jr. Ajor and Co-Tan are sisters, but does not explicitly mention or imply this. \nUpon reviewing all parent nodes, we found that hallucinations did not propagate to higher layers. Generally, the hallucinations were minor and did not alter the thematic interpretation of the text. \nE.3 I MPACT ON  QA T ASKS \nIn our findings, hallucinations had no discernible impact on the performance of QA tasks. This sug- gests that hallucination is not a major concerns for the sum mari z ation component in our RAPTOR architecture. \nF P SEUDOCODE FOR  R ETRIEVAL  M ETHODS \nThe table contains a pseudocode listing for an algorithm titled ""Algorithm 1: Tree Traversal Algorithm."" This algorithm is designed to traverse a tree structure. Here is a brief explanation of the pseudocode:')]"
10011,Write down the pseudo code from appendix that corresponds to step 5 of the tree traversal method,"[Document(metadata={'page_source': '18', 'doc_source': '2401.18059v1', 'start_index': 706, '_id': '4264e246f74246fe9cb2591dcce06d99', '_collection_name': 'test'}, page_content=""4. **Initializing Result and Token Count**: The algorithm initializes an empty list `result` and sets `total_tokens` to zero.\n\n5. **Iterating Over Top Nodes**: It then iterates over each node in the sorted `top_nodes` list and checks if adding the current node's token size to `total_tokens` would exceed `max_tokens`.\n\n   - If not, the node is appended to the `result` list.\n   - The `total_tokens` is updated by adding the current node's token size.\n\n6. **Return Result**: The function finally returns the `result` list, which presumably contains a subset of nodes from the original tree that maximize some criteria while keeping within a token limit.\n\nThe algorithm essentially collapses a tree into a prioritized list of nodes based on their relevance to a query, constrained by a token limit.\nThe table compares RAPTOR and DPR responses to two questions about the story of Cinderella:""), Document(metadata={'page_source': '17', 'doc_source': '2401.18059v1', 'start_index': 1731, '_id': '9cd882e5d1cf45448b6bd9da57e6bf9b', '_collection_name': 'test'}, page_content='1. **Function Definition**: The function `TRAVERSETREE` takes three arguments: a `tree`, a `query`, and a parameter `k`.\n\n2. **Initialization**: \n   - `S_current` is initialized to the first layer of the tree using `tree.layer[0]`.\n\n3. **Loop through Tree Layers**: \n   - A `for` loop iterates over the number of layers in the tree using `range(tree.num_layers)`.\n\n4. **Processing Nodes in Current Layer**:\n   - An empty list `top_k` is initialized to keep track of the top-k nodes in each layer.\n   - Another loop iterates over each `node` in `S_current`, computing a `score` using the `dot_product` function with `query` and `node` as inputs.\n   - Each node along with its computed score is appended to the `top_k` list as a tuple `(node, score)`.\n\n5. **Select Top-k Nodes**:\n   - `S_layer` is assigned the top-k nodes from `sorted(top_k)[:k].nodes`, where `top_k` is sorted based on scores.'), Document(metadata={'page_source': '18', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '5773eb452b0843af8d5bfc7e17b5aed1', '_collection_name': 'test'}, page_content='The table contains pseudocode for an algorithm labeled as ""Algorithm 2: Collapsed Tree Algorithm"". The function `COLLAPSEDTREE` is designed to process a data structure, presumably a tree, based on several parameters: `tree`, `query`, `k`, and `max_tokens`. Here\'s a breakdown of the algorithm:\n\n1. **Flatten the Tree**: The tree is first flattened into a one-dimensional structure using a function `flatten(tree)`.\n\n2. **Calculate Dot Product**: For each node in the flattened tree, a dot product between the `query` and the `node` is computed and stored in a list `top_nodes` along with the node itself.\n\n3. **Sorting**: The `top_nodes` list is sorted, although it is not specified what it is sorted by.\n\n4. **Initializing Result and Token Count**: The algorithm initializes an empty list `result` and sets `total_tokens` to zero.'), Document(metadata={'page_source': '17', 'doc_source': '2401.18059v1', 'start_index': 2482, '_id': 'ae0343d74d364b44ba2900ff483d30f2', '_collection_name': 'test'}, page_content='5. **Select Top-k Nodes**:\n   - `S_layer` is assigned the top-k nodes from `sorted(top_k)[:k].nodes`, where `top_k` is sorted based on scores.\n\n6. **Update Current Layer**:\n   - `S_current` is updated to be the current set of selected top-k nodes, `S_layer`.\n\n7. **Return Statement**:\n   - The function returns the union of sets from all the layers traversed (`S_0 ∪ S_1 ∪ S_2 ... ∪ S_k`).\n\n8. **End of Function**: The function is concluded with an `end function` statement.'), Document(metadata={'page_source': '17', 'doc_source': '2401.18059v1', 'start_index': 905, '_id': '4f74f284dead4c3fa8d836bcce39f3bb', '_collection_name': 'test'}, page_content='The hallucination here is that the summary states that Jr. Ajor and Co-Tan are sisters, but does not explicitly mention or imply this. \nUpon reviewing all parent nodes, we found that hallucinations did not propagate to higher layers. Generally, the hallucinations were minor and did not alter the thematic interpretation of the text. \nE.3 I MPACT ON  QA T ASKS \nIn our findings, hallucinations had no discernible impact on the performance of QA tasks. This sug- gests that hallucination is not a major concerns for the sum mari z ation component in our RAPTOR architecture. \nF P SEUDOCODE FOR  R ETRIEVAL  M ETHODS \nThe table contains a pseudocode listing for an algorithm titled ""Algorithm 1: Tree Traversal Algorithm."" This algorithm is designed to traverse a tree structure. Here is a brief explanation of the pseudocode:'), Document(metadata={'page_source': '1', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '8cb93675a3fa4506b1d10fd773541bce', '_collection_name': 'test'}, page_content=""The image illustrates the tree construction process used by RAPTOR to cluster and summarize text. Here's a breakdown of its components:\n\n1. **RAPTOR Tree (Left Panel):**\n   - **Root Layer:** Contains nodes (9 and 10).\n   - **Leaf Layer:** Contains nodes (1 to 5).\n   - Clustering of nodes forms a hierarchical structure from the leaf layer up to the root.\n\n2. **Formation of One Tree Layer (Middle Panel):**\n   - **Step 1: Clustering** - Text chunks (1 through 5) are clustered based on their embeddings.\n   - **Step 2: Summarization by LLM** - Clusters (such as those formed by nodes 2 and 3) are summarized to create another layer in the tree (nodes 6, 7, 8).\n\n3. **Contents of a Node (Right Panel):**\n   - **Index:** Identifies the node (e.g., #8).\n   - **Child Nodes:** Lists nodes contained within a cluster (e.g., 2, 3).\n   - **Text Summary:** Provides a summary of the cluster's contents.\n   - **Text Embedding:** Displays the vector representation used for clustering.""), Document(metadata={'page_source': '3', 'doc_source': '2401.18059v1', 'start_index': 3826, '_id': '97acd1da24a24ee8a786b079fa0c91ab', '_collection_name': 'test'}, page_content='Querying In this section, we elaborate on the two querying mechanisms employed by RAPTOR: tree traversal and collapsed tree. These methods offer unique ways of traversing the multi-layered RAPTOR tree to retrieve relevant information, each with its own advantages and trade-offs. We provide the pseudocode of both methods in Appendix  F . Note that we embed all nodes using SBERT. \nThe  tree traversal  method first selects the top-k most relevant root nodes based on their cosine similarity to the query embedding. The children of these selected nodes are considered at the next layer and the top  $\\cdot\\mathrm{k}$   nodes are selected from this pool again based on their cosine similarity to the query vector. This process is repeated until we reach the leaf nodes. Finally, the text from all selected nodes is concatenated to form the retrieved context. The algorithm’s steps are outlined below:'), Document(metadata={'page_source': '4', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '58b166c194b7456f93d6ff256ce170fc', '_collection_name': 'test'}, page_content='The image illustrates two mechanisms for retrieving information using tree structures: Tree Traversal Retrieval and Collapsed Tree Retrieval.\n\n1. **Tree Traversal Retrieval (A)**:\n   - Starts with a query that is encoded into a vector.\n   - The tree structure is traversed starting from the root.\n   - At each level, it retrieves the top nodes based on cosine similarity to the query vector.\n   - Retrieves context from selected nodes which is then combined with the query to generate an answer using a language model (LLM).\n\n2. **Collapsed Tree Retrieval (B)**:\n   - Also starts with a query that is encoded.\n   - The tree is collapsed into a single layer.\n   - Nodes are retrieved based on cosine similarity until reaching a threshold number of tokens.\n   - The retrieved context is combined with the query to produce an answer using an LLM.'), Document(metadata={'page_source': '20', 'doc_source': '2401.18059v1', 'start_index': 2127, '_id': '2788f6b3d7fe4f9d8401b96cb8c582a6', '_collection_name': 'test'}, page_content='Table 14: Performance of RAPTOR when querying different layers of the tree for Story 2. \nThe table presents data on a multi-layered structure with three rows and four columns. The columns represent ""Layers Queried / Start Layer,"" ""Layer 0 (Leaf Nodes),"" ""Layer 1,"" and ""Layer 2."" The rows are labeled as ""1 layer,"" ""2 layers,"" and ""3 layers,"" likely indicating the number of layers queried or starting layers in some hierarchy or system.'), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 1149, '_id': '5498fd44698440e09d35d94c8a3e8ba4', '_collection_name': 'test'}, page_content='1. The first column is labeled ""Layers Queried / Start Layer"" and lists the different scenarios of the layers queried:\n   - 1 layer\n   - 2 layers\n   - 3 layers\n\n2. The second column, ""Layer 0 (Leaf Nodes),"" provides values for each of the scenarios:\n   - For ""1 layer,"" the value is 66.6.\n   - There is no value (indicated by a dash) for ""2 layers"" and ""3 layers.""\n\n3. The third column, ""Layer 1,"" presents values as follows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - There is no value (indicated by a dash) for ""3 layers.""\n\n4. The final column, ""Layer 2,"" shows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - For ""3 layers,"" the value is 83.3, which is also bolded for emphasis.'), Document(metadata={'page_source': '3', 'doc_source': '2401.18059v1', 'start_index': 4727, '_id': 'a0b5ca33bbfb44158471dbee115ac13e', '_collection_name': 'test'}, page_content='1. Start at the root layer of the RAPTOR tree. Compute the cosine similarity between the query embedding and the embeddings of all nodes present at this initial layer. 2. Choose the top-  $\\cdot k$   nodes based on the highest cosine similarity scores, forming the set  $S_{1}$  .'), Document(metadata={'page_source': '2', 'doc_source': '2401.18059v1', 'start_index': 3408, '_id': '4b9f0856bfca4b1c83d8c3719a636d86', '_collection_name': 'test'}, page_content='For querying within this tree, we introduce two distinct strategies: tree traversal and collapsed tree. The tree traversal method traverses the tree layer-by-layer, pruning and selecting the most relevant nodes at each level. The collapsed tree method evaluates nodes collectively across all layers to find the most relevant ones. \nClustering Algorithm Clustering plays a key role in building the RAPTOR tree, organizing text segments into cohesive groups. This step groups related content together, which helps the subse- quent retrieval process. \nOne of the unique aspects of our clustering approach is the use of soft clustering, where nodes can belong to multiple clusters without requiring a fixed number of clusters. This flexibility is essen- tial because individual text segments often contain information relevant to various topics, thereby warranting their inclusion in multiple summaries.'), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 2606, '_id': 'ce88e64b67e44615a0c33c7735773cd6', '_collection_name': 'test'}, page_content='- For ""1 layer"":\n  - Layer 0 (Leaf Nodes): 57.9\n  - Layer 1: 47.3\n  \n- For ""2 layers"":\n  - Layer 0 (Leaf Nodes): Not applicable or missing\n  - Layer 1: 68.4 (in bold)\nI.2 W HICH  L AYERS DO  R ETRIEVED  N ODES COME FROM  ? \nWe further conduct an ablation study across all three datasets and across three different retrievers with RAPTOR with the collapsed tree retrieval to examine the layers from which the retrieved nodes originate. We observe that between  $18.5\\%$   to  $57\\%$   of the retrieved nodes come from non-leaf nodes. As illustrated in Figure  7 , the retrieval pattern across layers reveals the importance of RAPTOR’s multi-layered tree structure. Notably, a significant percentage of the nodes retrieved by RAPTOR using the DPR retriever for the Narrative QA dataset come from the first and second layers of the tree, as opposed to the leaf nodes. This pattern is consistent across the other datasets and retrievers, albeit with varying percentages.'), Document(metadata={'page_source': '8', 'doc_source': '2401.18059v1', 'start_index': 2692, '_id': '1cf7ac3e98b24fdf83aaf67afb761fac', '_collection_name': 'test'}, page_content='There is no additional caption or description provided with the table to give context to what these layers specifically refer to, but the format suggests a progression or calculation regarding multiple layers or stages within a system or model.\nWe validated this hypothesis both quantitatively and qualitatively. We present qualitative analysis in appendix  G . To quantitatively understand the contribution of the upper-level nodes, we used stories from the QuALITY dataset. The RAPTOR tree is built for each of these stories, as described in Section  3 . However, during retrieval, we limit the search to different subsets of layers. For example, we exclusively retrieve from the leaf nodes and each upper layer, as well as from different contiguous subsets of the layers. We show findings specific to one story in Table  8 , revealing that a full-tree search, utilizing all layers, outperformed retrieval strategies that focused only on specific layers.'), Document(metadata={'page_source': '8', 'doc_source': '2401.18059v1', 'start_index': 1422, '_id': '758016051d5d41689db20e902f9688ca', '_collection_name': 'test'}, page_content='3. CoLISA (DeBERTaV3-large) (Dong et al., 2023a)\n   - Test Set Accuracy: 62.3\n   - Hard Subset Accuracy: 54.7\n\n4. RAPTOR + GPT-4\n   - Test Set Accuracy: 82.6\n   - Hard Subset Accuracy: 76.2\n\nThe RAPTOR + GPT-4 model achieves the highest accuracy on both the Test Set and the Hard Subset compared to the other models listed.\nThe table presents data on different layers and their corresponding numeric values under various conditions. It consists of three main columns after the initial descriptive column:\n\n1. **Layers Queried / Start Layer**: This column lists the number of layers queried or the start layer for each row.\n   - 1 layer\n   - 2 layers\n   - 3 layers\n\n2. **Layer 0 (Leaf Nodes)**: This column provides the values associated with Layer 0 for different queries:\n   - For 1 layer: 57.9\n   - No data is provided for 2 layers and 3 layers scenarios for Layer 0.')]"
10012,"In the figure that has a tree shape, what is the name of the branch that has the least leafs?","[Document(metadata={'page_source': '8', 'doc_source': '2401.18059v1', 'start_index': 1422, '_id': '758016051d5d41689db20e902f9688ca', '_collection_name': 'test'}, page_content='3. CoLISA (DeBERTaV3-large) (Dong et al., 2023a)\n   - Test Set Accuracy: 62.3\n   - Hard Subset Accuracy: 54.7\n\n4. RAPTOR + GPT-4\n   - Test Set Accuracy: 82.6\n   - Hard Subset Accuracy: 76.2\n\nThe RAPTOR + GPT-4 model achieves the highest accuracy on both the Test Set and the Hard Subset compared to the other models listed.\nThe table presents data on different layers and their corresponding numeric values under various conditions. It consists of three main columns after the initial descriptive column:\n\n1. **Layers Queried / Start Layer**: This column lists the number of layers queried or the start layer for each row.\n   - 1 layer\n   - 2 layers\n   - 3 layers\n\n2. **Layer 0 (Leaf Nodes)**: This column provides the values associated with Layer 0 for different queries:\n   - For 1 layer: 57.9\n   - No data is provided for 2 layers and 3 layers scenarios for Layer 0.'), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 924, '_id': 'e3f6a37c00f84138a18b530acf4c2c2f', '_collection_name': 'test'}, page_content=""The table presents data relating to three different queries or layers with corresponding values across three different columns labeled as Layer 0 (Leaf Nodes), Layer 1, and Layer 2. Here's a breakdown of the table contents:""), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 3574, '_id': 'd718b2bd339d424397045c49674657cb', '_collection_name': 'test'}, page_content='Table 18: Percentage of nodes from non-leaf nodes across different datasets and retrievers \nThe table presents the performance of three different information retrieval models—DPR, SBERT, and BM25—across three datasets: NarrativeQA, Quality, and Qasper. The values in the table represent the performance percentages of each model on each dataset. Here is the detailed breakdown:'), Document(metadata={'page_source': '17', 'doc_source': '2401.18059v1', 'start_index': 1731, '_id': '9cd882e5d1cf45448b6bd9da57e6bf9b', '_collection_name': 'test'}, page_content='1. **Function Definition**: The function `TRAVERSETREE` takes three arguments: a `tree`, a `query`, and a parameter `k`.\n\n2. **Initialization**: \n   - `S_current` is initialized to the first layer of the tree using `tree.layer[0]`.\n\n3. **Loop through Tree Layers**: \n   - A `for` loop iterates over the number of layers in the tree using `range(tree.num_layers)`.\n\n4. **Processing Nodes in Current Layer**:\n   - An empty list `top_k` is initialized to keep track of the top-k nodes in each layer.\n   - Another loop iterates over each `node` in `S_current`, computing a `score` using the `dot_product` function with `query` and `node` as inputs.\n   - Each node along with its computed score is appended to the `top_k` list as a tuple `(node, score)`.\n\n5. **Select Top-k Nodes**:\n   - `S_layer` is assigned the top-k nodes from `sorted(top_k)[:k].nodes`, where `top_k` is sorted based on scores.'), Document(metadata={'page_source': '20', 'doc_source': '2401.18059v1', 'start_index': 2127, '_id': '2788f6b3d7fe4f9d8401b96cb8c582a6', '_collection_name': 'test'}, page_content='Table 14: Performance of RAPTOR when querying different layers of the tree for Story 2. \nThe table presents data on a multi-layered structure with three rows and four columns. The columns represent ""Layers Queried / Start Layer,"" ""Layer 0 (Leaf Nodes),"" ""Layer 1,"" and ""Layer 2."" The rows are labeled as ""1 layer,"" ""2 layers,"" and ""3 layers,"" likely indicating the number of layers queried or starting layers in some hierarchy or system.'), Document(metadata={'page_source': '18', 'doc_source': '2401.18059v1', 'start_index': 706, '_id': '4264e246f74246fe9cb2591dcce06d99', '_collection_name': 'test'}, page_content=""4. **Initializing Result and Token Count**: The algorithm initializes an empty list `result` and sets `total_tokens` to zero.\n\n5. **Iterating Over Top Nodes**: It then iterates over each node in the sorted `top_nodes` list and checks if adding the current node's token size to `total_tokens` would exceed `max_tokens`.\n\n   - If not, the node is appended to the `result` list.\n   - The `total_tokens` is updated by adding the current node's token size.\n\n6. **Return Result**: The function finally returns the `result` list, which presumably contains a subset of nodes from the original tree that maximize some criteria while keeping within a token limit.\n\nThe algorithm essentially collapses a tree into a prioritized list of nodes based on their relevance to a query, constrained by a token limit.\nThe table compares RAPTOR and DPR responses to two questions about the story of Cinderella:""), Document(metadata={'page_source': '1', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '8cb93675a3fa4506b1d10fd773541bce', '_collection_name': 'test'}, page_content=""The image illustrates the tree construction process used by RAPTOR to cluster and summarize text. Here's a breakdown of its components:\n\n1. **RAPTOR Tree (Left Panel):**\n   - **Root Layer:** Contains nodes (9 and 10).\n   - **Leaf Layer:** Contains nodes (1 to 5).\n   - Clustering of nodes forms a hierarchical structure from the leaf layer up to the root.\n\n2. **Formation of One Tree Layer (Middle Panel):**\n   - **Step 1: Clustering** - Text chunks (1 through 5) are clustered based on their embeddings.\n   - **Step 2: Summarization by LLM** - Clusters (such as those formed by nodes 2 and 3) are summarized to create another layer in the tree (nodes 6, 7, 8).\n\n3. **Contents of a Node (Right Panel):**\n   - **Index:** Identifies the node (e.g., #8).\n   - **Child Nodes:** Lists nodes contained within a cluster (e.g., 2, 3).\n   - **Text Summary:** Provides a summary of the cluster's contents.\n   - **Text Embedding:** Displays the vector representation used for clustering.""), Document(metadata={'page_source': '8', 'doc_source': '2401.18059v1', 'start_index': 2692, '_id': '1cf7ac3e98b24fdf83aaf67afb761fac', '_collection_name': 'test'}, page_content='There is no additional caption or description provided with the table to give context to what these layers specifically refer to, but the format suggests a progression or calculation regarding multiple layers or stages within a system or model.\nWe validated this hypothesis both quantitatively and qualitatively. We present qualitative analysis in appendix  G . To quantitatively understand the contribution of the upper-level nodes, we used stories from the QuALITY dataset. The RAPTOR tree is built for each of these stories, as described in Section  3 . However, during retrieval, we limit the search to different subsets of layers. For example, we exclusively retrieve from the leaf nodes and each upper layer, as well as from different contiguous subsets of the layers. We show findings specific to one story in Table  8 , revealing that a full-tree search, utilizing all layers, outperformed retrieval strategies that focused only on specific layers.'), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '53ff9f7bafd94c41a8359a6aa2589abb', '_collection_name': 'test'}, page_content='The image displays three bar charts comparing the performance of three different models (SBERT, BM25, and DPR) across various tasks: NarrativeQA, Quality, and Qasper. Each chart shows the percentage of some metric across different layers (Layer 0, Layer 1, etc.). The colors represent different models: light blue for SBERT, green for BM25, and red for DPR. The percentage values decrease as the layer number increases across all tasks.\nFigure 7: Histogram showing the percentage of nodes retrieved from different layers of the RAPTOR tree across three datasets (Narrative QA, Quality, and Qasper) using three retrievers (SBERT, BM25, and DPR). The data indicate that a substantial portion of the nodes contributing to the final retrieval comes from non-leaf layers, with a notable percentage from the first and second layers, highlighting the importance of RAPTOR’s hierarchical sum mari z ation in the retrieval process.'), Document(metadata={'page_source': '3', 'doc_source': '2401.18059v1', 'start_index': 4727, '_id': 'a0b5ca33bbfb44158471dbee115ac13e', '_collection_name': 'test'}, page_content='1. Start at the root layer of the RAPTOR tree. Compute the cosine similarity between the query embedding and the embeddings of all nodes present at this initial layer. 2. Choose the top-  $\\cdot k$   nodes based on the highest cosine similarity scores, forming the set  $S_{1}$  .'), Document(metadata={'page_source': '4', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '58b166c194b7456f93d6ff256ce170fc', '_collection_name': 'test'}, page_content='The image illustrates two mechanisms for retrieving information using tree structures: Tree Traversal Retrieval and Collapsed Tree Retrieval.\n\n1. **Tree Traversal Retrieval (A)**:\n   - Starts with a query that is encoded into a vector.\n   - The tree structure is traversed starting from the root.\n   - At each level, it retrieves the top nodes based on cosine similarity to the query vector.\n   - Retrieves context from selected nodes which is then combined with the query to generate an answer using a language model (LLM).\n\n2. **Collapsed Tree Retrieval (B)**:\n   - Also starts with a query that is encoded.\n   - The tree is collapsed into a single layer.\n   - Nodes are retrieved based on cosine similarity until reaching a threshold number of tokens.\n   - The retrieved context is combined with the query to produce an answer using an LLM.'), Document(metadata={'page_source': '18', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '5773eb452b0843af8d5bfc7e17b5aed1', '_collection_name': 'test'}, page_content='The table contains pseudocode for an algorithm labeled as ""Algorithm 2: Collapsed Tree Algorithm"". The function `COLLAPSEDTREE` is designed to process a data structure, presumably a tree, based on several parameters: `tree`, `query`, `k`, and `max_tokens`. Here\'s a breakdown of the algorithm:\n\n1. **Flatten the Tree**: The tree is first flattened into a one-dimensional structure using a function `flatten(tree)`.\n\n2. **Calculate Dot Product**: For each node in the flattened tree, a dot product between the `query` and the `node` is computed and stored in a list `top_nodes` along with the node itself.\n\n3. **Sorting**: The `top_nodes` list is sorted, although it is not specified what it is sorted by.\n\n4. **Initializing Result and Token Count**: The algorithm initializes an empty list `result` and sets `total_tokens` to zero.'), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 1149, '_id': '5498fd44698440e09d35d94c8a3e8ba4', '_collection_name': 'test'}, page_content='1. The first column is labeled ""Layers Queried / Start Layer"" and lists the different scenarios of the layers queried:\n   - 1 layer\n   - 2 layers\n   - 3 layers\n\n2. The second column, ""Layer 0 (Leaf Nodes),"" provides values for each of the scenarios:\n   - For ""1 layer,"" the value is 66.6.\n   - There is no value (indicated by a dash) for ""2 layers"" and ""3 layers.""\n\n3. The third column, ""Layer 1,"" presents values as follows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - There is no value (indicated by a dash) for ""3 layers.""\n\n4. The final column, ""Layer 2,"" shows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - For ""3 layers,"" the value is 83.3, which is also bolded for emphasis.'), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 2606, '_id': 'ce88e64b67e44615a0c33c7735773cd6', '_collection_name': 'test'}, page_content='- For ""1 layer"":\n  - Layer 0 (Leaf Nodes): 57.9\n  - Layer 1: 47.3\n  \n- For ""2 layers"":\n  - Layer 0 (Leaf Nodes): Not applicable or missing\n  - Layer 1: 68.4 (in bold)\nI.2 W HICH  L AYERS DO  R ETRIEVED  N ODES COME FROM  ? \nWe further conduct an ablation study across all three datasets and across three different retrievers with RAPTOR with the collapsed tree retrieval to examine the layers from which the retrieved nodes originate. We observe that between  $18.5\\%$   to  $57\\%$   of the retrieved nodes come from non-leaf nodes. As illustrated in Figure  7 , the retrieval pattern across layers reveals the importance of RAPTOR’s multi-layered tree structure. Notably, a significant percentage of the nodes retrieved by RAPTOR using the DPR retriever for the Narrative QA dataset come from the first and second layers of the tree, as opposed to the leaf nodes. This pattern is consistent across the other datasets and retrievers, albeit with varying percentages.'), Document(metadata={'page_source': '17', 'doc_source': '2401.18059v1', 'start_index': 905, '_id': '4f74f284dead4c3fa8d836bcce39f3bb', '_collection_name': 'test'}, page_content='The hallucination here is that the summary states that Jr. Ajor and Co-Tan are sisters, but does not explicitly mention or imply this. \nUpon reviewing all parent nodes, we found that hallucinations did not propagate to higher layers. Generally, the hallucinations were minor and did not alter the thematic interpretation of the text. \nE.3 I MPACT ON  QA T ASKS \nIn our findings, hallucinations had no discernible impact on the performance of QA tasks. This sug- gests that hallucination is not a major concerns for the sum mari z ation component in our RAPTOR architecture. \nF P SEUDOCODE FOR  R ETRIEVAL  M ETHODS \nThe table contains a pseudocode listing for an algorithm titled ""Algorithm 1: Tree Traversal Algorithm."" This algorithm is designed to traverse a tree structure. Here is a brief explanation of the pseudocode:')]"
10013,"In figure 3, how many distinct icons are used?","[Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '3052e2970a974ba6adf9e018f6e0710c', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '18cc6bf5b1824e29ae65cdaaee677cc0', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '0c0a742e33b44860915b53d79b0d1f67', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '708b8bfda47042bcacfcb11fd496b91e', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': 'ebb3238aeedc41fe8e360783679cdb7e', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 842, '_id': 'b3655412fd554ec1a1cc03ed1e4b7536', '_collection_name': 'test'}, page_content='The numbers beside model names, such as GPT-4 (0.5), likely refer to some version or parameter setting used in the evaluation.\nThe experimental results are the mean of three runs, and the standard deviation is reported in brackets.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 842, '_id': '87eda747378e46c299631fab5eff4a7c', '_collection_name': 'test'}, page_content='The numbers beside model names, such as GPT-4 (0.5), likely refer to some version or parameter setting used in the evaluation.\nThe experimental results are the mean of three runs, and the standard deviation is reported in brackets.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 842, '_id': '4863ee8c1c854fe589cf860d6da366e7', '_collection_name': 'test'}, page_content='The numbers beside model names, such as GPT-4 (0.5), likely refer to some version or parameter setting used in the evaluation.\nThe experimental results are the mean of three runs, and the standard deviation is reported in brackets.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 842, '_id': '5eed23f8c7274de396bb93a6f291a5d2', '_collection_name': 'test'}, page_content='The numbers beside model names, such as GPT-4 (0.5), likely refer to some version or parameter setting used in the evaluation.\nThe experimental results are the mean of three runs, and the standard deviation is reported in brackets.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 842, '_id': '2c85271f55cf45d2921b709ea68329a5', '_collection_name': 'test'}, page_content='The numbers beside model names, such as GPT-4 (0.5), likely refer to some version or parameter setting used in the evaluation.\nThe experimental results are the mean of three runs, and the standard deviation is reported in brackets.'), Document(metadata={'page_source': '20', 'doc_source': '2401.18059v1', 'start_index': 2566, '_id': '1567924ffdb647df8cae4e0f2d0a7448', '_collection_name': 'test'}, page_content='- For ""1 layer,"" the values are 58.8 for Layer 0, 47.1 for Layer 1, and 41.1 for Layer 2.\n- For ""2 layers,"" no value is given for Layer 0, but Layer 1 has a value of 64.7, and Layer 2 is at 52.9.\n- For ""3 layers,"" no values are given for Layer 0 and Layer 1, but Layer 2 is at 47.1.\n\nThis table seems to depict some kind of measurement or metric across different layers of a structure, where Layer 0 is referred to as ""Leaf Nodes."" The values could represent percentages, scores, or any other relevant metric, but the table does not provide further context on what these numbers specifically represent.'), Document(metadata={'page_source': '13', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': '01fbbf9fa3644714aa96f14b6d2fb597', '_collection_name': 'test'}, page_content='The table appears to categorize different evaluation metrics or criteria (listed in the first column) based on which aspects they assess (represented by the column headers from the second to the last column). Each checkmark (✓) indicates that the corresponding metric evaluates or is relevant to the corresponding aspect.\n\nHere is a breakdown of the contents:\n\n- **Context Relevance** is assessed by Accuracy, EM, Recall, Precision, Cosine Similarity, Hit Rate, MRR, and ROUGE/ROUGE-L.\n- **Faithfulness** is assessed by Accuracy, EM, BLEU, and ROUGE/ROUGE-L.\n- **Answer Relevance** is assessed by Accuracy, EM, and R-Rate.\n- **Noise Robustness** is assessed by Accuracy, Recall, and Precision.\n- **Negative Rejection** is assessed by Accuracy and EM.\n- **Information Integration** is assessed by Accuracy, MRR, and ROUGE/ROUGE-L.\n- **Counterfactual Robustness** is assessed by Accuracy and ROUGE/ROUGE-L.'), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 1149, '_id': '5498fd44698440e09d35d94c8a3e8ba4', '_collection_name': 'test'}, page_content='1. The first column is labeled ""Layers Queried / Start Layer"" and lists the different scenarios of the layers queried:\n   - 1 layer\n   - 2 layers\n   - 3 layers\n\n2. The second column, ""Layer 0 (Leaf Nodes),"" provides values for each of the scenarios:\n   - For ""1 layer,"" the value is 66.6.\n   - There is no value (indicated by a dash) for ""2 layers"" and ""3 layers.""\n\n3. The third column, ""Layer 1,"" presents values as follows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - There is no value (indicated by a dash) for ""3 layers.""\n\n4. The final column, ""Layer 2,"" shows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - For ""3 layers,"" the value is 83.3, which is also bolded for emphasis.'), Document(metadata={'page_source': '8', 'doc_source': '2401.18059v1', 'start_index': 2293, '_id': '33558f02c544404fa1b2042cc5f87eda', '_collection_name': 'test'}, page_content='3. **Layer 1**: This column provides the values associated with Layer 1:\n   - For 1 layer: 57.8\n   - For 2 layers: 52.6\n   - No data is provided for 3 layers scenario for Layer 1.\n\n4. **Layer 2**: This column provides the values associated with Layer 2:\n   - For 1 layer: 57.9\n   - For 2 layers: 63.15\n   - For 3 layers: 73.68 (which is in bold, possibly indicating a significant value or result).'), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 924, '_id': 'e3f6a37c00f84138a18b530acf4c2c2f', '_collection_name': 'test'}, page_content=""The table presents data relating to three different queries or layers with corresponding values across three different columns labeled as Layer 0 (Leaf Nodes), Layer 1, and Layer 2. Here's a breakdown of the table contents:"")]"
10014,What is the paper's full title that proposes the method that has a retrieval granularity of phrase?,"[Document(metadata={'page_source': '5', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': '83c36e68ef3143cc97aee3e86894e146', '_collection_name': 'test'}, page_content=""The table lists various methods used in retrieval tasks, with details on their retrieval sources, data types, granularity, augmentation stage, and processes. Here's a breakdown of the columns:\n\n1. **Method**: Names of the retrieval methods.\n2. **Retrieval Source**: Sources from where data is retrieved (e.g., Wikipedia, Search Engine, Dataset-base).\n3. **Retrieval Data Type**: Type of data used for retrieval, such as Text or Knowledge Graph (KG).\n4. **Retrieval Granularity**: The level at which data is retrieved, e.g., Phrase, Sentence, Chunk, Item, Doc, Sub-Graph, etc.\n5. **Augmentation Stage**: The stage at which augmentation occurs, e.g., Pre-training, Tuning, Inference.\n6. **Retrieval Process**: The processes involved, such as Iterative or Once.\n\nThe table organizes different methods based on these characteristics to show a comparative view of various retrieval techniques.""), Document(metadata={'page_source': '2', 'doc_source': '2312.10997v5', 'start_index': 1377, '_id': 'a2c29f83ea764f57acf61b66ed1e45c3', '_collection_name': 'test'}, page_content='Retrieval . Upon receipt of a user query, the RAG system employs the same encoding model utilized during the indexing phase to transform the query into a vector representation. It then computes the similarity scores between the query vector and the vector of chunks within the indexed corpus. The system prioritizes and retrieves the top K chunks that demonstrate the greatest similarity to the query. These chunks are subsequently used as the expanded context in prompt.'), Document(metadata={'page_source': '7', 'doc_source': '2312.10997v5', 'start_index': 1, '_id': '90c55ab993284b7f96de65b77c79e371', '_collection_name': 'test'}, page_content='In text, retrieval granularity ranges from fine to coarse, including Token, Phrase, Sentence, Proposition, Chunks, Doc- ument. Among them, DenseX [30]proposed the concept of using propositions as retrieval units. Propositions are defined as atomic expressions in the text, each encapsulating a unique factual segment and presented in a concise, self-contained nat- ural language format. This approach aims to enhance retrieval precision and relevance. On the Knowledge Graph (KG), retrieval granularity includes Entity, Triplet, and sub-Graph. The granularity of retrieval can also be adapted to downstream tasks, such as retrieving Item IDs [40]in recommendation tasks and Sentence pairs [38]. Detailed information is illustrated in Table I. \nB. Indexing Optimization'), Document(metadata={'page_source': '7', 'doc_source': '2312.10997v5', 'start_index': 1017, '_id': '38a6b358a63a48ef911bc4c2f23900ca', '_collection_name': 'test'}, page_content='1) Chunking Strategy:  The most common method is to split the document into chunks on a fixed number of tokens (e.g., 100, 256, 512) [88]. Larger chunks can capture more context, but they also generate more noise, requiring longer processing time and higher costs. While smaller chunks may not fully convey the necessary context, they do have less noise. How- ever, chunks leads to truncation within sentences, prompting the optimization of a recursive splits and sliding window meth- ods, enabling layered retrieval by merging globally related information across multiple retrieval processes [89]. Never- theless, these approaches still cannot strike a balance between semantic completeness and context length. Therefore, methods like Small2Big have been proposed, where sentences (small) are used as the retrieval unit, and the preceding and following sentences are provided as (big) context to LLMs [90].'), Document(metadata={'page_source': '3', 'doc_source': '2312.10997v5', 'start_index': 1710, '_id': '74d65d9b1cb240168dd7221cc6503b2b', '_collection_name': 'test'}, page_content='Post-Retrieval Process . Once relevant context is retrieved, it’s crucial to integrate it effectively with the query. The main methods in post-retrieval process include rerank chunks and context compressing. Re-ranking the retrieved information to relocate the most relevant content to the edges of the prompt is a key strategy. This concept has been implemented in frame- works such as LlamaIndex 2 , LangChain 3 , and HayStack [12]. Feeding all relevant documents directly into LLMs can lead to information overload, diluting the focus on key details with irrelevant content.To mitigate this, post-retrieval efforts con- centrate on selecting the essential information, emphasizing critical sections, and shortening the context to be processed. \nC. Modular RAG'), Document(metadata={'page_source': '8', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': 'fab582da0aa34644a9415506d6e9c9ed', '_collection_name': 'test'}, page_content='2) Query Transformation:  The core concept is to retrieve chunks based on a transformed query instead of the user’s original query. \nQuery Rewrite .The original queries are not always optimal for LLM retrieval, especially in real-world scenarios. There- fore, we can prompt LLM to rewrite the queries. In addition to using LLM for query rewriting, specialized smaller language models, such as RRR (Rewrite-retrieve-read) [7]. The imple- mentation of the query rewrite method in the Taobao, known as BEQUE [9] has notably enhanced recall effectiveness for long-tail queries, resulting in a rise in GMV.'), Document(metadata={'page_source': '6', 'doc_source': '2312.10997v5', 'start_index': 4028, '_id': '604556e2867f4e2db1478f46d47ba548', '_collection_name': 'test'}, page_content='2) Retrieval Granularity:  Another important factor besides the data format of the retrieval source is the granularity of the retrieved data. Coarse-grained retrieval units theoretically can provide more relevant information for the problem, but they may also contain redundant content, which could distract the retriever and language models in downstream tasks [50], [87]. On the other hand, fine-grained retrieval unit granularity increases the burden of retrieval and does not guarantee seman- tic integrity and meeting the required knowledge. Choosing the appropriate retrieval granularity during inference can be a simple and effective strategy to improve the retrieval and downstream task performance of dense retrievers.'), Document(metadata={'page_source': '7', 'doc_source': '2401.18059v1', 'start_index': 470, '_id': 'aa55f5d31b554cddaae107882b917fda', '_collection_name': 'test'}, page_content='- **DPR with RAPTOR**\n  - Accuracy (QuALITY): 54.7%\n  - Answer F1 (QASPER): 32.23%\n\n- **DPR without RAPTOR**\n  - Accuracy (QuALITY): 53.1%\n  - Answer F1 (QASPER): 31.70%\n\nThe models are assessed with and without the RAPTOR component, showing differences in performance across the metrics.\nTable 3: Controlled comparison of F-1 scores on the QASPER dataset, using three different lan- guage models (GPT-3, GPT-4, UnifiedQA 3B) and various retrieval methods. The column ”Title  $^+$  Abstract” reflects performance when only the title and abstract of the papers are used for context. RAPTOR outperforms the established baselines BM25 and DPR across all tested language models. Specifically, RAPTOR’s F-1 scores are at least   $1.8\\%$   points higher than DPR and at least  $5.3\\%$   points higher than BM25. \nThe table shows F-1 Match scores for different retrievers when combined with different models: GPT-3, GPT-4, and UnifiedQA.'), Document(metadata={'page_source': '4', 'doc_source': '2312.10997v5', 'start_index': 4552, '_id': 'c2129ac33c3c46a49157bd6ec0b36f18', '_collection_name': 'test'}, page_content='III. R ETRIEVAL \nIn the context of RAG, it is crucial to efficiently retrieve relevant documents from the data source. There are several key issues involved, such as the retrieval source, retrieval granularity, pre-processing of the retrieval, and selection of the corresponding embedding model. \nA. Retrieval Source \nRAG relies on external knowledge to enhance LLMs, while the type of retrieval source and the granularity of retrieval units both affect the final generation results. \n1) Data Structure:  Initially, text is s the mainstream source of retrieval. Subsequently, the retrieval source expanded to in- clude semi-structured data (PDF) and structured data (Knowl- edge Graph, KG) for enhancement. In addition to retrieving from original external sources, there is also a growing trend in recent researches towards utilizing content generated by LLMs themselves for retrieval and enhancement purposes.'), Document(metadata={'page_source': '1', 'doc_source': '2401.18059v1', 'start_index': 1931, '_id': '99f7dd55d2224c9f8383f70e930d8140', '_collection_name': 'test'}, page_content='2 R ELATED  W ORK \nWhy Retrieval? Recent advances in hardware and algorithms have indeed expanded the con- text lengths that models can handle, leading to questions about the need for retrieval systems ( Dai et al. ,  2019 ;  Dao et al. ,  2022 ;  Liu et al. ,  2023 ). However, as  Liu et al.  ( 2023 ) and  Sun et al.  ( 2021 ) have noted, models tend to under utilize long-range context and see diminishing performance as con- text length increases, especially when pertinent information is embedded within a lengthy context. Moreover, practically, use of long contexts is expensive and slow. This suggests that selecting the most relevant information for knowledge-intensive tasks is still crucial.'), Document(metadata={'page_source': '13', 'doc_source': '2401.18059v1', 'start_index': 2426, '_id': '118c7ddd6ffe4c9d9b9ceaa7d1ae13e0', '_collection_name': 'test'}, page_content='Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Z ett le moyer, Joelle Pineau, and Manzil Zaheer. Questions are all you need to train a dense passage retriever.  Transactions of the As- sociation for Computational Linguistics , 11:600–616, 2023. doi: 10.1162/tacl a 00564. URL https://a cl anthology.org/2023.tacl-1.35 . \nGideon Schwarz. Estimating the Dimension of a Model.  The annals of statistics , pp. 461–464, 1978. URL  https://project euclid.org/journals/annals-of-statistics/ volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/ aos/1176344136.full . \nKaren Sp¨ arck Jones. A Statistical Interpretation of Term Specificity and its Application in Re- trieval.  Journal of documentation , 28(1):11–21, 1972. URL  https://doi.org/10.1108/ eb026526 .'), Document(metadata={'page_source': '16', 'doc_source': '2312.10997v5', 'start_index': 1647, '_id': '7c8c781934724831bf20e7a8f142ba6c', '_collection_name': 'test'}, page_content='[6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray  et al. , “Training language models to follow instructions with human feedback,”  Advances in neural information processing systems , vol. 35, pp. 27 730–27 744, 2022.\n\n [7] X. Ma, Y. Gong, P. He, H. Zhao, and N. Duan, “Query rewrit- ing for retrieval-augmented large language models,”  arXiv preprint arXiv:2305.14283 , 2023.\n\n [8] I. ILIN, “Advanced rag techniques: an il- lustrated overview,” https://pub.towardsai.net/ advanced-rag-techniques-an-illustrated-overview-04 d 193 d 8 fec 6, 2023.\n\n [9] W. Peng, G. Li, Y. Jiang, Z. Wang, D. Ou, X. Zeng, E. Chen  et al. , “Large language model based long-tail query rewriting in taobao search,”  arXiv preprint arXiv:2311.03758 , 2023.'), Document(metadata={'page_source': '10', 'doc_source': '2312.10997v5', 'start_index': 1959, '_id': 'c5ceb3d5bc944d79a766c9c2d71ca3a5', '_collection_name': 'test'}, page_content='B. Recursive Retrieval \nRecursive retrieval is often used in information retrieval and NLP to improve the depth and relevance of search results. The process involves iterative ly refining search queries based on the results obtained from previous searches. Recursive Retrieval aims to enhance the search experience by gradu- ally converging on the most pertinent information through a feedback loop. IRCoT [61] uses chain-of-thought to guide the retrieval process and refines the CoT with the obtained retrieval results. ToC [57] creates a clarification tree that systematically optimizes the ambiguous parts in the Query. It can be particularly useful in complex search scenarios where the user’s needs are not entirely clear from the outset or where the information sought is highly specialized or nuanced. The recursive nature of the process allows for continuous learning and adaptation to the user’s requirements, often resulting in improved satisfaction with the search outcomes.'), Document(metadata={'page_source': '3', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': 'b8c45259ad084b39ae7b1660ebf4fea7', '_collection_name': 'test'}, page_content='The image compares three paradigms of Retrieval-Augmented Generation (RAG).\n\n1. **Naive RAG** (Left): \n   - Involves three main steps: indexing, retrieval, and generation.\n   - A user query is processed by indexing documents, then retrieving information, followed by prompting a frozen language model to generate output.\n\n2. **Advanced RAG** (Middle):\n   - Builds on naive RAG, adding optimization strategies in pre-retrieval (e.g., query routing, rewriting, expansion) and post-retrieval stages (e.g., reranking, summarization, fusion).\n   - The process is similar but incorporates these enhancements to improve performance.'), Document(metadata={'page_source': '14', 'doc_source': '2312.10997v5', 'start_index': 1297, '_id': '2e0253571673437685701e829b980d7a', '_collection_name': 'test'}, page_content='*The asterisks (*) indicate unspecified metrics in the table.*\nand non-parameterized advantages are areas ripe for explo- ration [27]. Another trend is to introduce SLMs with specific functionalities into RAG and fine-tuned by the results of RAG system. For example, CRAG [67] trains a lightweight retrieval evaluator to assess the overall quality of the retrieved docu- ments for a query and triggers different knowledge retrieval actions based on confidence levels. \nD. Scaling laws of RAG')]"
10015,"According to table II, which are the datasets that has exactly three methods?","[Document(metadata={'page_source': '5', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': '83c36e68ef3143cc97aee3e86894e146', '_collection_name': 'test'}, page_content=""The table lists various methods used in retrieval tasks, with details on their retrieval sources, data types, granularity, augmentation stage, and processes. Here's a breakdown of the columns:\n\n1. **Method**: Names of the retrieval methods.\n2. **Retrieval Source**: Sources from where data is retrieved (e.g., Wikipedia, Search Engine, Dataset-base).\n3. **Retrieval Data Type**: Type of data used for retrieval, such as Text or Knowledge Graph (KG).\n4. **Retrieval Granularity**: The level at which data is retrieved, e.g., Phrase, Sentence, Chunk, Item, Doc, Sub-Graph, etc.\n5. **Augmentation Stage**: The stage at which augmentation occurs, e.g., Pre-training, Tuning, Inference.\n6. **Retrieval Process**: The processes involved, such as Iterative or Once.\n\nThe table organizes different methods based on these characteristics to show a comparative view of various retrieval techniques.""), Document(metadata={'page_source': '8', 'doc_source': '2401.18059v1', 'start_index': 2293, '_id': '33558f02c544404fa1b2042cc5f87eda', '_collection_name': 'test'}, page_content='3. **Layer 1**: This column provides the values associated with Layer 1:\n   - For 1 layer: 57.8\n   - For 2 layers: 52.6\n   - No data is provided for 3 layers scenario for Layer 1.\n\n4. **Layer 2**: This column provides the values associated with Layer 2:\n   - For 1 layer: 57.9\n   - For 2 layers: 63.15\n   - For 3 layers: 73.68 (which is in bold, possibly indicating a significant value or result).'), Document(metadata={'page_source': '13', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': '01fbbf9fa3644714aa96f14b6d2fb597', '_collection_name': 'test'}, page_content='The table appears to categorize different evaluation metrics or criteria (listed in the first column) based on which aspects they assess (represented by the column headers from the second to the last column). Each checkmark (✓) indicates that the corresponding metric evaluates or is relevant to the corresponding aspect.\n\nHere is a breakdown of the contents:\n\n- **Context Relevance** is assessed by Accuracy, EM, Recall, Precision, Cosine Similarity, Hit Rate, MRR, and ROUGE/ROUGE-L.\n- **Faithfulness** is assessed by Accuracy, EM, BLEU, and ROUGE/ROUGE-L.\n- **Answer Relevance** is assessed by Accuracy, EM, and R-Rate.\n- **Noise Robustness** is assessed by Accuracy, Recall, and Precision.\n- **Negative Rejection** is assessed by Accuracy and EM.\n- **Information Integration** is assessed by Accuracy, MRR, and ROUGE/ROUGE-L.\n- **Counterfactual Robustness** is assessed by Accuracy and ROUGE/ROUGE-L.'), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 924, '_id': 'e3f6a37c00f84138a18b530acf4c2c2f', '_collection_name': 'test'}, page_content=""The table presents data relating to three different queries or layers with corresponding values across three different columns labeled as Layer 0 (Leaf Nodes), Layer 1, and Layer 2. Here's a breakdown of the table contents:""), Document(metadata={'page_source': '16', 'doc_source': '2401.18059v1', 'start_index': 1901, '_id': '5f94b11d14264cdeb3a90977269b7e96', '_collection_name': 'test'}, page_content='5. **Avg. Compression Ratio (%)**: Represents the average compression ratio in percentage for each dataset. The ratios are:\n   - All Datasets: 28%\n   - QuALITY: 28%\n   - NarrativeQA: 27%\n   - QASPER: 35%\nD S UM MARI Z ATION  P ROMPT \nTable  11  shows the prompt used for sum mari z ation. \nThe table has two columns: ""Role"" and ""Content."" It has two rows:'), Document(metadata={'page_source': '11', 'doc_source': '2310.05634v2', 'start_index': 1262, '_id': '17813dc0a8434dae8710436ab6b3a56f', '_collection_name': 'test'}, page_content='A Dataset Construction \nIn this section, we will explain the detailed process and algorithms for the automatic dataset construc- tion pipeline. Using this pipeline, we are able to construct datasets with a greater scale or in other domains. \nA.1 Person Selection'), Document(metadata={'page_source': '11', 'doc_source': '2310.05634v2', 'start_index': 1262, '_id': '2cad5ba400d64a3e8fbbadf91d44594d', '_collection_name': 'test'}, page_content='A Dataset Construction \nIn this section, we will explain the detailed process and algorithms for the automatic dataset construc- tion pipeline. Using this pipeline, we are able to construct datasets with a greater scale or in other domains. \nA.1 Person Selection'), Document(metadata={'page_source': '11', 'doc_source': '2310.05634v2', 'start_index': 1262, '_id': '417f0d18ce48464abf6cbb2ebc761016', '_collection_name': 'test'}, page_content='A Dataset Construction \nIn this section, we will explain the detailed process and algorithms for the automatic dataset construc- tion pipeline. Using this pipeline, we are able to construct datasets with a greater scale or in other domains. \nA.1 Person Selection'), Document(metadata={'page_source': '11', 'doc_source': '2310.05634v2', 'start_index': 1262, '_id': '8aa02167a62a43ca898c089444dfdcc9', '_collection_name': 'test'}, page_content='A Dataset Construction \nIn this section, we will explain the detailed process and algorithms for the automatic dataset construc- tion pipeline. Using this pipeline, we are able to construct datasets with a greater scale or in other domains. \nA.1 Person Selection'), Document(metadata={'page_source': '11', 'doc_source': '2310.05634v2', 'start_index': 1262, '_id': '680cf72f24ee423f924f71de2737d634', '_collection_name': 'test'}, page_content='A Dataset Construction \nIn this section, we will explain the detailed process and algorithms for the automatic dataset construc- tion pipeline. Using this pipeline, we are able to construct datasets with a greater scale or in other domains. \nA.1 Person Selection'), Document(metadata={'page_source': '6', 'doc_source': '2312.10997v5', 'start_index': 851, '_id': 'e34eec6e1d1f4936978151089763a43b', '_collection_name': 'test'}, page_content='The diagram visually represents these methods as positioned on the axes of ""External Knowledge Required"" vs. ""Model Adaptation Required"".\nUnstructured Data , such as text, is the most widely used retrieval source, which are mainly gathered from corpus. For open-domain question-answering (ODQA) tasks, the primary retrieval sources are Wikipedia Dump with the current major versions including HotpotQA   4   (1st October , 2017), DPR 5   (20 December, 2018). In addition to encyclopedic data, common unstructured data includes cross-lingual text [19] and domain- specific data (such as medical [67]and legal domains [29]).'), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 1716, '_id': 'cf2e8e2fc4254465be0fc9bdbf6157f0', '_collection_name': 'test'}, page_content='4. The final column, ""Layer 2,"" shows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - For ""3 layers,"" the value is 83.3, which is also bolded for emphasis.\n\nThe table seems to depict performance metrics or results specific to each layer query scenario across the three layers mentioned.\nThe table shows data related to layers queried or start layers across two main columns: ""Layer 0 (Leaf Nodes)"" and ""Layer 1"". It contains two rows that describe:\n\n1. For ""1 layer"" queried or started:\n   - The value in ""Layer 0 (Leaf Nodes)"" is 94.7.\n   - The value in ""Layer 1"" is 84.2.\n\n2. For ""2 layers"" queried or started:\n   - There is no value for ""Layer 0 (Leaf Nodes)"" (indicated by \'-\').\n   - The value in ""Layer 1"" is 89.4.\nThe table shows data for different layers queried or start layers, with respective values for ""Layer 0 (Leaf Nodes)"" and ""Layer 1"":'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 842, '_id': '2c85271f55cf45d2921b709ea68329a5', '_collection_name': 'test'}, page_content='The numbers beside model names, such as GPT-4 (0.5), likely refer to some version or parameter setting used in the evaluation.\nThe experimental results are the mean of three runs, and the standard deviation is reported in brackets.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 842, '_id': '4863ee8c1c854fe589cf860d6da366e7', '_collection_name': 'test'}, page_content='The numbers beside model names, such as GPT-4 (0.5), likely refer to some version or parameter setting used in the evaluation.\nThe experimental results are the mean of three runs, and the standard deviation is reported in brackets.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 842, '_id': 'b3655412fd554ec1a1cc03ed1e4b7536', '_collection_name': 'test'}, page_content='The numbers beside model names, such as GPT-4 (0.5), likely refer to some version or parameter setting used in the evaluation.\nThe experimental results are the mean of three runs, and the standard deviation is reported in brackets.')]"
10016,"Which subsection does the section ""AUGMENTATION PROCESS IN RAG"" include?","[Document(metadata={'page_source': '0', 'doc_source': '2312.10997v5', 'start_index': 3596, '_id': '536d2b14b004494a8c53b2cb80041c25', '_collection_name': 'test'}, page_content='during the inference stage, leading to rapid development in RAG studies. As research progressed, the enhancement of RAG was no longer limited to the inference stage but began to incorporate more with LLM fine-tuning techniques.'), Document(metadata={'page_source': '3', 'doc_source': '2312.10997v5', 'start_index': 627, '_id': 'a340e07d645a4fffabd7121175e02817', '_collection_name': 'test'}, page_content='3. **Modular RAG** (Right):\n   - Enhances flexibility by introducing various specific functional modules.\n   - Includes iterative and adaptive processes like routing, searching, retrieving, reading, predicting, rewriting, reranking, demonstrating, and fusion.\n   - Facilitates non-sequential methods such as iterative retrieval and adaptive approaches, allowing for more dynamic interactions.'), Document(metadata={'page_source': '9', 'doc_source': '2312.10997v5', 'start_index': 1316, '_id': '707695b0fccc43ea9b2c7a28e7a86119', '_collection_name': 'test'}, page_content='2) Context Selection/Compression:  A common misconcep- tion in the RAG process is the belief that retrieving as many relevant documents as possible and concatenating them to form a lengthy retrieval prompt is beneficial. However, excessive context can introduce more noise, diminishing the LLM’s perception of key information .'), Document(metadata={'page_source': '0', 'doc_source': '2312.10997v5', 'start_index': 1312, '_id': '2d6a912ffb084708b26194e3c77b2fa3', '_collection_name': 'test'}, page_content='RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the- art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evalua- tion framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development   1 .'), Document(metadata={'page_source': '18', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '8463fdffd62147849251ea50fffa9955', '_collection_name': 'test'}, page_content='Instruction: You answer the question based on your knowledge, with the given information for annotation, following the given format. Use [NA] for claims that need annotation but is unprovided. \nQuestion: Considering the information:'), Document(metadata={'page_source': '18', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '627450c1c9a14e8784bdd42db56e38c2', '_collection_name': 'test'}, page_content='Instruction: You answer the question based on your knowledge, with the given information for annotation, following the given format. Use [NA] for claims that need annotation but is unprovided. \nQuestion: Considering the information:'), Document(metadata={'page_source': '18', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '8d06e9e6af6e464ea4c5802b25fa943d', '_collection_name': 'test'}, page_content='Instruction: You answer the question based on your knowledge, with the given information for annotation, following the given format. Use [NA] for claims that need annotation but is unprovided. \nQuestion: Considering the information:'), Document(metadata={'page_source': '18', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': 'e629a2d6da484e1d8172a4adb2e06492', '_collection_name': 'test'}, page_content='Instruction: You answer the question based on your knowledge, with the given information for annotation, following the given format. Use [NA] for claims that need annotation but is unprovided. \nQuestion: Considering the information:'), Document(metadata={'page_source': '18', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': 'df3fd53b51d944798dc6cf23c4ff5c84', '_collection_name': 'test'}, page_content='Instruction: You answer the question based on your knowledge, with the given information for annotation, following the given format. Use [NA] for claims that need annotation but is unprovided. \nQuestion: Considering the information:'), Document(metadata={'page_source': '2', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': '8ec4223e41694a358adcddaee861ccf7', '_collection_name': 'test'}, page_content='The image illustrates the RAG (Retrieval-Augmented Generation) process used in question answering systems. It consists of three main steps:\n\n1. **Indexing**: Documents are divided into chunks, encoded into vectors, and stored in a vector database.\n2. **Retrieval**: The system retrieves the top K chunks most relevant to the query based on semantic similarity.\n3. **Generation**: Combines the original question and retrieved chunks as inputs into a large language model (LLM) to generate the final answer.'), Document(metadata={'page_source': '13', 'doc_source': '2312.10997v5', 'start_index': 4716, '_id': '016a888806e448ba86adc1fa56c1e853', '_collection_name': 'test'}, page_content='C. Hybrid Approaches \nCombining RAG with fine-tuning is emerging as a leading strategy. Determining the optimal integration of RAG and fine-tuning whether sequential, alternating, or through end-to- end joint training—and how to harness both parameterized'), Document(metadata={'page_source': '2', 'doc_source': '2312.10997v5', 'start_index': 2973, '_id': '7638a50adfeb49eaa71b6f44c6bc1a9b', '_collection_name': 'test'}, page_content='Augmentation Hurdles . Integrating retrieved information with the different task can be challenging, sometimes resulting in disjointed or incoherent outputs. The process may also encounter redundancy when similar information is retrieved from multiple sources, leading to repetitive responses. Deter- mining the significance and relevance of various passages and ensuring stylistic and tonal consistency add further complexity. Facing complex issues, a single retrieval based on the original query may not suffice to acquire adequate context information. \nMoreover, there’s a concern that generation models might overly rely on augmented information, leading to outputs that simply echo retrieved content without adding insightful or synthesized information. \nB. Advanced RAG'), Document(metadata={'page_source': '6', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': '7c2b6b098b4e4d299c8f889a0c917ca2', '_collection_name': 'test'}, page_content='The image is a diagram comparing different model optimization methods in terms of ""External Knowledge Required"" and ""Model Adaptation Required"". \n\nKey elements:\n\n1. **RAG (Retrieval-Augmented Generation)**: Shown as evolving from Naive RAG to Advanced and Modular RAG.\n   - **Naive RAG**: Involves adding contextual paragraphs with low model modifications.\n   - **Advanced RAG**: Includes index and retrieval optimizations.\n   - **Modular RAG**: Combines multiple modules organically.\n\n2. **Prompt Engineering**: \n   - Requires low modifications and external knowledge, using the capabilities of large language models.\n   - Includes Standard Prompt, Few-shot Prompt, and XoT Prompt (e.g., CoT, ToT).\n\n3. **Fine-tuning**: \n   - Requires high model adaptation.\n   - Includes Retriever Fine-tuning, Collaborative Fine-tuning, and Generator Fine-tuning.'), Document(metadata={'page_source': '2', 'doc_source': '2312.10997v5', 'start_index': 1377, '_id': 'a2c29f83ea764f57acf61b66ed1e45c3', '_collection_name': 'test'}, page_content='Retrieval . Upon receipt of a user query, the RAG system employs the same encoding model utilized during the indexing phase to transform the query into a vector representation. It then computes the similarity scores between the query vector and the vector of chunks within the indexed corpus. The system prioritizes and retrieves the top K chunks that demonstrate the greatest similarity to the query. These chunks are subsequently used as the expanded context in prompt.'), Document(metadata={'page_source': '3', 'doc_source': '2312.10997v5', 'start_index': 2458, '_id': '1be1de49170f4e94958f7d0845aae3d8', '_collection_name': 'test'}, page_content='C. Modular RAG \nThe modular RAG architecture advances beyond the for- mer two RAG paradigms, offering enhanced adaptability and versatility. It incorporates diverse strategies for improving its components, such as adding a search module for similarity searches and refining the retriever through fine-tuning. Inno- vations like restructured RAG modules [13] and rearranged RAG pipelines [14] have been introduced to tackle specific challenges. The shift towards a modular RAG approach is becoming prevalent, supporting both sequential processing and integrated end-to-end training across its components. Despite its distinctiveness, Modular RAG builds upon the foundational principles of Advanced and Naive RAG, illustrating a progres- sion and refinement within the RAG family.')]"
10017,Which method integrates knowledge into white-box models via directive fine-tuning?,"[Document(metadata={'page_source': '8', 'doc_source': '2312.10997v5', 'start_index': 6257, '_id': 'af5061316bfd45689b09dd59bb475bf4', '_collection_name': 'test'}, page_content='Furthermore, PKG introduces an innovative method for integrating knowledge into white-box models via directive fine-tuning [75]. In this approach, the retriever module is directly substituted to gen- erate relevant documents according to a query. This method assists in addressing the difficulties encountered during the fine-tuning process and enhances model performance.'), Document(metadata={'page_source': '4', 'doc_source': '2312.10997v5', 'start_index': 2212, '_id': 'a833281365664752b77fe9a94481c457', '_collection_name': 'test'}, page_content='D. RAG vs Fine-tuning \nThe augmentation of LLMs has attracted considerable atten- tion due to their growing prevalence. Among the optimization methods for LLMs, RAG is often compared with Fine-tuning (FT) and prompt engineering. Each method has distinct charac- teristics as illustrated in Figure 4. We used a quadrant chart to illustrate the differences among three methods in two dimen- sions: external knowledge requirements and model adaption requirements. Prompt engineering leverages a model’s inherent capabilities with minimum necessity for external knowledge and model adaption. RAG can be likened to providing a model with a tailored textbook for information retrieval, ideal for pre- cise information retrieval tasks. In contrast, FT is comparable to a student internalizing knowledge over time, suitable for scenarios requiring replication of specific structures, styles, or formats.'), Document(metadata={'page_source': '4', 'doc_source': '2312.10997v5', 'start_index': 3673, '_id': '5dfd954f791a49bbb783517a5b4c87c8', '_collection_name': 'test'}, page_content='In multiple evaluations of their performance on various knowledge-intensive tasks across different topics, [28] re- vealed that while unsupervised fine-tuning shows some im- provement, RAG consistently outperforms it, for both exist- ing knowledge encountered during training and entirely new knowledge. Additionally, it was found that LLMs struggle to learn new factual information through unsupervised fine- tuning. The choice between RAG and FT depends on the specific needs for data dynamics, customization, and com- putational capabilities in the application context. RAG and FT are not mutually exclusive and can complement each other, enhancing a model’s capabilities at different levels. In some instances, their combined use may lead to optimal performance. The optimization process involving RAG and FT may require multiple iterations to achieve satisfactory results. \nIII. R ETRIEVAL'), Document(metadata={'page_source': '6', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': '7c2b6b098b4e4d299c8f889a0c917ca2', '_collection_name': 'test'}, page_content='The image is a diagram comparing different model optimization methods in terms of ""External Knowledge Required"" and ""Model Adaptation Required"". \n\nKey elements:\n\n1. **RAG (Retrieval-Augmented Generation)**: Shown as evolving from Naive RAG to Advanced and Modular RAG.\n   - **Naive RAG**: Involves adding contextual paragraphs with low model modifications.\n   - **Advanced RAG**: Includes index and retrieval optimizations.\n   - **Modular RAG**: Combines multiple modules organically.\n\n2. **Prompt Engineering**: \n   - Requires low modifications and external knowledge, using the capabilities of large language models.\n   - Includes Standard Prompt, Few-shot Prompt, and XoT Prompt (e.g., CoT, ToT).\n\n3. **Fine-tuning**: \n   - Requires high model adaptation.\n   - Includes Retriever Fine-tuning, Collaborative Fine-tuning, and Generator Fine-tuning.'), Document(metadata={'page_source': '0', 'doc_source': '2312.10997v5', 'start_index': 4627, '_id': '35049f2de7094b8498f920d17294ff35', '_collection_name': 'test'}, page_content='methods applicable to RAG. Overall, this paper sets out to meticulously compile and categorize the foundational technical concepts, historical progression, and the spectrum of RAG methodologies and applications that have emerged post-LLMs. It is designed to equip readers and professionals with a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and speculate on upcoming trends and innovations.'), Document(metadata={'page_source': '8', 'doc_source': '2312.10997v5', 'start_index': 5176, '_id': '376780bc6e7a419295c97faeac87d04d', '_collection_name': 'test'}, page_content='E. Adapter \nFine-tuning models may present challenges, such as in- tegrating functionality through an API or addressing con- straints arising from limited local computational resources. Consequently, some approaches opt to incorporate an external adapter to aid in alignment.'), Document(metadata={'page_source': '4', 'doc_source': '2312.10997v5', 'start_index': 1, '_id': '9ed85e7a10aa443083dbfe113e8e86da', '_collection_name': 'test'}, page_content='2) New Patterns:  Modular RAG offers remarkable adapt- ability by allowing module substitution or reconfiguration to address specific challenges. This goes beyond the fixed structures of Naive and Advanced RAG, characterized by a simple “Retrieve” and “Read” mechanism. Moreover, Modular RAG expands this flexibility by integrating new modules or adjusting interaction flow among existing ones, enhancing its applicability across different tasks.'), Document(metadata={'page_source': '14', 'doc_source': '2312.10997v5', 'start_index': 2760, '_id': '3eaaa26f7ad54de3b6fe53b82e209f92', '_collection_name': 'test'}, page_content='The development of the RAG ecosystem is greatly impacted by the progression of its technical stack. Key tools like LangChain and LLamaIndex have quickly gained popularity with the emergence of ChatGPT, providing extensive RAG- related APIs and becoming essential in the realm of LLMs.The emerging technology stack, while not as rich in features as LangChain and LLamaIndex, stands out through its specialized products. For example, Flowise AI prioritizes a low-code approach, allowing users to deploy AI applications, including RAG, through a user-friendly drag-and-drop interface. Other technologies like HayStack, Meltano, and Cohere Coral are also gaining attention for their unique contributions to the field.'), Document(metadata={'page_source': '15', 'doc_source': '2312.10997v5', 'start_index': 3732, '_id': '77c60ba3251043e5b8dffd752a7582d3', '_collection_name': 'test'}, page_content='The summary of this paper, as depicted in Figure 6, empha- sizes RAG’s significant advancement in enhancing the capa- bilities of LLMs by integrating parameterized knowledge from language models with extensive non-parameterized data from external knowledge bases. The survey showcases the evolution of RAG technologies and their application on many different tasks. The analysis outlines three developmental paradigms within the RAG framework: Naive, Advanced, and Modu- lar RAG, each representing a progressive enhancement over its predecessors. RAG’s technical integration with other AI methodologies, such as fine-tuning and reinforcement learning, has further expanded its capabilities. Despite the progress in RAG technology, there are research opportunities to improve its robustness and its ability to handle extended contexts. RAG’s application scope is expanding into multimodal do- mains, adapting its principles to interpret and process diverse data forms like images, videos, and code.'), Document(metadata={'page_source': '10', 'doc_source': '2312.10997v5', 'start_index': 3551, '_id': '31937ea1f93548ae8df57d5b78ecf65a', '_collection_name': 'test'}, page_content='C. Adaptive Retrieval \nAdaptive retrieval methods, exemplified by Flare [24] and Self-RAG [25], refine the RAG framework by enabling LLMs to actively determine the optimal moments and content for retrieval, thus enhancing the efficiency and relevance of the information sourced. \nThese methods are part of a broader trend wherein LLMs employ active judgment in their operations, as seen in model agents like AutoGPT, Toolformer, and Graph- Toolformer [107]–[109]. Graph-Toolformer, for instance, di- vides its retrieval process into distinct steps where LLMs pro actively use retrievers, apply Self-Ask techniques, and em- ploy few-shot prompts to initiate search queries. This proactive stance allows LLMs to decide when to search for necessary information, akin to how an agent utilizes tools.'), Document(metadata={'page_source': '4', 'doc_source': '2312.10997v5', 'start_index': 1230, '_id': '82f76694657e420f8eb50848429a2333', '_collection_name': 'test'}, page_content='Adjustments in module arrangement and interaction, such as the Demonstrate-Search-Predict (DSP) [23] framework and the iterative Retrieve-Read-Retrieve-Read flow of ITER- RETGEN [14], showcase the dynamic use of module out- puts to bolster another module’s functionality, illustrating a sophisticated understanding of enhancing module synergy. The flexible orchestration of Modular RAG Flow showcases the benefits of adaptive retrieval through techniques such as FLARE [24] and Self-RAG [25]. This approach transcends the fixed RAG retrieval process by evaluating the necessity of retrieval based on different scenarios. Another benefit of a flexible architecture is that the RAG system can more easily integrate with other technologies (such as fine-tuning or reinforcement learning) [26]. For example, this can involve fine-tuning the retriever for better retrieval results, fine-tuning the generator for more personalized outputs, or engaging in collaborative fine-tuning [27].'), Document(metadata={'page_source': '8', 'doc_source': '2312.10997v5', 'start_index': 3821, '_id': '832d31da4b4d4f9eb0a0f06902045fba', '_collection_name': 'test'}, page_content='In addition to supplementing domain knowledge, another purpose of fine-tuning is to align the retriever and generator, for example, using the results of LLM as the supervision signal for fine-tuning, known as LSR (LM-supervised Retriever). PROMPT A GATOR [21] utilizes the LLM as a few-shot query generator to create task-specific retrievers, addressing chal- lenges in supervised fine-tuning, particularly in data-scarce domains. Another approach, LLM-Embedder [97], exploits LLMs to generate reward signals across multiple downstream tasks. The retriever is fine-tuned with two types of supervised signals: hard labels for the dataset and soft rewards from the LLMs. This dual-signal approach fosters a more effective fine-tuning process, tailoring the embedding model to diverse downstream applications. REPLUG [72] utilizes a retriever and an LLM to calculate the probability distributions of the retrieved documents and then performs supervised training by computing the KL divergence. This'), Document(metadata={'page_source': '9', 'doc_source': '2312.10997v5', 'start_index': 4196, '_id': 'e73c30b3f322407ca5cbb970bed6515a', '_collection_name': 'test'}, page_content='Aligning LLM outputs with human or retriever preferences through reinforcement learning is a potential approach. For instance, manually annotating the final generated answers and then providing feedback through reinforcement learning. In addition to aligning with human preferences, it is also possible to align with the preferences of fine-tuned models and retrievers [79]. When circumstances prevent access to powerful proprietary models or larger parameter open-source models, a simple and effective method is to distill the more powerful models(e.g. GPT-4). Fine-tuning of LLM can also be coordinated with fine-tuning of the retriever to align pref- erences. A typical approach, such as RA-DIT [27], aligns the scoring functions between Retriever and Generator using KL divergence. \nV. A UG MENT ATION PROCESS IN  RAG'), Document(metadata={'page_source': '8', 'doc_source': '2312.10997v5', 'start_index': 3458, '_id': 'd7c80da5b2b548b2b1206735661bd447', '_collection_name': 'test'}, page_content='2) Fine-tuning Embedding Model:  In instances where the context significantly deviates from pre-training corpus, partic- ularly within highly specialized disciplines such as healthcare, legal practice, and other sectors replete with proprietary jargon, fine-tuning the embedding model on your own domain dataset becomes essential to mitigate such discrepancies.'), Document(metadata={'page_source': '3', 'doc_source': '2312.10997v5', 'start_index': 627, '_id': 'a340e07d645a4fffabd7121175e02817', '_collection_name': 'test'}, page_content='3. **Modular RAG** (Right):\n   - Enhances flexibility by introducing various specific functional modules.\n   - Includes iterative and adaptive processes like routing, searching, retrieving, reading, predicting, rewriting, reranking, demonstrating, and fusion.\n   - Facilitates non-sequential methods such as iterative retrieval and adaptive approaches, allowing for more dynamic interactions.')]"
10018,What degree does LEBOUR have?,"[Document(metadata={'page_source': '22', 'doc_source': '2310.05634v2', 'start_index': 1884, '_id': 'e097a298cf914613949b6d8fdaba7ee2', '_collection_name': 'test'}, page_content='Crane attended Syracuse University and was a member of the Syracuse Orange baseball team [Q206534, alma mater: Syracuse University, sport: baseball, member of sports team: Syracuse Orange baseball]. His experiences at Syracuse University influenced his writing, and he wrote several stories about college life, including ""The Eighty-Yard Run,"" which was based on his own experiences as a football player at Syracuse [NA]. \nTable 19: An example of the attributed answer generated by ChatGPT (temperature 0.5). We use blue color for the question and brown color for the retrieved knowledge.'), Document(metadata={'page_source': '22', 'doc_source': '2310.05634v2', 'start_index': 1884, '_id': 'ad1fee484eec4bd58be48513109c1b41', '_collection_name': 'test'}, page_content='Crane attended Syracuse University and was a member of the Syracuse Orange baseball team [Q206534, alma mater: Syracuse University, sport: baseball, member of sports team: Syracuse Orange baseball]. His experiences at Syracuse University influenced his writing, and he wrote several stories about college life, including ""The Eighty-Yard Run,"" which was based on his own experiences as a football player at Syracuse [NA]. \nTable 19: An example of the attributed answer generated by ChatGPT (temperature 0.5). We use blue color for the question and brown color for the retrieved knowledge.'), Document(metadata={'page_source': '22', 'doc_source': '2310.05634v2', 'start_index': 1884, '_id': 'd13e941631654900aac4eb967d50cb09', '_collection_name': 'test'}, page_content='Crane attended Syracuse University and was a member of the Syracuse Orange baseball team [Q206534, alma mater: Syracuse University, sport: baseball, member of sports team: Syracuse Orange baseball]. His experiences at Syracuse University influenced his writing, and he wrote several stories about college life, including ""The Eighty-Yard Run,"" which was based on his own experiences as a football player at Syracuse [NA]. \nTable 19: An example of the attributed answer generated by ChatGPT (temperature 0.5). We use blue color for the question and brown color for the retrieved knowledge.'), Document(metadata={'page_source': '22', 'doc_source': '2310.05634v2', 'start_index': 1884, '_id': '3fb7cec5f19f46a2946043e8e0ce34b7', '_collection_name': 'test'}, page_content='Crane attended Syracuse University and was a member of the Syracuse Orange baseball team [Q206534, alma mater: Syracuse University, sport: baseball, member of sports team: Syracuse Orange baseball]. His experiences at Syracuse University influenced his writing, and he wrote several stories about college life, including ""The Eighty-Yard Run,"" which was based on his own experiences as a football player at Syracuse [NA]. \nTable 19: An example of the attributed answer generated by ChatGPT (temperature 0.5). We use blue color for the question and brown color for the retrieved knowledge.'), Document(metadata={'page_source': '22', 'doc_source': '2310.05634v2', 'start_index': 1884, '_id': 'd0bebc152f254fd6b5bc764c480200ce', '_collection_name': 'test'}, page_content='Crane attended Syracuse University and was a member of the Syracuse Orange baseball team [Q206534, alma mater: Syracuse University, sport: baseball, member of sports team: Syracuse Orange baseball]. His experiences at Syracuse University influenced his writing, and he wrote several stories about college life, including ""The Eighty-Yard Run,"" which was based on his own experiences as a football player at Syracuse [NA]. \nTable 19: An example of the attributed answer generated by ChatGPT (temperature 0.5). We use blue color for the question and brown color for the retrieved knowledge.'), Document(metadata={'page_source': '23', 'doc_source': '2310.05634v2', 'start_index': 2043, '_id': '968d0884d84d43999602da6af76c0226', '_collection_name': 'test'}, page_content='While at Syracuse University, Stephen Crane was a member of the Syracuse Orange baseball team [Q206534, member of sports team: Syracuse Orange baseball]. \nTable 20: An example of the attributed answer generated by GPT-4. We use blue color for the question and brown color for the retrieved knowledge.'), Document(metadata={'page_source': '23', 'doc_source': '2310.05634v2', 'start_index': 2043, '_id': '33552834b5a74dd7897f52c4c3be98f6', '_collection_name': 'test'}, page_content='While at Syracuse University, Stephen Crane was a member of the Syracuse Orange baseball team [Q206534, member of sports team: Syracuse Orange baseball]. \nTable 20: An example of the attributed answer generated by GPT-4. We use blue color for the question and brown color for the retrieved knowledge.'), Document(metadata={'page_source': '23', 'doc_source': '2310.05634v2', 'start_index': 2043, '_id': '3326010a33fb4ce9920ab580c1444b2b', '_collection_name': 'test'}, page_content='While at Syracuse University, Stephen Crane was a member of the Syracuse Orange baseball team [Q206534, member of sports team: Syracuse Orange baseball]. \nTable 20: An example of the attributed answer generated by GPT-4. We use blue color for the question and brown color for the retrieved knowledge.'), Document(metadata={'page_source': '23', 'doc_source': '2310.05634v2', 'start_index': 2043, '_id': 'e487235f66e14a90be3c0f7b7afbb3d9', '_collection_name': 'test'}, page_content='While at Syracuse University, Stephen Crane was a member of the Syracuse Orange baseball team [Q206534, member of sports team: Syracuse Orange baseball]. \nTable 20: An example of the attributed answer generated by GPT-4. We use blue color for the question and brown color for the retrieved knowledge.'), Document(metadata={'page_source': '23', 'doc_source': '2310.05634v2', 'start_index': 2043, '_id': '145756ca3c0a4407913a7268b4dc62a6', '_collection_name': 'test'}, page_content='While at Syracuse University, Stephen Crane was a member of the Syracuse Orange baseball team [Q206534, member of sports team: Syracuse Orange baseball]. \nTable 20: An example of the attributed answer generated by GPT-4. We use blue color for the question and brown color for the retrieved knowledge.'), Document(metadata={'page_source': '22', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '1ab79a44788d44f0bd89e3a2de94585d', '_collection_name': 'test'}, page_content='Question : \nWhat were some of Stephen Crane’s notable works that reflected his views on religion and the harsh realities of life, and how did his experiences at Syracuse University influence his writing? Also, what was the name of the sports team he was a member of at Syracuse University? \nRetrieved Knowledge :'), Document(metadata={'page_source': '23', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '7402a1b365274d9cb8e1921570c88b67', '_collection_name': 'test'}, page_content='Question : \nWhat were some of Stephen Crane’s notable works that reflected his views on religion and the harsh realities of life, and how did his experiences at Syracuse University influence his writing? Also, what was the name of the sports team he was a member of at Syracuse University? \nRetrieved Knowledge :'), Document(metadata={'page_source': '23', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '01f819845b694bb2a33cf75848a44454', '_collection_name': 'test'}, page_content='Question : \nWhat were some of Stephen Crane’s notable works that reflected his views on religion and the harsh realities of life, and how did his experiences at Syracuse University influence his writing? Also, what was the name of the sports team he was a member of at Syracuse University? \nRetrieved Knowledge :'), Document(metadata={'page_source': '22', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '463f7425122a4c98bbcfc234e9c6c3f6', '_collection_name': 'test'}, page_content='Question : \nWhat were some of Stephen Crane’s notable works that reflected his views on religion and the harsh realities of life, and how did his experiences at Syracuse University influence his writing? Also, what was the name of the sports team he was a member of at Syracuse University? \nRetrieved Knowledge :'), Document(metadata={'page_source': '23', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': 'b8c29d4cd0b04fcea02fb1b223b44061', '_collection_name': 'test'}, page_content='Question : \nWhat were some of Stephen Crane’s notable works that reflected his views on religion and the harsh realities of life, and how did his experiences at Syracuse University influence his writing? Also, what was the name of the sports team he was a member of at Syracuse University? \nRetrieved Knowledge :')]"
10019,What is the title of the of the Figure 2?,"[Document(metadata={'page_source': '2', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 703, '_id': '7ae63d8c27dc471f8b08c7ef87a5ffea', '_collection_name': 'test'}, page_content='The image is a diagram displaying ""Breccia Gashes,"" which are formations within a cliff. The top part of these gashes is shown as being eroded or ""denuded,"" while the lower portion is visible within the cliff face. ""Breccia"" refers to a rock composed of broken fragments of minerals or rock cemented together by a fine-grained matrix, which is depicted in the diagram as a pattern of angular fragments, representing the texture of the breccia within these formations.\nDia g rum of Breccia Gash in Cli fr with top denuded of fund Bottom concealed by the beach. \nThe image is a line drawing labeled ""FiG. 3,"" depicting a geological cross-section with features that resemble vertical geological formations or structures. These structures could represent elements like veins or intrusions within layered rock formations. The drawing uses different textures and lines to distinguish between various layers and features, suggesting a focus on geological processes or rock formations.'), Document(metadata={'page_source': '2', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 1681, '_id': 'ad284ff5cb594a51915816b2271111d2', '_collection_name': 'test'}, page_content='The image appears to be a geological diagram illustrating a specific formation process. It depicts layers of sedimentary rock with a visible deformation feature, which resembles a fault or crack filled with fragmented material, indicative of a fracture or a fault zone in the rock layers. This may represent the formation process of a particular geological structure, potentially related to a gas formation, although the text accompanying the image contains typographical errors and is difficult to interpret accurately.\nN.B. In.the above sketches the Cementing matter is represented by diagonal shading.'), Document(metadata={'page_source': '2', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 0, '_id': 'afd8cc791c2c498f963bbc9314a5dea7', '_collection_name': 'test'}, page_content='To illustrate Prt. Lcbour\'s paper ""On the Breccia Gushes of Durham? \nThe image appears to be a drawing or illustration depicting a stone wall with a section in the middle showing rubble or irregularly shaped stones. The caption text mentions ""FiG. I,"" indicating that this might be Figure 1 in a larger set of illustrations. The words ""evil & cypress"" are written at the top left corner of the image, but without additional context, their relevance to the image is unclear. The style suggests it may be from an older publication on construction, archaeology, or geology, possibly illustrating a type of masonry or wall construction method involving rough-hewn stones used for the inner layer of a wall.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '708b8bfda47042bcacfcb11fd496b91e', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '18cc6bf5b1824e29ae65cdaaee677cc0', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': 'ebb3238aeedc41fe8e360783679cdb7e', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '3052e2970a974ba6adf9e018f6e0710c', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '0c0a742e33b44860915b53d79b0d1f67', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '4', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 0, '_id': '58d9b81171cb4bc19933d23c18e9f779', '_collection_name': 'test'}, page_content='The image appears to be a sketch or drawing depicting a textured, rocky landscape or a cliffside. It uses a monochromatic palette with detailed shading to create depth and texture.'), Document(metadata={'page_source': '14', 'doc_source': '2401.18059v1', 'start_index': 2556, '_id': '3c95baaaa06a45ba88f24e140e699e62', '_collection_name': 'test'}, page_content='1. **QASPER** (left plot):\n   - The plot uses red color to represent data.\n   - The horizontal axis shows document length in tokens, ranging from 0 to 30,000.\n   - The vertical axis shows the total number of tokens for both the prompt and completion, ranging from 0 to 40,000.\n   - The graph illustrates a linear increase in token cost as document length increases.\n\n2. **Narrative QA** (middle plot):\n   - The plot uses blue color to represent data.\n   - The horizontal axis shows document length, ranging from 0 to 400,000 tokens.\n   - The vertical axis shows the total number of tokens for both the prompt and completion, ranging from 0 to 600,000.\n   - The graph depicts a linear relationship, with token cost steadily increasing with document length.'), Document(metadata={'page_source': '8', 'doc_source': '2310.05634v2', 'start_index': 3097, '_id': '02c2bbc76663460c96689eee69f478fb', '_collection_name': 'test'}, page_content='6 Related Work'), Document(metadata={'page_source': '8', 'doc_source': '2310.05634v2', 'start_index': 3097, '_id': '16843e083f724307a7798a00dc4acd7b', '_collection_name': 'test'}, page_content='6 Related Work'), Document(metadata={'page_source': '8', 'doc_source': '2310.05634v2', 'start_index': 3097, '_id': '2a6815baf8d347858c6dc5f18c640701', '_collection_name': 'test'}, page_content='6 Related Work'), Document(metadata={'page_source': '8', 'doc_source': '2310.05634v2', 'start_index': 3097, '_id': 'e5fbfa67ed044279864892a97cdedcc9', '_collection_name': 'test'}, page_content='6 Related Work'), Document(metadata={'page_source': '8', 'doc_source': '2310.05634v2', 'start_index': 3097, '_id': 'f29a0482f75a48e19776071c5f1dfe00', '_collection_name': 'test'}, page_content='6 Related Work')]"
10000,"In figure 1, which relation arrows do not point to specific leaf nodes?","[Document(metadata={'page_source': '0', 'doc_source': '2310.05634v2', 'start_index': 2751, '_id': '5b933b37abfb4cdd88aae58eece53758', '_collection_name': 'test'}, page_content='The knowledge graph connects these figures with concepts like occupation, style, place of birth, and their citizen status. The output suggests information about Artemisia’s birth and her status as an Italian painter.\ntries that require precision and factual knowledge like finance, law, and medical treatment.'), Document(metadata={'page_source': '0', 'doc_source': '2310.05634v2', 'start_index': 2751, '_id': 'd7f628cc27134c0d89e5dd2352f2ceca', '_collection_name': 'test'}, page_content='The knowledge graph connects these figures with concepts like occupation, style, place of birth, and their citizen status. The output suggests information about Artemisia’s birth and her status as an Italian painter.\ntries that require precision and factual knowledge like finance, law, and medical treatment.'), Document(metadata={'page_source': '0', 'doc_source': '2310.05634v2', 'start_index': 2751, '_id': '78f232bae3e64c2cad8e96450131f42d', '_collection_name': 'test'}, page_content='The knowledge graph connects these figures with concepts like occupation, style, place of birth, and their citizen status. The output suggests information about Artemisia’s birth and her status as an Italian painter.\ntries that require precision and factual knowledge like finance, law, and medical treatment.'), Document(metadata={'page_source': '0', 'doc_source': '2310.05634v2', 'start_index': 2751, '_id': '51fafc5459e748608d1bb962bf8ff93c', '_collection_name': 'test'}, page_content='The knowledge graph connects these figures with concepts like occupation, style, place of birth, and their citizen status. The output suggests information about Artemisia’s birth and her status as an Italian painter.\ntries that require precision and factual knowledge like finance, law, and medical treatment.'), Document(metadata={'page_source': '0', 'doc_source': '2310.05634v2', 'start_index': 2751, '_id': '1ada8077eb0c47a88c6b9307dea7b085', '_collection_name': 'test'}, page_content='The knowledge graph connects these figures with concepts like occupation, style, place of birth, and their citizen status. The output suggests information about Artemisia’s birth and her status as an Italian painter.\ntries that require precision and factual knowledge like finance, law, and medical treatment.'), Document(metadata={'page_source': '0', 'doc_source': '2310.05634v2', 'start_index': 2751, '_id': '478a7f94a7ee40709637e32745b101bb', '_collection_name': 'test'}, page_content='The knowledge graph connects these figures with concepts like occupation, style, place of birth, and their citizen status. The output suggests information about Artemisia’s birth and her status as an Italian painter.\ntries that require precision and factual knowledge like finance, law, and medical treatment.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': 'ebb3238aeedc41fe8e360783679cdb7e', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': 'b66c63b7556d48088e92dcf1e087db0b', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '18cc6bf5b1824e29ae65cdaaee677cc0', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '708b8bfda47042bcacfcb11fd496b91e', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '3052e2970a974ba6adf9e018f6e0710c', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '0c0a742e33b44860915b53d79b0d1f67', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '2', 'doc_source': '2310.05634v2', 'start_index': 3564, '_id': '6ee751588fb44c17ba40418d0f4404f4', '_collection_name': 'test'}, page_content='To our best knowledge, there is no perfect au- tomatic evaluation for these metrics. Naturalness and significance are subjective. Hence, we apply human evaluation to ensure the dataset quality. \nWe randomly sample 50 data entries from BioKaLMA and ask human annotators to evalu- ate the data entries based on the four metrics. The general and specific questions are evaluated sepa- rately. More details are given in Appendix  C .'), Document(metadata={'page_source': '2', 'doc_source': '2310.05634v2', 'start_index': 3564, '_id': '3b612a4bba7546bba90489486f3fbd63', '_collection_name': 'test'}, page_content='To our best knowledge, there is no perfect au- tomatic evaluation for these metrics. Naturalness and significance are subjective. Hence, we apply human evaluation to ensure the dataset quality. \nWe randomly sample 50 data entries from BioKaLMA and ask human annotators to evalu- ate the data entries based on the four metrics. The general and specific questions are evaluated sepa- rately. More details are given in Appendix  C .'), Document(metadata={'page_source': '2', 'doc_source': '2310.05634v2', 'start_index': 3564, '_id': 'd7fb405454054706b0cafd460d9854a4', '_collection_name': 'test'}, page_content='To our best knowledge, there is no perfect au- tomatic evaluation for these metrics. Naturalness and significance are subjective. Hence, we apply human evaluation to ensure the dataset quality. \nWe randomly sample 50 data entries from BioKaLMA and ask human annotators to evalu- ate the data entries based on the four metrics. The general and specific questions are evaluated sepa- rately. More details are given in Appendix  C .')]"
10001,"In figure 5, what is the color of the line that has no intersection with any other line?","[Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': 'b66c63b7556d48088e92dcf1e087db0b', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '0c0a742e33b44860915b53d79b0d1f67', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '708b8bfda47042bcacfcb11fd496b91e', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '18cc6bf5b1824e29ae65cdaaee677cc0', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': 'ebb3238aeedc41fe8e360783679cdb7e', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '3052e2970a974ba6adf9e018f6e0710c', '_collection_name': 'test'}, page_content='The image is a line graph titled ""Experiment Result on Conscious Incompetence.” It shows the relationship between the number of knowledge elements removed and three metrics: precision, recall, and F1-Score.\n\n- **Precision** (blue line with circle markers) increases significantly as more knowledge is removed, starting at around 14 and reaching about 26.\n- **Recall** (orange line with diamond markers) remains relatively stable, starting around 14 and ending slightly below 15.\n- **F1-Score** (green line with triangle markers) shows a moderate increase, starting around 14 and ending around 18.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 4155, '_id': '7913b604c52f40aa9cfc72d34f585809', '_collection_name': 'test'}, page_content='- The row labeled ""0 (gold)"" has values 95.5 (Corr.), 30.1 (Prec.), 57.1 (Rec.), and 39.4 (F1.).\n- The row labeled ""1"" has values 94.1 (Corr.), 26.1 (Prec.), 42.5 (Rec.), and 32.3 (F1.).\n- The row labeled ""2"" has values 94.0 (Corr.), 21.0 (Prec.), 31.4 (Rec.), and 25.2 (F1.).\n- The row labeled ""3"" has values 93.9 (Corr.), 16.3 (Prec.), 20.4 (Rec.), and 18.1 (F1.).'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 4155, '_id': '8ccbb4b3a39a4e0f8440747d28b8350b', '_collection_name': 'test'}, page_content='- The row labeled ""0 (gold)"" has values 95.5 (Corr.), 30.1 (Prec.), 57.1 (Rec.), and 39.4 (F1.).\n- The row labeled ""1"" has values 94.1 (Corr.), 26.1 (Prec.), 42.5 (Rec.), and 32.3 (F1.).\n- The row labeled ""2"" has values 94.0 (Corr.), 21.0 (Prec.), 31.4 (Rec.), and 25.2 (F1.).\n- The row labeled ""3"" has values 93.9 (Corr.), 16.3 (Prec.), 20.4 (Rec.), and 18.1 (F1.).'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 4155, '_id': 'dba35a2413b3404d8216a02cb937929f', '_collection_name': 'test'}, page_content='- The row labeled ""0 (gold)"" has values 95.5 (Corr.), 30.1 (Prec.), 57.1 (Rec.), and 39.4 (F1.).\n- The row labeled ""1"" has values 94.1 (Corr.), 26.1 (Prec.), 42.5 (Rec.), and 32.3 (F1.).\n- The row labeled ""2"" has values 94.0 (Corr.), 21.0 (Prec.), 31.4 (Rec.), and 25.2 (F1.).\n- The row labeled ""3"" has values 93.9 (Corr.), 16.3 (Prec.), 20.4 (Rec.), and 18.1 (F1.).'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 4155, '_id': '7f6f809d6bfd4933bee1ab611842c294', '_collection_name': 'test'}, page_content='- The row labeled ""0 (gold)"" has values 95.5 (Corr.), 30.1 (Prec.), 57.1 (Rec.), and 39.4 (F1.).\n- The row labeled ""1"" has values 94.1 (Corr.), 26.1 (Prec.), 42.5 (Rec.), and 32.3 (F1.).\n- The row labeled ""2"" has values 94.0 (Corr.), 21.0 (Prec.), 31.4 (Rec.), and 25.2 (F1.).\n- The row labeled ""3"" has values 93.9 (Corr.), 16.3 (Prec.), 20.4 (Rec.), and 18.1 (F1.).'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 4155, '_id': 'b16cbd6ea13a42bead9e6732d5ce244a', '_collection_name': 'test'}, page_content='- The row labeled ""0 (gold)"" has values 95.5 (Corr.), 30.1 (Prec.), 57.1 (Rec.), and 39.4 (F1.).\n- The row labeled ""1"" has values 94.1 (Corr.), 26.1 (Prec.), 42.5 (Rec.), and 32.3 (F1.).\n- The row labeled ""2"" has values 94.0 (Corr.), 21.0 (Prec.), 31.4 (Rec.), and 25.2 (F1.).\n- The row labeled ""3"" has values 93.9 (Corr.), 16.3 (Prec.), 20.4 (Rec.), and 18.1 (F1.).'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 4155, '_id': '9e777ed8044d4b449a63e45cdffd8e56', '_collection_name': 'test'}, page_content='- The row labeled ""0 (gold)"" has values 95.5 (Corr.), 30.1 (Prec.), 57.1 (Rec.), and 39.4 (F1.).\n- The row labeled ""1"" has values 94.1 (Corr.), 26.1 (Prec.), 42.5 (Rec.), and 32.3 (F1.).\n- The row labeled ""2"" has values 94.0 (Corr.), 21.0 (Prec.), 31.4 (Rec.), and 25.2 (F1.).\n- The row labeled ""3"" has values 93.9 (Corr.), 16.3 (Prec.), 20.4 (Rec.), and 18.1 (F1.).'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': '5077b28c8d8d41eabf3b8e77a954949d', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': '80f4128715084ef8b6c1c30e3de503e8', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': 'f83c90f322654fedaaa45697ac763aa3', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.')]"
10002,"How many tables include ""F1"" as a metric?","[Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': '80f4128715084ef8b6c1c30e3de503e8', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': 'f83c90f322654fedaaa45697ac763aa3', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': 'cbf9c1fe83e24a248ba2021bedcc9cdb', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': '5077b28c8d8d41eabf3b8e77a954949d', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': 'c4c68069516f432d91129fc86dd2aa12', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': 'a2ff907be4384f49b3e7114ffcf90875', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': 'a935f1e31a86496fa8e430efbaef8c6a', '_collection_name': 'test'}, page_content='The table presents a comparison of different models, including GPT-4, ChatGPT, Alpaca-7B, LLaMA-7B, LLaMA-13B, and Vicuna-13B. It compares metrics such as Alignment, Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1-score (F1.) under both ""Micro"" and ""Macro"" settings. Each metric is accompanied by a smaller value in parentheses, indicating some form of sub-measurement or statistical variation. \n\nHere are the columns explained:\n\n1. **Align.** - Alignment score of the models.\n2. **Corr.** - Correctness score.\n3. **Micro:** \n   - **Prec.** - Precision under micro averaging.\n   - **Rec.** - Recall under micro averaging.\n   - **F1.** - F1-score under micro averaging.\n4. **Macro:** \n   - **Prec.** - Precision under macro averaging.\n   - **Rec.** - Recall under macro averaging.\n   - **F1.** - F1-score under macro averaging.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '586ca4cef5b24633b8e6cf92435af0ec', '_collection_name': 'test'}, page_content='The table presents a comparison of different models, including GPT-4, ChatGPT, Alpaca-7B, LLaMA-7B, LLaMA-13B, and Vicuna-13B. It compares metrics such as Alignment, Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1-score (F1.) under both ""Micro"" and ""Macro"" settings. Each metric is accompanied by a smaller value in parentheses, indicating some form of sub-measurement or statistical variation. \n\nHere are the columns explained:\n\n1. **Align.** - Alignment score of the models.\n2. **Corr.** - Correctness score.\n3. **Micro:** \n   - **Prec.** - Precision under micro averaging.\n   - **Rec.** - Recall under micro averaging.\n   - **F1.** - F1-score under micro averaging.\n4. **Macro:** \n   - **Prec.** - Precision under macro averaging.\n   - **Rec.** - Recall under macro averaging.\n   - **F1.** - F1-score under macro averaging.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': 'a499b22f3e9a4afe94b238c844892287', '_collection_name': 'test'}, page_content='The table presents a comparison of different models, including GPT-4, ChatGPT, Alpaca-7B, LLaMA-7B, LLaMA-13B, and Vicuna-13B. It compares metrics such as Alignment, Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1-score (F1.) under both ""Micro"" and ""Macro"" settings. Each metric is accompanied by a smaller value in parentheses, indicating some form of sub-measurement or statistical variation. \n\nHere are the columns explained:\n\n1. **Align.** - Alignment score of the models.\n2. **Corr.** - Correctness score.\n3. **Micro:** \n   - **Prec.** - Precision under micro averaging.\n   - **Rec.** - Recall under micro averaging.\n   - **F1.** - F1-score under micro averaging.\n4. **Macro:** \n   - **Prec.** - Precision under macro averaging.\n   - **Rec.** - Recall under macro averaging.\n   - **F1.** - F1-score under macro averaging.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '14025d0926e14806a63db374a6157f1f', '_collection_name': 'test'}, page_content='The table presents a comparison of different models, including GPT-4, ChatGPT, Alpaca-7B, LLaMA-7B, LLaMA-13B, and Vicuna-13B. It compares metrics such as Alignment, Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1-score (F1.) under both ""Micro"" and ""Macro"" settings. Each metric is accompanied by a smaller value in parentheses, indicating some form of sub-measurement or statistical variation. \n\nHere are the columns explained:\n\n1. **Align.** - Alignment score of the models.\n2. **Corr.** - Correctness score.\n3. **Micro:** \n   - **Prec.** - Precision under micro averaging.\n   - **Rec.** - Recall under micro averaging.\n   - **F1.** - F1-score under micro averaging.\n4. **Macro:** \n   - **Prec.** - Precision under macro averaging.\n   - **Rec.** - Recall under macro averaging.\n   - **F1.** - F1-score under macro averaging.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': 'b3ec090246da4b6b830e76f04e702468', '_collection_name': 'test'}, page_content='The table presents a comparison of different models, including GPT-4, ChatGPT, Alpaca-7B, LLaMA-7B, LLaMA-13B, and Vicuna-13B. It compares metrics such as Alignment, Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1-score (F1.) under both ""Micro"" and ""Macro"" settings. Each metric is accompanied by a smaller value in parentheses, indicating some form of sub-measurement or statistical variation. \n\nHere are the columns explained:\n\n1. **Align.** - Alignment score of the models.\n2. **Corr.** - Correctness score.\n3. **Micro:** \n   - **Prec.** - Precision under micro averaging.\n   - **Rec.** - Recall under micro averaging.\n   - **F1.** - F1-score under micro averaging.\n4. **Macro:** \n   - **Prec.** - Precision under macro averaging.\n   - **Rec.** - Recall under macro averaging.\n   - **F1.** - F1-score under macro averaging.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '0ebda3f81dff41508f35d68cc0991e02', '_collection_name': 'test'}, page_content='The table presents a comparison of different models, including GPT-4, ChatGPT, Alpaca-7B, LLaMA-7B, LLaMA-13B, and Vicuna-13B. It compares metrics such as Alignment, Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1-score (F1.) under both ""Micro"" and ""Macro"" settings. Each metric is accompanied by a smaller value in parentheses, indicating some form of sub-measurement or statistical variation. \n\nHere are the columns explained:\n\n1. **Align.** - Alignment score of the models.\n2. **Corr.** - Correctness score.\n3. **Micro:** \n   - **Prec.** - Precision under micro averaging.\n   - **Rec.** - Recall under micro averaging.\n   - **F1.** - F1-score under micro averaging.\n4. **Macro:** \n   - **Prec.** - Precision under macro averaging.\n   - **Rec.** - Recall under macro averaging.\n   - **F1.** - F1-score under macro averaging.'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 3490, '_id': '554a47df83064870b03d08d8bd3683e5', '_collection_name': 'test'}, page_content='The image uses precise color-coding for clarity: green check marks for correct matches, red crosses for incorrect ones, and arrows to represent the calculation of precision and recall.\nknowledge triplet from minimum knowledge set of the question. (See Figure  2 .) \nRecall We calculate citation recall for each knowledge (0 or 1) in minimum knowledge set, and average over all knowledge to get micro recall. Recall  $=1$   if and only if the knowledge if hit by a correct citation. (See Figure  2 .) \nWe average over all citations/knowledge in an an- swer, and average all answer-level precision/recall to get macro precision and recall. we calculate micro and macro F1-Score from corresponding pre- cision and recall. \n4.3 Text-Citation Alignment'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 3490, '_id': '9ad5a641aabb48d784d63e547c36b8ca', '_collection_name': 'test'}, page_content='The image uses precise color-coding for clarity: green check marks for correct matches, red crosses for incorrect ones, and arrows to represent the calculation of precision and recall.\nknowledge triplet from minimum knowledge set of the question. (See Figure  2 .) \nRecall We calculate citation recall for each knowledge (0 or 1) in minimum knowledge set, and average over all knowledge to get micro recall. Recall  $=1$   if and only if the knowledge if hit by a correct citation. (See Figure  2 .) \nWe average over all citations/knowledge in an an- swer, and average all answer-level precision/recall to get macro precision and recall. we calculate micro and macro F1-Score from corresponding pre- cision and recall. \n4.3 Text-Citation Alignment'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 3490, '_id': 'c0847333b75b4f86b5167a6d4a55182d', '_collection_name': 'test'}, page_content='The image uses precise color-coding for clarity: green check marks for correct matches, red crosses for incorrect ones, and arrows to represent the calculation of precision and recall.\nknowledge triplet from minimum knowledge set of the question. (See Figure  2 .) \nRecall We calculate citation recall for each knowledge (0 or 1) in minimum knowledge set, and average over all knowledge to get micro recall. Recall  $=1$   if and only if the knowledge if hit by a correct citation. (See Figure  2 .) \nWe average over all citations/knowledge in an an- swer, and average all answer-level precision/recall to get macro precision and recall. we calculate micro and macro F1-Score from corresponding pre- cision and recall. \n4.3 Text-Citation Alignment')]"
10003,"From the paper, which temperature gives ChatGPT the highest alignment score?","[Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': 'c4c68069516f432d91129fc86dd2aa12', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': '5077b28c8d8d41eabf3b8e77a954949d', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': 'a2ff907be4384f49b3e7114ffcf90875', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': 'f83c90f322654fedaaa45697ac763aa3', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': '80f4128715084ef8b6c1c30e3de503e8', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '7', 'doc_source': '2310.05634v2', 'start_index': 2755, '_id': 'cbf9c1fe83e24a248ba2021bedcc9cdb', '_collection_name': 'test'}, page_content='There are four lines indicating different metrics:\n- **Precision** (blue circles), which decreases as retrieval accuracy decreases.\n- **Recall** (orange curve), also decreasing.\n- **F1 Score** (green inverted triangles), following a similar downward trend.\n- **Correctness** (red triangles), which starts high and slightly decreases.\n\nCorrectness has the highest score across all levels of retrieval accuracy, while precision shows the lowest.\nThe table is comparing the ""Alignment"" and ""Human Avg."" scores of three models: ChatGPT(0.5), LLaMA-7B, and Vicuna-13B. \n\n- ChatGPT(0.5) has an Alignment score of 84.5 and a Human Avg. score of 82.0.\n- LLaMA-7B has an Alignment score of 47.8 and a Human Avg. score of 45.5.\n- Vicuna-13B has an Alignment score of 66.9 and a Human Avg. score of 64.5.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': 'a499b22f3e9a4afe94b238c844892287', '_collection_name': 'test'}, page_content='The table presents a comparison of different models, including GPT-4, ChatGPT, Alpaca-7B, LLaMA-7B, LLaMA-13B, and Vicuna-13B. It compares metrics such as Alignment, Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1-score (F1.) under both ""Micro"" and ""Macro"" settings. Each metric is accompanied by a smaller value in parentheses, indicating some form of sub-measurement or statistical variation. \n\nHere are the columns explained:\n\n1. **Align.** - Alignment score of the models.\n2. **Corr.** - Correctness score.\n3. **Micro:** \n   - **Prec.** - Precision under micro averaging.\n   - **Rec.** - Recall under micro averaging.\n   - **F1.** - F1-score under micro averaging.\n4. **Macro:** \n   - **Prec.** - Precision under macro averaging.\n   - **Rec.** - Recall under macro averaging.\n   - **F1.** - F1-score under macro averaging.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': 'a935f1e31a86496fa8e430efbaef8c6a', '_collection_name': 'test'}, page_content='The table presents a comparison of different models, including GPT-4, ChatGPT, Alpaca-7B, LLaMA-7B, LLaMA-13B, and Vicuna-13B. It compares metrics such as Alignment, Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1-score (F1.) under both ""Micro"" and ""Macro"" settings. Each metric is accompanied by a smaller value in parentheses, indicating some form of sub-measurement or statistical variation. \n\nHere are the columns explained:\n\n1. **Align.** - Alignment score of the models.\n2. **Corr.** - Correctness score.\n3. **Micro:** \n   - **Prec.** - Precision under micro averaging.\n   - **Rec.** - Recall under micro averaging.\n   - **F1.** - F1-score under micro averaging.\n4. **Macro:** \n   - **Prec.** - Precision under macro averaging.\n   - **Rec.** - Recall under macro averaging.\n   - **F1.** - F1-score under macro averaging.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': 'b3ec090246da4b6b830e76f04e702468', '_collection_name': 'test'}, page_content='The table presents a comparison of different models, including GPT-4, ChatGPT, Alpaca-7B, LLaMA-7B, LLaMA-13B, and Vicuna-13B. It compares metrics such as Alignment, Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1-score (F1.) under both ""Micro"" and ""Macro"" settings. Each metric is accompanied by a smaller value in parentheses, indicating some form of sub-measurement or statistical variation. \n\nHere are the columns explained:\n\n1. **Align.** - Alignment score of the models.\n2. **Corr.** - Correctness score.\n3. **Micro:** \n   - **Prec.** - Precision under micro averaging.\n   - **Rec.** - Recall under micro averaging.\n   - **F1.** - F1-score under micro averaging.\n4. **Macro:** \n   - **Prec.** - Precision under macro averaging.\n   - **Rec.** - Recall under macro averaging.\n   - **F1.** - F1-score under macro averaging.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '586ca4cef5b24633b8e6cf92435af0ec', '_collection_name': 'test'}, page_content='The table presents a comparison of different models, including GPT-4, ChatGPT, Alpaca-7B, LLaMA-7B, LLaMA-13B, and Vicuna-13B. It compares metrics such as Alignment, Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1-score (F1.) under both ""Micro"" and ""Macro"" settings. Each metric is accompanied by a smaller value in parentheses, indicating some form of sub-measurement or statistical variation. \n\nHere are the columns explained:\n\n1. **Align.** - Alignment score of the models.\n2. **Corr.** - Correctness score.\n3. **Micro:** \n   - **Prec.** - Precision under micro averaging.\n   - **Rec.** - Recall under micro averaging.\n   - **F1.** - F1-score under micro averaging.\n4. **Macro:** \n   - **Prec.** - Precision under macro averaging.\n   - **Rec.** - Recall under macro averaging.\n   - **F1.** - F1-score under macro averaging.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '14025d0926e14806a63db374a6157f1f', '_collection_name': 'test'}, page_content='The table presents a comparison of different models, including GPT-4, ChatGPT, Alpaca-7B, LLaMA-7B, LLaMA-13B, and Vicuna-13B. It compares metrics such as Alignment, Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1-score (F1.) under both ""Micro"" and ""Macro"" settings. Each metric is accompanied by a smaller value in parentheses, indicating some form of sub-measurement or statistical variation. \n\nHere are the columns explained:\n\n1. **Align.** - Alignment score of the models.\n2. **Corr.** - Correctness score.\n3. **Micro:** \n   - **Prec.** - Precision under micro averaging.\n   - **Rec.** - Recall under micro averaging.\n   - **F1.** - F1-score under micro averaging.\n4. **Macro:** \n   - **Prec.** - Precision under macro averaging.\n   - **Rec.** - Recall under macro averaging.\n   - **F1.** - F1-score under macro averaging.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 0, '_id': '0ebda3f81dff41508f35d68cc0991e02', '_collection_name': 'test'}, page_content='The table presents a comparison of different models, including GPT-4, ChatGPT, Alpaca-7B, LLaMA-7B, LLaMA-13B, and Vicuna-13B. It compares metrics such as Alignment, Correctness (Corr.), Precision (Prec.), Recall (Rec.), and F1-score (F1.) under both ""Micro"" and ""Macro"" settings. Each metric is accompanied by a smaller value in parentheses, indicating some form of sub-measurement or statistical variation. \n\nHere are the columns explained:\n\n1. **Align.** - Alignment score of the models.\n2. **Corr.** - Correctness score.\n3. **Micro:** \n   - **Prec.** - Precision under micro averaging.\n   - **Rec.** - Recall under micro averaging.\n   - **F1.** - F1-score under micro averaging.\n4. **Macro:** \n   - **Prec.** - Precision under macro averaging.\n   - **Rec.** - Recall under macro averaging.\n   - **F1.** - F1-score under macro averaging.'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 3225, '_id': '6822661cfd0e403e8e4800761df67e72', '_collection_name': 'test'}, page_content='1. **GPT-4 (0.5)**\n   - Coh.: 4.48\n   - Con.: 4.89\n   - Flu.: 4.64\n   - Rel.: 4.72\n\n2. **ChatGPT (0.1)**\n   - Coh.: 4.57\n   - Con.: 4.94\n   - Flu.: 4.69\n   - Rel.: 4.83\n\n3. **ChatGPT (0.5)**\n   - Coh.: 4.57\n   - Con.: 4.94\n   - Flu.: 4.71\n   - Rel.: 4.81\n\n4. **ChatGPT (0.9)**\n   - Coh.: 4.52\n   - Con.: 4.91\n   - Flu.: 4.67\n   - Rel.: 4.79\n\n5. **Alpaca-7B**\n   - Coh.: 4.10\n   - Con.: 4.46\n   - Flu.: 4.23\n   - Rel.: 3.76\n\n6. **LLaMa-7B**\n   - Coh.: 3.06\n   - Con.: 3.79\n   - Flu.: 3.62\n   - Rel.: 2.96\n\n7. **LLaMa-13B**\n   - Coh.: 3.60\n   - Con.: 4.23\n   - Flu.: 3.94\n   - Rel.: 3.56\n\n8. **Vicuna-13B**\n   - Coh.: 3.67\n   - Con.: 4.50\n   - Flu.: 3.96\n   - Rel.: 3.64\n\nOverall, the ChatGPT variants, particularly ChatGPT (0.1) and ChatGPT (0.5), tend to have higher scores across all metrics compared to the other models.\nThe table displays the following data across five columns: ""Removed"", ""Corr."", ""Prec."", ""Rec."", and ""F1.""'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 3225, '_id': 'e13836a21ada41ad912e755510851073', '_collection_name': 'test'}, page_content='1. **GPT-4 (0.5)**\n   - Coh.: 4.48\n   - Con.: 4.89\n   - Flu.: 4.64\n   - Rel.: 4.72\n\n2. **ChatGPT (0.1)**\n   - Coh.: 4.57\n   - Con.: 4.94\n   - Flu.: 4.69\n   - Rel.: 4.83\n\n3. **ChatGPT (0.5)**\n   - Coh.: 4.57\n   - Con.: 4.94\n   - Flu.: 4.71\n   - Rel.: 4.81\n\n4. **ChatGPT (0.9)**\n   - Coh.: 4.52\n   - Con.: 4.91\n   - Flu.: 4.67\n   - Rel.: 4.79\n\n5. **Alpaca-7B**\n   - Coh.: 4.10\n   - Con.: 4.46\n   - Flu.: 4.23\n   - Rel.: 3.76\n\n6. **LLaMa-7B**\n   - Coh.: 3.06\n   - Con.: 3.79\n   - Flu.: 3.62\n   - Rel.: 2.96\n\n7. **LLaMa-13B**\n   - Coh.: 3.60\n   - Con.: 4.23\n   - Flu.: 3.94\n   - Rel.: 3.56\n\n8. **Vicuna-13B**\n   - Coh.: 3.67\n   - Con.: 4.50\n   - Flu.: 3.96\n   - Rel.: 3.64\n\nOverall, the ChatGPT variants, particularly ChatGPT (0.1) and ChatGPT (0.5), tend to have higher scores across all metrics compared to the other models.\nThe table displays the following data across five columns: ""Removed"", ""Corr."", ""Prec."", ""Rec."", and ""F1.""'), Document(metadata={'page_source': '6', 'doc_source': '2310.05634v2', 'start_index': 3225, '_id': 'bdc6ad4eda1f4ed6b57c22b8f65819e4', '_collection_name': 'test'}, page_content='1. **GPT-4 (0.5)**\n   - Coh.: 4.48\n   - Con.: 4.89\n   - Flu.: 4.64\n   - Rel.: 4.72\n\n2. **ChatGPT (0.1)**\n   - Coh.: 4.57\n   - Con.: 4.94\n   - Flu.: 4.69\n   - Rel.: 4.83\n\n3. **ChatGPT (0.5)**\n   - Coh.: 4.57\n   - Con.: 4.94\n   - Flu.: 4.71\n   - Rel.: 4.81\n\n4. **ChatGPT (0.9)**\n   - Coh.: 4.52\n   - Con.: 4.91\n   - Flu.: 4.67\n   - Rel.: 4.79\n\n5. **Alpaca-7B**\n   - Coh.: 4.10\n   - Con.: 4.46\n   - Flu.: 4.23\n   - Rel.: 3.76\n\n6. **LLaMa-7B**\n   - Coh.: 3.06\n   - Con.: 3.79\n   - Flu.: 3.62\n   - Rel.: 2.96\n\n7. **LLaMa-13B**\n   - Coh.: 3.60\n   - Con.: 4.23\n   - Flu.: 3.94\n   - Rel.: 3.56\n\n8. **Vicuna-13B**\n   - Coh.: 3.67\n   - Con.: 4.50\n   - Flu.: 3.96\n   - Rel.: 3.64\n\nOverall, the ChatGPT variants, particularly ChatGPT (0.1) and ChatGPT (0.5), tend to have higher scores across all metrics compared to the other models.\nThe table displays the following data across five columns: ""Removed"", ""Corr."", ""Prec."", ""Rec."", and ""F1.""')]"
10004,"For dataset construction, which step takes the most word to describe than the others. ","[Document(metadata={'page_source': '11', 'doc_source': '2310.05634v2', 'start_index': 1262, '_id': 'd085db0eb1fe4d06a86ee1d346c7eb42', '_collection_name': 'test'}, page_content='A Dataset Construction \nIn this section, we will explain the detailed process and algorithms for the automatic dataset construc- tion pipeline. Using this pipeline, we are able to construct datasets with a greater scale or in other domains. \nA.1 Person Selection'), Document(metadata={'page_source': '11', 'doc_source': '2310.05634v2', 'start_index': 1262, '_id': '8aa02167a62a43ca898c089444dfdcc9', '_collection_name': 'test'}, page_content='A Dataset Construction \nIn this section, we will explain the detailed process and algorithms for the automatic dataset construc- tion pipeline. Using this pipeline, we are able to construct datasets with a greater scale or in other domains. \nA.1 Person Selection'), Document(metadata={'page_source': '11', 'doc_source': '2310.05634v2', 'start_index': 1262, '_id': '417f0d18ce48464abf6cbb2ebc761016', '_collection_name': 'test'}, page_content='A Dataset Construction \nIn this section, we will explain the detailed process and algorithms for the automatic dataset construc- tion pipeline. Using this pipeline, we are able to construct datasets with a greater scale or in other domains. \nA.1 Person Selection'), Document(metadata={'page_source': '11', 'doc_source': '2310.05634v2', 'start_index': 1262, '_id': '2cad5ba400d64a3e8fbbadf91d44594d', '_collection_name': 'test'}, page_content='A Dataset Construction \nIn this section, we will explain the detailed process and algorithms for the automatic dataset construc- tion pipeline. Using this pipeline, we are able to construct datasets with a greater scale or in other domains. \nA.1 Person Selection'), Document(metadata={'page_source': '11', 'doc_source': '2310.05634v2', 'start_index': 1262, '_id': '680cf72f24ee423f924f71de2737d634', '_collection_name': 'test'}, page_content='A Dataset Construction \nIn this section, we will explain the detailed process and algorithms for the automatic dataset construc- tion pipeline. Using this pipeline, we are able to construct datasets with a greater scale or in other domains. \nA.1 Person Selection'), Document(metadata={'page_source': '11', 'doc_source': '2310.05634v2', 'start_index': 1262, '_id': '17813dc0a8434dae8710436ab6b3a56f', '_collection_name': 'test'}, page_content='A Dataset Construction \nIn this section, we will explain the detailed process and algorithms for the automatic dataset construc- tion pipeline. Using this pipeline, we are able to construct datasets with a greater scale or in other domains. \nA.1 Person Selection'), Document(metadata={'page_source': '13', 'doc_source': '2310.05634v2', 'start_index': 776, '_id': '36a51d3d8a3b44a8b04fdfdab73d5ac5', '_collection_name': 'test'}, page_content='C.1 Dataset Evaluation \nTo evaluate the dataset quality, we have two indi- vidual annotators who are proficient in the English language. Below are the exact method for evaluat- ing each metric: \n•  Authenticity . We ask the annotators to check from WikiPedia and understand the back- ground stories of the mentioned people, and decide if the generated question matches the background story. Each question is assigned score 1 if it matches the background story, and score 0 if there is contradiction. \n•  Relevance . After understanding the back- ground stories, we ask the annotators to label each piece of knowledge from the minimum knowledge set. A piece of knowledge is la- beled 1 if the annotator thinks it is necessary to answer the question, and 0 if it is redun- dant. The relevance score of a question is the ratio of number of necessary knowledge to the number of knowledge in the minimum set.'), Document(metadata={'page_source': '13', 'doc_source': '2310.05634v2', 'start_index': 776, '_id': 'a15cd9b40ed143f9a4b71fd55ae92e6d', '_collection_name': 'test'}, page_content='C.1 Dataset Evaluation \nTo evaluate the dataset quality, we have two indi- vidual annotators who are proficient in the English language. Below are the exact method for evaluat- ing each metric: \n•  Authenticity . We ask the annotators to check from WikiPedia and understand the back- ground stories of the mentioned people, and decide if the generated question matches the background story. Each question is assigned score 1 if it matches the background story, and score 0 if there is contradiction. \n•  Relevance . After understanding the back- ground stories, we ask the annotators to label each piece of knowledge from the minimum knowledge set. A piece of knowledge is la- beled 1 if the annotator thinks it is necessary to answer the question, and 0 if it is redun- dant. The relevance score of a question is the ratio of number of necessary knowledge to the number of knowledge in the minimum set.'), Document(metadata={'page_source': '13', 'doc_source': '2310.05634v2', 'start_index': 776, '_id': '38978546810645fda41519bb2bb9d788', '_collection_name': 'test'}, page_content='C.1 Dataset Evaluation \nTo evaluate the dataset quality, we have two indi- vidual annotators who are proficient in the English language. Below are the exact method for evaluat- ing each metric: \n•  Authenticity . We ask the annotators to check from WikiPedia and understand the back- ground stories of the mentioned people, and decide if the generated question matches the background story. Each question is assigned score 1 if it matches the background story, and score 0 if there is contradiction. \n•  Relevance . After understanding the back- ground stories, we ask the annotators to label each piece of knowledge from the minimum knowledge set. A piece of knowledge is la- beled 1 if the annotator thinks it is necessary to answer the question, and 0 if it is redun- dant. The relevance score of a question is the ratio of number of necessary knowledge to the number of knowledge in the minimum set.'), Document(metadata={'page_source': '13', 'doc_source': '2310.05634v2', 'start_index': 776, '_id': '61744bbca1cb4b3f92b092300726d697', '_collection_name': 'test'}, page_content='C.1 Dataset Evaluation \nTo evaluate the dataset quality, we have two indi- vidual annotators who are proficient in the English language. Below are the exact method for evaluat- ing each metric: \n•  Authenticity . We ask the annotators to check from WikiPedia and understand the back- ground stories of the mentioned people, and decide if the generated question matches the background story. Each question is assigned score 1 if it matches the background story, and score 0 if there is contradiction. \n•  Relevance . After understanding the back- ground stories, we ask the annotators to label each piece of knowledge from the minimum knowledge set. A piece of knowledge is la- beled 1 if the annotator thinks it is necessary to answer the question, and 0 if it is redun- dant. The relevance score of a question is the ratio of number of necessary knowledge to the number of knowledge in the minimum set.'), Document(metadata={'page_source': '13', 'doc_source': '2310.05634v2', 'start_index': 776, '_id': 'f36440d2efd74eecafdead344e2a8bc8', '_collection_name': 'test'}, page_content='C.1 Dataset Evaluation \nTo evaluate the dataset quality, we have two indi- vidual annotators who are proficient in the English language. Below are the exact method for evaluat- ing each metric: \n•  Authenticity . We ask the annotators to check from WikiPedia and understand the back- ground stories of the mentioned people, and decide if the generated question matches the background story. Each question is assigned score 1 if it matches the background story, and score 0 if there is contradiction. \n•  Relevance . After understanding the back- ground stories, we ask the annotators to label each piece of knowledge from the minimum knowledge set. A piece of knowledge is la- beled 1 if the annotator thinks it is necessary to answer the question, and 0 if it is redun- dant. The relevance score of a question is the ratio of number of necessary knowledge to the number of knowledge in the minimum set.'), Document(metadata={'page_source': '13', 'doc_source': '2310.05634v2', 'start_index': 776, '_id': 'db219bbd5fae457ebf855e2a79790a66', '_collection_name': 'test'}, page_content='C.1 Dataset Evaluation \nTo evaluate the dataset quality, we have two indi- vidual annotators who are proficient in the English language. Below are the exact method for evaluat- ing each metric: \n•  Authenticity . We ask the annotators to check from WikiPedia and understand the back- ground stories of the mentioned people, and decide if the generated question matches the background story. Each question is assigned score 1 if it matches the background story, and score 0 if there is contradiction. \n•  Relevance . After understanding the back- ground stories, we ask the annotators to label each piece of knowledge from the minimum knowledge set. A piece of knowledge is la- beled 1 if the annotator thinks it is necessary to answer the question, and 0 if it is redun- dant. The relevance score of a question is the ratio of number of necessary knowledge to the number of knowledge in the minimum set.'), Document(metadata={'page_source': '2', 'doc_source': '2310.05634v2', 'start_index': 2839, '_id': 'a74de255a7924783a1b98d3da9ba04b1', '_collection_name': 'test'}, page_content='Each metric shows how both General and Specific categories are scored, with full scores given in parentheses next to each metric name for reference.\nEvaluation of Dataset Quality We evaluate the BioKaLMA dataset on the following four metrics to ensure the quality of the dataset: 1)  Authen- ticity : The generated questions should accurately reflect the objective facts. 2)  Relevance : Each min- imum knowledge set should provide support to the corresponding question. Each piece of knowledge from the minimum knowledge set is not redundant. 3)  Naturalness : The generated question should be concise and understandable by human readers. 4) Significance : The generated question should be meaningful and helpful to users. \nTo our best knowledge, there is no perfect au- tomatic evaluation for these metrics. Naturalness and significance are subjective. Hence, we apply human evaluation to ensure the dataset quality.'), Document(metadata={'page_source': '2', 'doc_source': '2310.05634v2', 'start_index': 2839, '_id': 'd5113a83e9214bc5bdadf0820b969e32', '_collection_name': 'test'}, page_content='Each metric shows how both General and Specific categories are scored, with full scores given in parentheses next to each metric name for reference.\nEvaluation of Dataset Quality We evaluate the BioKaLMA dataset on the following four metrics to ensure the quality of the dataset: 1)  Authen- ticity : The generated questions should accurately reflect the objective facts. 2)  Relevance : Each min- imum knowledge set should provide support to the corresponding question. Each piece of knowledge from the minimum knowledge set is not redundant. 3)  Naturalness : The generated question should be concise and understandable by human readers. 4) Significance : The generated question should be meaningful and helpful to users. \nTo our best knowledge, there is no perfect au- tomatic evaluation for these metrics. Naturalness and significance are subjective. Hence, we apply human evaluation to ensure the dataset quality.'), Document(metadata={'page_source': '2', 'doc_source': '2310.05634v2', 'start_index': 2839, '_id': 'd41e32efeb464c1e9ee3f4376e2e76ca', '_collection_name': 'test'}, page_content='Each metric shows how both General and Specific categories are scored, with full scores given in parentheses next to each metric name for reference.\nEvaluation of Dataset Quality We evaluate the BioKaLMA dataset on the following four metrics to ensure the quality of the dataset: 1)  Authen- ticity : The generated questions should accurately reflect the objective facts. 2)  Relevance : Each min- imum knowledge set should provide support to the corresponding question. Each piece of knowledge from the minimum knowledge set is not redundant. 3)  Naturalness : The generated question should be concise and understandable by human readers. 4) Significance : The generated question should be meaningful and helpful to users. \nTo our best knowledge, there is no perfect au- tomatic evaluation for these metrics. Naturalness and significance are subjective. Hence, we apply human evaluation to ensure the dataset quality.')]"
10005,"According to author's definition on conscious incompetence, when can a sentence map to both [NA] and a list of sub-graph knowledge?","[Document(metadata={'page_source': '1', 'doc_source': '2310.05634v2', 'start_index': 3851, '_id': 'd58df3e6ffe747a69670c145944adc20', '_collection_name': 'test'}, page_content='Setting of Conscious Incompetence We extend this task setting to include conscious incompetence. Given the same input, each sentence    $s$   in the output text  $t$   can map to a Not Applicable Citation (we use [NA] to represent it) if it includes some knowledge to be verified, but the knowledge is absent in the knowledge graph    $G$  . A sentence can map to both [NA] and a list of sub-graph knowledge if it can be partially verified by the knowledge graph    $G$  . [NA] is not a citation on conventional means, but a indicator of knowledge gap.'), Document(metadata={'page_source': '1', 'doc_source': '2310.05634v2', 'start_index': 3851, '_id': '7f61014dce754fba9f1cd42fc935a42d', '_collection_name': 'test'}, page_content='Setting of Conscious Incompetence We extend this task setting to include conscious incompetence. Given the same input, each sentence    $s$   in the output text  $t$   can map to a Not Applicable Citation (we use [NA] to represent it) if it includes some knowledge to be verified, but the knowledge is absent in the knowledge graph    $G$  . A sentence can map to both [NA] and a list of sub-graph knowledge if it can be partially verified by the knowledge graph    $G$  . [NA] is not a citation on conventional means, but a indicator of knowledge gap.'), Document(metadata={'page_source': '1', 'doc_source': '2310.05634v2', 'start_index': 3851, '_id': 'ce74b6cda36247a0a0811f9358dc200e', '_collection_name': 'test'}, page_content='Setting of Conscious Incompetence We extend this task setting to include conscious incompetence. Given the same input, each sentence    $s$   in the output text  $t$   can map to a Not Applicable Citation (we use [NA] to represent it) if it includes some knowledge to be verified, but the knowledge is absent in the knowledge graph    $G$  . A sentence can map to both [NA] and a list of sub-graph knowledge if it can be partially verified by the knowledge graph    $G$  . [NA] is not a citation on conventional means, but a indicator of knowledge gap.'), Document(metadata={'page_source': '1', 'doc_source': '2310.05634v2', 'start_index': 3851, '_id': 'eed8b3b2d535406ab949e3202da6efda', '_collection_name': 'test'}, page_content='Setting of Conscious Incompetence We extend this task setting to include conscious incompetence. Given the same input, each sentence    $s$   in the output text  $t$   can map to a Not Applicable Citation (we use [NA] to represent it) if it includes some knowledge to be verified, but the knowledge is absent in the knowledge graph    $G$  . A sentence can map to both [NA] and a list of sub-graph knowledge if it can be partially verified by the knowledge graph    $G$  . [NA] is not a citation on conventional means, but a indicator of knowledge gap.'), Document(metadata={'page_source': '1', 'doc_source': '2310.05634v2', 'start_index': 3851, '_id': '18a946b4609f403f986b399b8a7feaaa', '_collection_name': 'test'}, page_content='Setting of Conscious Incompetence We extend this task setting to include conscious incompetence. Given the same input, each sentence    $s$   in the output text  $t$   can map to a Not Applicable Citation (we use [NA] to represent it) if it includes some knowledge to be verified, but the knowledge is absent in the knowledge graph    $G$  . A sentence can map to both [NA] and a list of sub-graph knowledge if it can be partially verified by the knowledge graph    $G$  . [NA] is not a citation on conventional means, but a indicator of knowledge gap.'), Document(metadata={'page_source': '1', 'doc_source': '2310.05634v2', 'start_index': 3851, '_id': '6e78c59f68e447779a6d5deb18c7b4a3', '_collection_name': 'test'}, page_content='Setting of Conscious Incompetence We extend this task setting to include conscious incompetence. Given the same input, each sentence    $s$   in the output text  $t$   can map to a Not Applicable Citation (we use [NA] to represent it) if it includes some knowledge to be verified, but the knowledge is absent in the knowledge graph    $G$  . A sentence can map to both [NA] and a list of sub-graph knowledge if it can be partially verified by the knowledge graph    $G$  . [NA] is not a citation on conventional means, but a indicator of knowledge gap.'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 5134, '_id': '10bcc67976e84b30a08108d38be1b3e2', '_collection_name': 'test'}, page_content='4.4 Conscious Incompetence Evaluation \nTheoretically, each [NA] mark should map to a piece of knowledge absent from the retrieved knowledge graph. However, it is difficult to identify if sentence requires any absent knowledge since there is no ground truth. Therefore, we conduct a three-round experiment to manually create ground truth for absent knowledge. In round 1, we select one knowledge from the minimum knowledge set, and remove it from the ground-truth knowledge graph. We let the LLMs attribute to this incomplete knowledge graph to generate answers, whereby the removed knowledge forms the “absent knowledge ground truth”. In subsequent rounds, we each re- move one additional knowledge from the minimum knowledge set, simulating a knowledge graph with more serious coverage problem.'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 5134, '_id': '6750e5505fca47c8aa71258cbd8417e9', '_collection_name': 'test'}, page_content='4.4 Conscious Incompetence Evaluation \nTheoretically, each [NA] mark should map to a piece of knowledge absent from the retrieved knowledge graph. However, it is difficult to identify if sentence requires any absent knowledge since there is no ground truth. Therefore, we conduct a three-round experiment to manually create ground truth for absent knowledge. In round 1, we select one knowledge from the minimum knowledge set, and remove it from the ground-truth knowledge graph. We let the LLMs attribute to this incomplete knowledge graph to generate answers, whereby the removed knowledge forms the “absent knowledge ground truth”. In subsequent rounds, we each re- move one additional knowledge from the minimum knowledge set, simulating a knowledge graph with more serious coverage problem.'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 5134, '_id': '133b12bfa37e45a4b6c313d265d22416', '_collection_name': 'test'}, page_content='4.4 Conscious Incompetence Evaluation \nTheoretically, each [NA] mark should map to a piece of knowledge absent from the retrieved knowledge graph. However, it is difficult to identify if sentence requires any absent knowledge since there is no ground truth. Therefore, we conduct a three-round experiment to manually create ground truth for absent knowledge. In round 1, we select one knowledge from the minimum knowledge set, and remove it from the ground-truth knowledge graph. We let the LLMs attribute to this incomplete knowledge graph to generate answers, whereby the removed knowledge forms the “absent knowledge ground truth”. In subsequent rounds, we each re- move one additional knowledge from the minimum knowledge set, simulating a knowledge graph with more serious coverage problem.'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 5134, '_id': '690f1f6a73e74f31abeffb94556d67aa', '_collection_name': 'test'}, page_content='4.4 Conscious Incompetence Evaluation \nTheoretically, each [NA] mark should map to a piece of knowledge absent from the retrieved knowledge graph. However, it is difficult to identify if sentence requires any absent knowledge since there is no ground truth. Therefore, we conduct a three-round experiment to manually create ground truth for absent knowledge. In round 1, we select one knowledge from the minimum knowledge set, and remove it from the ground-truth knowledge graph. We let the LLMs attribute to this incomplete knowledge graph to generate answers, whereby the removed knowledge forms the “absent knowledge ground truth”. In subsequent rounds, we each re- move one additional knowledge from the minimum knowledge set, simulating a knowledge graph with more serious coverage problem.'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 5134, '_id': 'bfceef1e09414638af1d31ee520216c5', '_collection_name': 'test'}, page_content='4.4 Conscious Incompetence Evaluation \nTheoretically, each [NA] mark should map to a piece of knowledge absent from the retrieved knowledge graph. However, it is difficult to identify if sentence requires any absent knowledge since there is no ground truth. Therefore, we conduct a three-round experiment to manually create ground truth for absent knowledge. In round 1, we select one knowledge from the minimum knowledge set, and remove it from the ground-truth knowledge graph. We let the LLMs attribute to this incomplete knowledge graph to generate answers, whereby the removed knowledge forms the “absent knowledge ground truth”. In subsequent rounds, we each re- move one additional knowledge from the minimum knowledge set, simulating a knowledge graph with more serious coverage problem.'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 5134, '_id': 'ed31d151095f41d5831cd2b05e5bc42e', '_collection_name': 'test'}, page_content='4.4 Conscious Incompetence Evaluation \nTheoretically, each [NA] mark should map to a piece of knowledge absent from the retrieved knowledge graph. However, it is difficult to identify if sentence requires any absent knowledge since there is no ground truth. Therefore, we conduct a three-round experiment to manually create ground truth for absent knowledge. In round 1, we select one knowledge from the minimum knowledge set, and remove it from the ground-truth knowledge graph. We let the LLMs attribute to this incomplete knowledge graph to generate answers, whereby the removed knowledge forms the “absent knowledge ground truth”. In subsequent rounds, we each re- move one additional knowledge from the minimum knowledge set, simulating a knowledge graph with more serious coverage problem.'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 2547, '_id': '944a781f2ca141b094842aa1169fc9e0', '_collection_name': 'test'}, page_content='- **Model Output**: It shows two sentences with various knowledge elements marked by k1, k3, k6, etc., and tagged with [NA] for those marked as ""conscious incompetence.""\n  - Sentence1 has k1 tagged with [NA].\n  - Sentence2 has k3 and k6 tagged with [NA].\n  - Sentence3, absent of [NA], has elements k6 and k9.\n\n- **Absent Knowledge Set**: It includes knowledge elements k2, k4, and k5 that are associated with the absence of knowledge.\n\n- **NLI (Natural Language Inference) Evaluation**:\n  - **[NA] Precision**: Evaluates the accuracy of [NA] tagging in sentences when compared to the absent knowledge set.\n    - Sentence1\'s correct identification of absent knowledge (k2) achieves 1 out of 2 attempted, resulting in a precision of 0.5.\n  - **[NA] Recall**: Measures how effectively the model identified all items in the absent knowledge set as [NA].\n    - Sentence2\'s hit of k2 against an all of 3 absence marks results in a recall of 0.33.'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 2547, '_id': 'f1b826cfd235417687bfe8e662814273', '_collection_name': 'test'}, page_content='- **Model Output**: It shows two sentences with various knowledge elements marked by k1, k3, k6, etc., and tagged with [NA] for those marked as ""conscious incompetence.""\n  - Sentence1 has k1 tagged with [NA].\n  - Sentence2 has k3 and k6 tagged with [NA].\n  - Sentence3, absent of [NA], has elements k6 and k9.\n\n- **Absent Knowledge Set**: It includes knowledge elements k2, k4, and k5 that are associated with the absence of knowledge.\n\n- **NLI (Natural Language Inference) Evaluation**:\n  - **[NA] Precision**: Evaluates the accuracy of [NA] tagging in sentences when compared to the absent knowledge set.\n    - Sentence1\'s correct identification of absent knowledge (k2) achieves 1 out of 2 attempted, resulting in a precision of 0.5.\n  - **[NA] Recall**: Measures how effectively the model identified all items in the absent knowledge set as [NA].\n    - Sentence2\'s hit of k2 against an all of 3 absence marks results in a recall of 0.33.'), Document(metadata={'page_source': '4', 'doc_source': '2310.05634v2', 'start_index': 2547, '_id': '2a20479f05ca416aad95c82d69e97a86', '_collection_name': 'test'}, page_content='- **Model Output**: It shows two sentences with various knowledge elements marked by k1, k3, k6, etc., and tagged with [NA] for those marked as ""conscious incompetence.""\n  - Sentence1 has k1 tagged with [NA].\n  - Sentence2 has k3 and k6 tagged with [NA].\n  - Sentence3, absent of [NA], has elements k6 and k9.\n\n- **Absent Knowledge Set**: It includes knowledge elements k2, k4, and k5 that are associated with the absence of knowledge.\n\n- **NLI (Natural Language Inference) Evaluation**:\n  - **[NA] Precision**: Evaluates the accuracy of [NA] tagging in sentences when compared to the absent knowledge set.\n    - Sentence1\'s correct identification of absent knowledge (k2) achieves 1 out of 2 attempted, resulting in a precision of 0.5.\n  - **[NA] Recall**: Measures how effectively the model identified all items in the absent knowledge set as [NA].\n    - Sentence2\'s hit of k2 against an all of 3 absence marks results in a recall of 0.33.')]"
10006,"In figure 4, which nodes are retrieved by RAPTOR for both questions?","[Document(metadata={'page_source': '6', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': 'dd562d3f60d34a8f924b1e2c5c94f864', '_collection_name': 'test'}, page_content='The image is an illustration of the querying process by RAPTOR, a system for retrieving information. It shows how RAPTOR retrieves information for two questions about the Cinderella story. The diagram features nodes and arrows with different colors representing selections by RAPTOR and DPR (Dense Passage Retrieval). The nodes are arranged in a hierarchical structure with numbers, and the highlighted nodes indicate RAPTOR\'s selections, differentiated for two distinct questions: ""What is the central theme of the story?"" and ""How did Cinderella find a happy ending?"".\n\nKey components:\n\n- Orange and purple highlighted nodes: Indicate RAPTOR\'s selections for Question 1 and Question 2.\n- Arrows: Point to DPR’s leaf nodes for each question, with orange arrows for Question 1 and purple arrows for Question 2.\n- RAPTOR\'s context is shown to often encompass the information retrieved by DPR.'), Document(metadata={'page_source': '6', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': 'a384f3b42699438ab68b978baab4791b', '_collection_name': 'test'}, page_content='The image is an illustration of the querying process by RAPTOR, a system for retrieving information. It shows how RAPTOR retrieves information for two questions about the Cinderella story. The diagram features nodes and arrows with different colors representing selections by RAPTOR and DPR (Dense Passage Retrieval). The nodes are arranged in a hierarchical structure with numbers, and the highlighted nodes indicate RAPTOR\'s selections, differentiated for two distinct questions: ""What is the central theme of the story?"" and ""How did Cinderella find a happy ending?"".\n\nKey components:\n\n- Orange and purple highlighted nodes: Indicate RAPTOR\'s selections for Question 1 and Question 2.\n- Arrows: Point to DPR’s leaf nodes for each question, with orange arrows for Question 1 and purple arrows for Question 2.\n- RAPTOR\'s context is shown to often encompass the information retrieved by DPR.'), Document(metadata={'page_source': '1', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '8cb93675a3fa4506b1d10fd773541bce', '_collection_name': 'test'}, page_content=""The image illustrates the tree construction process used by RAPTOR to cluster and summarize text. Here's a breakdown of its components:\n\n1. **RAPTOR Tree (Left Panel):**\n   - **Root Layer:** Contains nodes (9 and 10).\n   - **Leaf Layer:** Contains nodes (1 to 5).\n   - Clustering of nodes forms a hierarchical structure from the leaf layer up to the root.\n\n2. **Formation of One Tree Layer (Middle Panel):**\n   - **Step 1: Clustering** - Text chunks (1 through 5) are clustered based on their embeddings.\n   - **Step 2: Summarization by LLM** - Clusters (such as those formed by nodes 2 and 3) are summarized to create another layer in the tree (nodes 6, 7, 8).\n\n3. **Contents of a Node (Right Panel):**\n   - **Index:** Identifies the node (e.g., #8).\n   - **Child Nodes:** Lists nodes contained within a cluster (e.g., 2, 3).\n   - **Text Summary:** Provides a summary of the cluster's contents.\n   - **Text Embedding:** Displays the vector representation used for clustering.""), Document(metadata={'page_source': '1', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '06f36d3a2bd04300bd63eac53f03789c', '_collection_name': 'test'}, page_content=""The image illustrates the tree construction process used by RAPTOR to cluster and summarize text. Here's a breakdown of its components:\n\n1. **RAPTOR Tree (Left Panel):**\n   - **Root Layer:** Contains nodes (9 and 10).\n   - **Leaf Layer:** Contains nodes (1 to 5).\n   - Clustering of nodes forms a hierarchical structure from the leaf layer up to the root.\n\n2. **Formation of One Tree Layer (Middle Panel):**\n   - **Step 1: Clustering** - Text chunks (1 through 5) are clustered based on their embeddings.\n   - **Step 2: Summarization by LLM** - Clusters (such as those formed by nodes 2 and 3) are summarized to create another layer in the tree (nodes 6, 7, 8).\n\n3. **Contents of a Node (Right Panel):**\n   - **Index:** Identifies the node (e.g., #8).\n   - **Child Nodes:** Lists nodes contained within a cluster (e.g., 2, 3).\n   - **Text Summary:** Provides a summary of the cluster's contents.\n   - **Text Embedding:** Displays the vector representation used for clustering.""), Document(metadata={'page_source': '18', 'doc_source': '2401.18059v1', 'start_index': 2109, '_id': '1f1240f54ebb4119bca65926342f9eb6', '_collection_name': 'test'}, page_content='2. **Question: How does Cinderella find a happy ending?**\n   - **RAPTOR**: Details the transformation magic, Cinderella impressing the Prince, her need to leave by eleven, and the Prince finding her through the glass slipper. It mentions her forgiveness.\n   - **DPR**: Focuses on the Prince searching for Cinderella, the Fairy transforming her rags back into a gown, and the creation of the glass slippers. It emphasizes the necessity to leave by eleven to maintain the magic.\nquestion at hand. Further, the information that would be retrieved by DPR is more often than not included in the context retrieved by RAPTOR, either directly as a leaf node or indirectly as part of a summary from a higher layer.'), Document(metadata={'page_source': '18', 'doc_source': '2401.18059v1', 'start_index': 2109, '_id': 'deb890527754493d868982229bfd48d4', '_collection_name': 'test'}, page_content='2. **Question: How does Cinderella find a happy ending?**\n   - **RAPTOR**: Details the transformation magic, Cinderella impressing the Prince, her need to leave by eleven, and the Prince finding her through the glass slipper. It mentions her forgiveness.\n   - **DPR**: Focuses on the Prince searching for Cinderella, the Fairy transforming her rags back into a gown, and the creation of the glass slippers. It emphasizes the necessity to leave by eleven to maintain the magic.\nquestion at hand. Further, the information that would be retrieved by DPR is more often than not included in the context retrieved by RAPTOR, either directly as a leaf node or indirectly as part of a summary from a higher layer.'), Document(metadata={'page_source': '20', 'doc_source': '2401.18059v1', 'start_index': 2127, '_id': '8054bf8576984603b192e9d2b1b93cd7', '_collection_name': 'test'}, page_content='Table 14: Performance of RAPTOR when querying different layers of the tree for Story 2. \nThe table presents data on a multi-layered structure with three rows and four columns. The columns represent ""Layers Queried / Start Layer,"" ""Layer 0 (Leaf Nodes),"" ""Layer 1,"" and ""Layer 2."" The rows are labeled as ""1 layer,"" ""2 layers,"" and ""3 layers,"" likely indicating the number of layers queried or starting layers in some hierarchy or system.'), Document(metadata={'page_source': '20', 'doc_source': '2401.18059v1', 'start_index': 2127, '_id': '2788f6b3d7fe4f9d8401b96cb8c582a6', '_collection_name': 'test'}, page_content='Table 14: Performance of RAPTOR when querying different layers of the tree for Story 2. \nThe table presents data on a multi-layered structure with three rows and four columns. The columns represent ""Layers Queried / Start Layer,"" ""Layer 0 (Leaf Nodes),"" ""Layer 1,"" and ""Layer 2."" The rows are labeled as ""1 layer,"" ""2 layers,"" and ""3 layers,"" likely indicating the number of layers queried or starting layers in some hierarchy or system.'), Document(metadata={'page_source': '3', 'doc_source': '2401.18059v1', 'start_index': 4727, '_id': 'a0b5ca33bbfb44158471dbee115ac13e', '_collection_name': 'test'}, page_content='1. Start at the root layer of the RAPTOR tree. Compute the cosine similarity between the query embedding and the embeddings of all nodes present at this initial layer. 2. Choose the top-  $\\cdot k$   nodes based on the highest cosine similarity scores, forming the set  $S_{1}$  .'), Document(metadata={'page_source': '3', 'doc_source': '2401.18059v1', 'start_index': 4727, '_id': '4d6b7f237d4f4ebcbb846de19c64ce73', '_collection_name': 'test'}, page_content='1. Start at the root layer of the RAPTOR tree. Compute the cosine similarity between the query embedding and the embeddings of all nodes present at this initial layer. 2. Choose the top-  $\\cdot k$   nodes based on the highest cosine similarity scores, forming the set  $S_{1}$  .'), Document(metadata={'page_source': '3', 'doc_source': '2401.18059v1', 'start_index': 3826, '_id': 'f246024f8f9b461e9eec4737923faeff', '_collection_name': 'test'}, page_content='Querying In this section, we elaborate on the two querying mechanisms employed by RAPTOR: tree traversal and collapsed tree. These methods offer unique ways of traversing the multi-layered RAPTOR tree to retrieve relevant information, each with its own advantages and trade-offs. We provide the pseudocode of both methods in Appendix  F . Note that we embed all nodes using SBERT. \nThe  tree traversal  method first selects the top-k most relevant root nodes based on their cosine similarity to the query embedding. The children of these selected nodes are considered at the next layer and the top  $\\cdot\\mathrm{k}$   nodes are selected from this pool again based on their cosine similarity to the query vector. This process is repeated until we reach the leaf nodes. Finally, the text from all selected nodes is concatenated to form the retrieved context. The algorithm’s steps are outlined below:'), Document(metadata={'page_source': '3', 'doc_source': '2401.18059v1', 'start_index': 3826, '_id': '97acd1da24a24ee8a786b079fa0c91ab', '_collection_name': 'test'}, page_content='Querying In this section, we elaborate on the two querying mechanisms employed by RAPTOR: tree traversal and collapsed tree. These methods offer unique ways of traversing the multi-layered RAPTOR tree to retrieve relevant information, each with its own advantages and trade-offs. We provide the pseudocode of both methods in Appendix  F . Note that we embed all nodes using SBERT. \nThe  tree traversal  method first selects the top-k most relevant root nodes based on their cosine similarity to the query embedding. The children of these selected nodes are considered at the next layer and the top  $\\cdot\\mathrm{k}$   nodes are selected from this pool again based on their cosine similarity to the query vector. This process is repeated until we reach the leaf nodes. Finally, the text from all selected nodes is concatenated to form the retrieved context. The algorithm’s steps are outlined below:'), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 2606, '_id': 'ce88e64b67e44615a0c33c7735773cd6', '_collection_name': 'test'}, page_content='- For ""1 layer"":\n  - Layer 0 (Leaf Nodes): 57.9\n  - Layer 1: 47.3\n  \n- For ""2 layers"":\n  - Layer 0 (Leaf Nodes): Not applicable or missing\n  - Layer 1: 68.4 (in bold)\nI.2 W HICH  L AYERS DO  R ETRIEVED  N ODES COME FROM  ? \nWe further conduct an ablation study across all three datasets and across three different retrievers with RAPTOR with the collapsed tree retrieval to examine the layers from which the retrieved nodes originate. We observe that between  $18.5\\%$   to  $57\\%$   of the retrieved nodes come from non-leaf nodes. As illustrated in Figure  7 , the retrieval pattern across layers reveals the importance of RAPTOR’s multi-layered tree structure. Notably, a significant percentage of the nodes retrieved by RAPTOR using the DPR retriever for the Narrative QA dataset come from the first and second layers of the tree, as opposed to the leaf nodes. This pattern is consistent across the other datasets and retrievers, albeit with varying percentages.'), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 2606, '_id': '505b31868cd44da48ebd5e4f14f67e29', '_collection_name': 'test'}, page_content='- For ""1 layer"":\n  - Layer 0 (Leaf Nodes): 57.9\n  - Layer 1: 47.3\n  \n- For ""2 layers"":\n  - Layer 0 (Leaf Nodes): Not applicable or missing\n  - Layer 1: 68.4 (in bold)\nI.2 W HICH  L AYERS DO  R ETRIEVED  N ODES COME FROM  ? \nWe further conduct an ablation study across all three datasets and across three different retrievers with RAPTOR with the collapsed tree retrieval to examine the layers from which the retrieved nodes originate. We observe that between  $18.5\\%$   to  $57\\%$   of the retrieved nodes come from non-leaf nodes. As illustrated in Figure  7 , the retrieval pattern across layers reveals the importance of RAPTOR’s multi-layered tree structure. Notably, a significant percentage of the nodes retrieved by RAPTOR using the DPR retriever for the Narrative QA dataset come from the first and second layers of the tree, as opposed to the leaf nodes. This pattern is consistent across the other datasets and retrievers, albeit with varying percentages.'), Document(metadata={'page_source': '17', 'doc_source': '2401.18059v1', 'start_index': 2959, '_id': '6455fb9a12584eb49b974ae077d8c376', '_collection_name': 'test'}, page_content='This algorithm likely performs a top-k selection of nodes at each layer of the tree according to some scoring system derived from the `dot_product` of nodes and a query, effectively filtering the tree traversed by prioritizing nodes with the highest scores.\nG Q UALITATIVE  A NALYSIS \nTo qualitatively examine RAPTOR’s retrieval process, we test it on thematic, multi-hop questions about a 1500-word version of the fairytale Cinderella. We compare the context retrieved by RAP- TOR with the context retrieved by Dense Passage Retrieval (DPR). Figure  4  in the main paper details the retrieval process within RAPTOR’s tree structure for two questions. The nodes that RAPTOR selects for each question are highlighted, while the leaf nodes that DPR selects for the same question are indicated with arrows. This comparison illustrates the advantage of RAPTOR’s tree structure. RAPTOR selects nodes from different layers depending on the level of granularity required by the')]"
10007,What are the horizontal and vertical axis of Figure 3 respectively?,"[Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 1149, '_id': 'db94385c51304560af5941dcfaa796a0', '_collection_name': 'test'}, page_content='1. The first column is labeled ""Layers Queried / Start Layer"" and lists the different scenarios of the layers queried:\n   - 1 layer\n   - 2 layers\n   - 3 layers\n\n2. The second column, ""Layer 0 (Leaf Nodes),"" provides values for each of the scenarios:\n   - For ""1 layer,"" the value is 66.6.\n   - There is no value (indicated by a dash) for ""2 layers"" and ""3 layers.""\n\n3. The third column, ""Layer 1,"" presents values as follows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - There is no value (indicated by a dash) for ""3 layers.""\n\n4. The final column, ""Layer 2,"" shows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - For ""3 layers,"" the value is 83.3, which is also bolded for emphasis.'), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 1149, '_id': '5498fd44698440e09d35d94c8a3e8ba4', '_collection_name': 'test'}, page_content='1. The first column is labeled ""Layers Queried / Start Layer"" and lists the different scenarios of the layers queried:\n   - 1 layer\n   - 2 layers\n   - 3 layers\n\n2. The second column, ""Layer 0 (Leaf Nodes),"" provides values for each of the scenarios:\n   - For ""1 layer,"" the value is 66.6.\n   - There is no value (indicated by a dash) for ""2 layers"" and ""3 layers.""\n\n3. The third column, ""Layer 1,"" presents values as follows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - There is no value (indicated by a dash) for ""3 layers.""\n\n4. The final column, ""Layer 2,"" shows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - For ""3 layers,"" the value is 83.3, which is also bolded for emphasis.'), Document(metadata={'page_source': '8', 'doc_source': '2401.18059v1', 'start_index': 2293, '_id': '33558f02c544404fa1b2042cc5f87eda', '_collection_name': 'test'}, page_content='3. **Layer 1**: This column provides the values associated with Layer 1:\n   - For 1 layer: 57.8\n   - For 2 layers: 52.6\n   - No data is provided for 3 layers scenario for Layer 1.\n\n4. **Layer 2**: This column provides the values associated with Layer 2:\n   - For 1 layer: 57.9\n   - For 2 layers: 63.15\n   - For 3 layers: 73.68 (which is in bold, possibly indicating a significant value or result).'), Document(metadata={'page_source': '8', 'doc_source': '2401.18059v1', 'start_index': 2293, '_id': 'c6a0f7b779d547f29c2d272ece0c56b4', '_collection_name': 'test'}, page_content='3. **Layer 1**: This column provides the values associated with Layer 1:\n   - For 1 layer: 57.8\n   - For 2 layers: 52.6\n   - No data is provided for 3 layers scenario for Layer 1.\n\n4. **Layer 2**: This column provides the values associated with Layer 2:\n   - For 1 layer: 57.9\n   - For 2 layers: 63.15\n   - For 3 layers: 73.68 (which is in bold, possibly indicating a significant value or result).'), Document(metadata={'page_source': '14', 'doc_source': '2401.18059v1', 'start_index': 2556, '_id': '3c95baaaa06a45ba88f24e140e699e62', '_collection_name': 'test'}, page_content='1. **QASPER** (left plot):\n   - The plot uses red color to represent data.\n   - The horizontal axis shows document length in tokens, ranging from 0 to 30,000.\n   - The vertical axis shows the total number of tokens for both the prompt and completion, ranging from 0 to 40,000.\n   - The graph illustrates a linear increase in token cost as document length increases.\n\n2. **Narrative QA** (middle plot):\n   - The plot uses blue color to represent data.\n   - The horizontal axis shows document length, ranging from 0 to 400,000 tokens.\n   - The vertical axis shows the total number of tokens for both the prompt and completion, ranging from 0 to 600,000.\n   - The graph depicts a linear relationship, with token cost steadily increasing with document length.'), Document(metadata={'page_source': '14', 'doc_source': '2401.18059v1', 'start_index': 2556, '_id': 'f36aaef6f76e43739f04032a7e1a8369', '_collection_name': 'test'}, page_content='1. **QASPER** (left plot):\n   - The plot uses red color to represent data.\n   - The horizontal axis shows document length in tokens, ranging from 0 to 30,000.\n   - The vertical axis shows the total number of tokens for both the prompt and completion, ranging from 0 to 40,000.\n   - The graph illustrates a linear increase in token cost as document length increases.\n\n2. **Narrative QA** (middle plot):\n   - The plot uses blue color to represent data.\n   - The horizontal axis shows document length, ranging from 0 to 400,000 tokens.\n   - The vertical axis shows the total number of tokens for both the prompt and completion, ranging from 0 to 600,000.\n   - The graph depicts a linear relationship, with token cost steadily increasing with document length.'), Document(metadata={'page_source': '20', 'doc_source': '2401.18059v1', 'start_index': 2566, '_id': '48b3a5ac41be4bc0bc0bd4d0f427f4b2', '_collection_name': 'test'}, page_content='- For ""1 layer,"" the values are 58.8 for Layer 0, 47.1 for Layer 1, and 41.1 for Layer 2.\n- For ""2 layers,"" no value is given for Layer 0, but Layer 1 has a value of 64.7, and Layer 2 is at 52.9.\n- For ""3 layers,"" no values are given for Layer 0 and Layer 1, but Layer 2 is at 47.1.\n\nThis table seems to depict some kind of measurement or metric across different layers of a structure, where Layer 0 is referred to as ""Leaf Nodes."" The values could represent percentages, scores, or any other relevant metric, but the table does not provide further context on what these numbers specifically represent.'), Document(metadata={'page_source': '20', 'doc_source': '2401.18059v1', 'start_index': 2566, '_id': '1567924ffdb647df8cae4e0f2d0a7448', '_collection_name': 'test'}, page_content='- For ""1 layer,"" the values are 58.8 for Layer 0, 47.1 for Layer 1, and 41.1 for Layer 2.\n- For ""2 layers,"" no value is given for Layer 0, but Layer 1 has a value of 64.7, and Layer 2 is at 52.9.\n- For ""3 layers,"" no values are given for Layer 0 and Layer 1, but Layer 2 is at 47.1.\n\nThis table seems to depict some kind of measurement or metric across different layers of a structure, where Layer 0 is referred to as ""Leaf Nodes."" The values could represent percentages, scores, or any other relevant metric, but the table does not provide further context on what these numbers specifically represent.'), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 924, '_id': 'eb8ad7f701204de4acfb94f66928663d', '_collection_name': 'test'}, page_content=""The table presents data relating to three different queries or layers with corresponding values across three different columns labeled as Layer 0 (Leaf Nodes), Layer 1, and Layer 2. Here's a breakdown of the table contents:""), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 924, '_id': 'e3f6a37c00f84138a18b530acf4c2c2f', '_collection_name': 'test'}, page_content=""The table presents data relating to three different queries or layers with corresponding values across three different columns labeled as Layer 0 (Leaf Nodes), Layer 1, and Layer 2. Here's a breakdown of the table contents:""), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 1716, '_id': 'cca65c38455747648116cca01f030d32', '_collection_name': 'test'}, page_content='4. The final column, ""Layer 2,"" shows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - For ""3 layers,"" the value is 83.3, which is also bolded for emphasis.\n\nThe table seems to depict performance metrics or results specific to each layer query scenario across the three layers mentioned.\nThe table shows data related to layers queried or start layers across two main columns: ""Layer 0 (Leaf Nodes)"" and ""Layer 1"". It contains two rows that describe:\n\n1. For ""1 layer"" queried or started:\n   - The value in ""Layer 0 (Leaf Nodes)"" is 94.7.\n   - The value in ""Layer 1"" is 84.2.\n\n2. For ""2 layers"" queried or started:\n   - There is no value for ""Layer 0 (Leaf Nodes)"" (indicated by \'-\').\n   - The value in ""Layer 1"" is 89.4.\nThe table shows data for different layers queried or start layers, with respective values for ""Layer 0 (Leaf Nodes)"" and ""Layer 1"":'), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 1716, '_id': 'cf2e8e2fc4254465be0fc9bdbf6157f0', '_collection_name': 'test'}, page_content='4. The final column, ""Layer 2,"" shows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - For ""3 layers,"" the value is 83.3, which is also bolded for emphasis.\n\nThe table seems to depict performance metrics or results specific to each layer query scenario across the three layers mentioned.\nThe table shows data related to layers queried or start layers across two main columns: ""Layer 0 (Leaf Nodes)"" and ""Layer 1"". It contains two rows that describe:\n\n1. For ""1 layer"" queried or started:\n   - The value in ""Layer 0 (Leaf Nodes)"" is 94.7.\n   - The value in ""Layer 1"" is 84.2.\n\n2. For ""2 layers"" queried or started:\n   - There is no value for ""Layer 0 (Leaf Nodes)"" (indicated by \'-\').\n   - The value in ""Layer 1"" is 89.4.\nThe table shows data for different layers queried or start layers, with respective values for ""Layer 0 (Leaf Nodes)"" and ""Layer 1"":'), Document(metadata={'page_source': '16', 'doc_source': '2401.18059v1', 'start_index': 1901, '_id': '5f94b11d14264cdeb3a90977269b7e96', '_collection_name': 'test'}, page_content='5. **Avg. Compression Ratio (%)**: Represents the average compression ratio in percentage for each dataset. The ratios are:\n   - All Datasets: 28%\n   - QuALITY: 28%\n   - NarrativeQA: 27%\n   - QASPER: 35%\nD S UM MARI Z ATION  P ROMPT \nTable  11  shows the prompt used for sum mari z ation. \nThe table has two columns: ""Role"" and ""Content."" It has two rows:'), Document(metadata={'page_source': '16', 'doc_source': '2401.18059v1', 'start_index': 1901, '_id': '1df0180ef9af4da3a93c0ddb93679e01', '_collection_name': 'test'}, page_content='5. **Avg. Compression Ratio (%)**: Represents the average compression ratio in percentage for each dataset. The ratios are:\n   - All Datasets: 28%\n   - QuALITY: 28%\n   - NarrativeQA: 27%\n   - QASPER: 35%\nD S UM MARI Z ATION  P ROMPT \nTable  11  shows the prompt used for sum mari z ation. \nThe table has two columns: ""Role"" and ""Content."" It has two rows:'), Document(metadata={'page_source': '16', 'doc_source': '2401.18059v1', 'start_index': 1042, '_id': '141522788ab343069bde8771e33522cc', '_collection_name': 'test'}, page_content='1. **Dataset**: Lists the datasets analyzed, including ""All Datasets,"" ""QuALITY,"" ""NarrativeQA,"" and ""QASPER.""\n\n2. **Avg. Summary Length (tokens)**: Displays the average summary length in terms of tokens for each dataset. The average summary lengths are:\n   - All Datasets: 131 tokens\n   - QuALITY: 124.4 tokens\n   - NarrativeQA: 129.7 tokens\n   - QASPER: 145.9 tokens\n\n3. **Avg. Child Node Text Length (tokens)**: Shows the average text length in tokens for child nodes in each dataset. The average child node text lengths are:\n   - All Datasets: 85.6 tokens\n   - QuALITY: 87.9 tokens\n   - NarrativeQA: 85.5 tokens\n   - QASPER: 86.2 tokens\n\n4. **Avg. # of Child Nodes Per Parent**: Indicates the average number of child nodes per parent node in each dataset. The averages are:\n   - All Datasets: 6.7\n   - QuALITY: 5.7\n   - NarrativeQA: 6.8\n   - QASPER: 5.7')]"
10008,"Based on ""PSEUDOCODE FOR RETRIEVAL METHODS"", which algorithm has more number of lines? (Give the algorithm name)","[Document(metadata={'page_source': '18', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '5773eb452b0843af8d5bfc7e17b5aed1', '_collection_name': 'test'}, page_content='The table contains pseudocode for an algorithm labeled as ""Algorithm 2: Collapsed Tree Algorithm"". The function `COLLAPSEDTREE` is designed to process a data structure, presumably a tree, based on several parameters: `tree`, `query`, `k`, and `max_tokens`. Here\'s a breakdown of the algorithm:\n\n1. **Flatten the Tree**: The tree is first flattened into a one-dimensional structure using a function `flatten(tree)`.\n\n2. **Calculate Dot Product**: For each node in the flattened tree, a dot product between the `query` and the `node` is computed and stored in a list `top_nodes` along with the node itself.\n\n3. **Sorting**: The `top_nodes` list is sorted, although it is not specified what it is sorted by.\n\n4. **Initializing Result and Token Count**: The algorithm initializes an empty list `result` and sets `total_tokens` to zero.'), Document(metadata={'page_source': '18', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '2df365cb9c89435685baa42b8f90838b', '_collection_name': 'test'}, page_content='The table contains pseudocode for an algorithm labeled as ""Algorithm 2: Collapsed Tree Algorithm"". The function `COLLAPSEDTREE` is designed to process a data structure, presumably a tree, based on several parameters: `tree`, `query`, `k`, and `max_tokens`. Here\'s a breakdown of the algorithm:\n\n1. **Flatten the Tree**: The tree is first flattened into a one-dimensional structure using a function `flatten(tree)`.\n\n2. **Calculate Dot Product**: For each node in the flattened tree, a dot product between the `query` and the `node` is computed and stored in a list `top_nodes` along with the node itself.\n\n3. **Sorting**: The `top_nodes` list is sorted, although it is not specified what it is sorted by.\n\n4. **Initializing Result and Token Count**: The algorithm initializes an empty list `result` and sets `total_tokens` to zero.'), Document(metadata={'page_source': '15', 'doc_source': '2401.18059v1', 'start_index': 2203, '_id': 'f67b9a6e62e64d1bb556e06dd7e26b1a', '_collection_name': 'test'}, page_content='B.2 R ESULTS  & D ISCUSSION \nThe results of the ablation study are presented in table  9 . The results from this ablation study clearly indicate an improvement in accuracy when employing RAPTOR’s clustering mechanism over the recency-based tree approach. This finding substantiates our hypothesis that the clustering strategy in RAPTOR is more effective in capturing homogeneous content for sum mari z ation, thereby enhancing the overall retrieval performance.'), Document(metadata={'page_source': '15', 'doc_source': '2401.18059v1', 'start_index': 2203, '_id': '751183f846544e5e853d7be1ecbb3ae9', '_collection_name': 'test'}, page_content='B.2 R ESULTS  & D ISCUSSION \nThe results of the ablation study are presented in table  9 . The results from this ablation study clearly indicate an improvement in accuracy when employing RAPTOR’s clustering mechanism over the recency-based tree approach. This finding substantiates our hypothesis that the clustering strategy in RAPTOR is more effective in capturing homogeneous content for sum mari z ation, thereby enhancing the overall retrieval performance.'), Document(metadata={'page_source': '17', 'doc_source': '2401.18059v1', 'start_index': 2959, '_id': '6455fb9a12584eb49b974ae077d8c376', '_collection_name': 'test'}, page_content='This algorithm likely performs a top-k selection of nodes at each layer of the tree according to some scoring system derived from the `dot_product` of nodes and a query, effectively filtering the tree traversed by prioritizing nodes with the highest scores.\nG Q UALITATIVE  A NALYSIS \nTo qualitatively examine RAPTOR’s retrieval process, we test it on thematic, multi-hop questions about a 1500-word version of the fairytale Cinderella. We compare the context retrieved by RAP- TOR with the context retrieved by Dense Passage Retrieval (DPR). Figure  4  in the main paper details the retrieval process within RAPTOR’s tree structure for two questions. The nodes that RAPTOR selects for each question are highlighted, while the leaf nodes that DPR selects for the same question are indicated with arrows. This comparison illustrates the advantage of RAPTOR’s tree structure. RAPTOR selects nodes from different layers depending on the level of granularity required by the'), Document(metadata={'page_source': '17', 'doc_source': '2401.18059v1', 'start_index': 2959, '_id': 'c927121369f24122970224b9ff662d77', '_collection_name': 'test'}, page_content='This algorithm likely performs a top-k selection of nodes at each layer of the tree according to some scoring system derived from the `dot_product` of nodes and a query, effectively filtering the tree traversed by prioritizing nodes with the highest scores.\nG Q UALITATIVE  A NALYSIS \nTo qualitatively examine RAPTOR’s retrieval process, we test it on thematic, multi-hop questions about a 1500-word version of the fairytale Cinderella. We compare the context retrieved by RAP- TOR with the context retrieved by Dense Passage Retrieval (DPR). Figure  4  in the main paper details the retrieval process within RAPTOR’s tree structure for two questions. The nodes that RAPTOR selects for each question are highlighted, while the leaf nodes that DPR selects for the same question are indicated with arrows. This comparison illustrates the advantage of RAPTOR’s tree structure. RAPTOR selects nodes from different layers depending on the level of granularity required by the'), Document(metadata={'page_source': '8', 'doc_source': '2401.18059v1', 'start_index': 784, '_id': '9bd51463aaa449cd84b0b9dfe11cbf21', '_collection_name': 'test'}, page_content='4. **Retriever + Reader (Izacard & Grave, 2022)**\n   - ROUGE-L: 32.0\n   - BLEU-1: 35.3\n   - BLEU-4: 7.5\n   - METEOR: 11.1\n\n5. **RAPTOR + UnifiedQA**\n   - ROUGE-L: 30.8\n   - BLEU-1: 23.5\n   - BLEU-4: 6.4\n   - METEOR: 19.1\n\nThe best scores for each metric are bolded in the table.\nThe table presents the accuracy of different models on two datasets: the ""Test Set"" and the ""Hard Subset"". The models compared in the table are:\n\n1. Longformer-base (Beltagy et al., 2020)\n   - Test Set Accuracy: 39.5\n   - Hard Subset Accuracy: 35.3\n\n2. DPR and DeBERTaV3-large (Pang et al., 2022)\n   - Test Set Accuracy: 55.4\n   - Hard Subset Accuracy: 46.1\n\n3. CoLISA (DeBERTaV3-large) (Dong et al., 2023a)\n   - Test Set Accuracy: 62.3\n   - Hard Subset Accuracy: 54.7\n\n4. RAPTOR + GPT-4\n   - Test Set Accuracy: 82.6\n   - Hard Subset Accuracy: 76.2'), Document(metadata={'page_source': '8', 'doc_source': '2401.18059v1', 'start_index': 784, '_id': 'e65578c01ccf45c09d0eadb26691ae44', '_collection_name': 'test'}, page_content='4. **Retriever + Reader (Izacard & Grave, 2022)**\n   - ROUGE-L: 32.0\n   - BLEU-1: 35.3\n   - BLEU-4: 7.5\n   - METEOR: 11.1\n\n5. **RAPTOR + UnifiedQA**\n   - ROUGE-L: 30.8\n   - BLEU-1: 23.5\n   - BLEU-4: 6.4\n   - METEOR: 19.1\n\nThe best scores for each metric are bolded in the table.\nThe table presents the accuracy of different models on two datasets: the ""Test Set"" and the ""Hard Subset"". The models compared in the table are:\n\n1. Longformer-base (Beltagy et al., 2020)\n   - Test Set Accuracy: 39.5\n   - Hard Subset Accuracy: 35.3\n\n2. DPR and DeBERTaV3-large (Pang et al., 2022)\n   - Test Set Accuracy: 55.4\n   - Hard Subset Accuracy: 46.1\n\n3. CoLISA (DeBERTaV3-large) (Dong et al., 2023a)\n   - Test Set Accuracy: 62.3\n   - Hard Subset Accuracy: 54.7\n\n4. RAPTOR + GPT-4\n   - Test Set Accuracy: 82.6\n   - Hard Subset Accuracy: 76.2'), Document(metadata={'page_source': '4', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '3d526011c1374e99ad048ee3c9961fa6', '_collection_name': 'test'}, page_content='The image illustrates two mechanisms for retrieving information using tree structures: Tree Traversal Retrieval and Collapsed Tree Retrieval.\n\n1. **Tree Traversal Retrieval (A)**:\n   - Starts with a query that is encoded into a vector.\n   - The tree structure is traversed starting from the root.\n   - At each level, it retrieves the top nodes based on cosine similarity to the query vector.\n   - Retrieves context from selected nodes which is then combined with the query to generate an answer using a language model (LLM).\n\n2. **Collapsed Tree Retrieval (B)**:\n   - Also starts with a query that is encoded.\n   - The tree is collapsed into a single layer.\n   - Nodes are retrieved based on cosine similarity until reaching a threshold number of tokens.\n   - The retrieved context is combined with the query to produce an answer using an LLM.'), Document(metadata={'page_source': '4', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '58b166c194b7456f93d6ff256ce170fc', '_collection_name': 'test'}, page_content='The image illustrates two mechanisms for retrieving information using tree structures: Tree Traversal Retrieval and Collapsed Tree Retrieval.\n\n1. **Tree Traversal Retrieval (A)**:\n   - Starts with a query that is encoded into a vector.\n   - The tree structure is traversed starting from the root.\n   - At each level, it retrieves the top nodes based on cosine similarity to the query vector.\n   - Retrieves context from selected nodes which is then combined with the query to generate an answer using a language model (LLM).\n\n2. **Collapsed Tree Retrieval (B)**:\n   - Also starts with a query that is encoded.\n   - The tree is collapsed into a single layer.\n   - Nodes are retrieved based on cosine similarity until reaching a threshold number of tokens.\n   - The retrieved context is combined with the query to produce an answer using an LLM.'), Document(metadata={'page_source': '7', 'doc_source': '2401.18059v1', 'start_index': 3511, '_id': '5332ab1e2b1e4df7aa03feb38b9ad099', '_collection_name': 'test'}, page_content='The table appears to be comparing the performance of these models based on the F-1 Match metric.\na new state-of-the-art METEOR score. When compared to the recursively summarizing model by Wu et al.  ( 2021 ), which also employs UnifiedQA, RAPTOR outperforms it on all metrics. While Wu et al.  ( 2021 ) rely solely on the summary in the top root node of the tree structure, RAPTOR benefits from its intermediate layers and clustering approaches, which allows it to capture a range of information, from general themes to specific details, contributing to its overall strong performance. \n4.1 C ON TRI BUT ION OF THE TREE STRUCTURE \nWe examine the contribution of each layer of nodes to RAPTOR’s retrieval capabilities. We hy- pothesized that upper nodes play a crucial role in handling thematic or multi-hop queries requiring a broader understanding of the text.'), Document(metadata={'page_source': '7', 'doc_source': '2401.18059v1', 'start_index': 3511, '_id': '6ddda168169f411e86e96727a05aa5be', '_collection_name': 'test'}, page_content='The table appears to be comparing the performance of these models based on the F-1 Match metric.\na new state-of-the-art METEOR score. When compared to the recursively summarizing model by Wu et al.  ( 2021 ), which also employs UnifiedQA, RAPTOR outperforms it on all metrics. While Wu et al.  ( 2021 ) rely solely on the summary in the top root node of the tree structure, RAPTOR benefits from its intermediate layers and clustering approaches, which allows it to capture a range of information, from general themes to specific details, contributing to its overall strong performance. \n4.1 C ON TRI BUT ION OF THE TREE STRUCTURE \nWe examine the contribution of each layer of nodes to RAPTOR’s retrieval capabilities. We hy- pothesized that upper nodes play a crucial role in handling thematic or multi-hop queries requiring a broader understanding of the text.'), Document(metadata={'page_source': '6', 'doc_source': '2401.18059v1', 'start_index': 893, '_id': '510e5c6db21f42858a5af52b18552e2c', '_collection_name': 'test'}, page_content=""This diagram visualizes the differing layers of retrieval and how RAPTOR's higher-layer summaries sometimes include DPR's specific retrievals directly or through summarization.\nour results demonstrate that RAPTOR, when combined with any retriever, consistently outperforms the respective retriever across all datasets.   2""), Document(metadata={'page_source': '6', 'doc_source': '2401.18059v1', 'start_index': 893, '_id': '8bd4dbfdd566480193e02a020d1f5cf9', '_collection_name': 'test'}, page_content=""This diagram visualizes the differing layers of retrieval and how RAPTOR's higher-layer summaries sometimes include DPR's specific retrievals directly or through summarization.\nour results demonstrate that RAPTOR, when combined with any retriever, consistently outperforms the respective retriever across all datasets.   2""), Document(metadata={'page_source': '18', 'doc_source': '2401.18059v1', 'start_index': 706, '_id': 'a9a3cb9961b9467fb7827135941cdb9b', '_collection_name': 'test'}, page_content=""4. **Initializing Result and Token Count**: The algorithm initializes an empty list `result` and sets `total_tokens` to zero.\n\n5. **Iterating Over Top Nodes**: It then iterates over each node in the sorted `top_nodes` list and checks if adding the current node's token size to `total_tokens` would exceed `max_tokens`.\n\n   - If not, the node is appended to the `result` list.\n   - The `total_tokens` is updated by adding the current node's token size.\n\n6. **Return Result**: The function finally returns the `result` list, which presumably contains a subset of nodes from the original tree that maximize some criteria while keeping within a token limit.\n\nThe algorithm essentially collapses a tree into a prioritized list of nodes based on their relevance to a query, constrained by a token limit.\nThe table compares RAPTOR and DPR responses to two questions about the story of Cinderella:"")]"
10009,"In Figure 1's demonstration, what are the color of the nodes that appear in more than one clusters?","[Document(metadata={'page_source': '1', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '8cb93675a3fa4506b1d10fd773541bce', '_collection_name': 'test'}, page_content=""The image illustrates the tree construction process used by RAPTOR to cluster and summarize text. Here's a breakdown of its components:\n\n1. **RAPTOR Tree (Left Panel):**\n   - **Root Layer:** Contains nodes (9 and 10).\n   - **Leaf Layer:** Contains nodes (1 to 5).\n   - Clustering of nodes forms a hierarchical structure from the leaf layer up to the root.\n\n2. **Formation of One Tree Layer (Middle Panel):**\n   - **Step 1: Clustering** - Text chunks (1 through 5) are clustered based on their embeddings.\n   - **Step 2: Summarization by LLM** - Clusters (such as those formed by nodes 2 and 3) are summarized to create another layer in the tree (nodes 6, 7, 8).\n\n3. **Contents of a Node (Right Panel):**\n   - **Index:** Identifies the node (e.g., #8).\n   - **Child Nodes:** Lists nodes contained within a cluster (e.g., 2, 3).\n   - **Text Summary:** Provides a summary of the cluster's contents.\n   - **Text Embedding:** Displays the vector representation used for clustering.""), Document(metadata={'page_source': '1', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '06f36d3a2bd04300bd63eac53f03789c', '_collection_name': 'test'}, page_content=""The image illustrates the tree construction process used by RAPTOR to cluster and summarize text. Here's a breakdown of its components:\n\n1. **RAPTOR Tree (Left Panel):**\n   - **Root Layer:** Contains nodes (9 and 10).\n   - **Leaf Layer:** Contains nodes (1 to 5).\n   - Clustering of nodes forms a hierarchical structure from the leaf layer up to the root.\n\n2. **Formation of One Tree Layer (Middle Panel):**\n   - **Step 1: Clustering** - Text chunks (1 through 5) are clustered based on their embeddings.\n   - **Step 2: Summarization by LLM** - Clusters (such as those formed by nodes 2 and 3) are summarized to create another layer in the tree (nodes 6, 7, 8).\n\n3. **Contents of a Node (Right Panel):**\n   - **Index:** Identifies the node (e.g., #8).\n   - **Child Nodes:** Lists nodes contained within a cluster (e.g., 2, 3).\n   - **Text Summary:** Provides a summary of the cluster's contents.\n   - **Text Embedding:** Displays the vector representation used for clustering.""), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 924, '_id': 'e3f6a37c00f84138a18b530acf4c2c2f', '_collection_name': 'test'}, page_content=""The table presents data relating to three different queries or layers with corresponding values across three different columns labeled as Layer 0 (Leaf Nodes), Layer 1, and Layer 2. Here's a breakdown of the table contents:""), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 924, '_id': 'eb8ad7f701204de4acfb94f66928663d', '_collection_name': 'test'}, page_content=""The table presents data relating to three different queries or layers with corresponding values across three different columns labeled as Layer 0 (Leaf Nodes), Layer 1, and Layer 2. Here's a breakdown of the table contents:""), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 1149, '_id': 'db94385c51304560af5941dcfaa796a0', '_collection_name': 'test'}, page_content='1. The first column is labeled ""Layers Queried / Start Layer"" and lists the different scenarios of the layers queried:\n   - 1 layer\n   - 2 layers\n   - 3 layers\n\n2. The second column, ""Layer 0 (Leaf Nodes),"" provides values for each of the scenarios:\n   - For ""1 layer,"" the value is 66.6.\n   - There is no value (indicated by a dash) for ""2 layers"" and ""3 layers.""\n\n3. The third column, ""Layer 1,"" presents values as follows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - There is no value (indicated by a dash) for ""3 layers.""\n\n4. The final column, ""Layer 2,"" shows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - For ""3 layers,"" the value is 83.3, which is also bolded for emphasis.'), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 1149, '_id': '5498fd44698440e09d35d94c8a3e8ba4', '_collection_name': 'test'}, page_content='1. The first column is labeled ""Layers Queried / Start Layer"" and lists the different scenarios of the layers queried:\n   - 1 layer\n   - 2 layers\n   - 3 layers\n\n2. The second column, ""Layer 0 (Leaf Nodes),"" provides values for each of the scenarios:\n   - For ""1 layer,"" the value is 66.6.\n   - There is no value (indicated by a dash) for ""2 layers"" and ""3 layers.""\n\n3. The third column, ""Layer 1,"" presents values as follows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - There is no value (indicated by a dash) for ""3 layers.""\n\n4. The final column, ""Layer 2,"" shows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - For ""3 layers,"" the value is 83.3, which is also bolded for emphasis.'), Document(metadata={'page_source': '16', 'doc_source': '2401.18059v1', 'start_index': 1042, '_id': 'a42a05f84a354fb8ab5a5c412e2df948', '_collection_name': 'test'}, page_content='1. **Dataset**: Lists the datasets analyzed, including ""All Datasets,"" ""QuALITY,"" ""NarrativeQA,"" and ""QASPER.""\n\n2. **Avg. Summary Length (tokens)**: Displays the average summary length in terms of tokens for each dataset. The average summary lengths are:\n   - All Datasets: 131 tokens\n   - QuALITY: 124.4 tokens\n   - NarrativeQA: 129.7 tokens\n   - QASPER: 145.9 tokens\n\n3. **Avg. Child Node Text Length (tokens)**: Shows the average text length in tokens for child nodes in each dataset. The average child node text lengths are:\n   - All Datasets: 85.6 tokens\n   - QuALITY: 87.9 tokens\n   - NarrativeQA: 85.5 tokens\n   - QASPER: 86.2 tokens\n\n4. **Avg. # of Child Nodes Per Parent**: Indicates the average number of child nodes per parent node in each dataset. The averages are:\n   - All Datasets: 6.7\n   - QuALITY: 5.7\n   - NarrativeQA: 6.8\n   - QASPER: 5.7'), Document(metadata={'page_source': '16', 'doc_source': '2401.18059v1', 'start_index': 1042, '_id': '141522788ab343069bde8771e33522cc', '_collection_name': 'test'}, page_content='1. **Dataset**: Lists the datasets analyzed, including ""All Datasets,"" ""QuALITY,"" ""NarrativeQA,"" and ""QASPER.""\n\n2. **Avg. Summary Length (tokens)**: Displays the average summary length in terms of tokens for each dataset. The average summary lengths are:\n   - All Datasets: 131 tokens\n   - QuALITY: 124.4 tokens\n   - NarrativeQA: 129.7 tokens\n   - QASPER: 145.9 tokens\n\n3. **Avg. Child Node Text Length (tokens)**: Shows the average text length in tokens for child nodes in each dataset. The average child node text lengths are:\n   - All Datasets: 85.6 tokens\n   - QuALITY: 87.9 tokens\n   - NarrativeQA: 85.5 tokens\n   - QASPER: 86.2 tokens\n\n4. **Avg. # of Child Nodes Per Parent**: Indicates the average number of child nodes per parent node in each dataset. The averages are:\n   - All Datasets: 6.7\n   - QuALITY: 5.7\n   - NarrativeQA: 6.8\n   - QASPER: 5.7'), Document(metadata={'page_source': '20', 'doc_source': '2401.18059v1', 'start_index': 2566, '_id': '1567924ffdb647df8cae4e0f2d0a7448', '_collection_name': 'test'}, page_content='- For ""1 layer,"" the values are 58.8 for Layer 0, 47.1 for Layer 1, and 41.1 for Layer 2.\n- For ""2 layers,"" no value is given for Layer 0, but Layer 1 has a value of 64.7, and Layer 2 is at 52.9.\n- For ""3 layers,"" no values are given for Layer 0 and Layer 1, but Layer 2 is at 47.1.\n\nThis table seems to depict some kind of measurement or metric across different layers of a structure, where Layer 0 is referred to as ""Leaf Nodes."" The values could represent percentages, scores, or any other relevant metric, but the table does not provide further context on what these numbers specifically represent.'), Document(metadata={'page_source': '20', 'doc_source': '2401.18059v1', 'start_index': 2566, '_id': '48b3a5ac41be4bc0bc0bd4d0f427f4b2', '_collection_name': 'test'}, page_content='- For ""1 layer,"" the values are 58.8 for Layer 0, 47.1 for Layer 1, and 41.1 for Layer 2.\n- For ""2 layers,"" no value is given for Layer 0, but Layer 1 has a value of 64.7, and Layer 2 is at 52.9.\n- For ""3 layers,"" no values are given for Layer 0 and Layer 1, but Layer 2 is at 47.1.\n\nThis table seems to depict some kind of measurement or metric across different layers of a structure, where Layer 0 is referred to as ""Leaf Nodes."" The values could represent percentages, scores, or any other relevant metric, but the table does not provide further context on what these numbers specifically represent.'), Document(metadata={'page_source': '3', 'doc_source': '2401.18059v1', 'start_index': 2837, '_id': '03bee50981904c2f8cdb848e8520ef00', '_collection_name': 'test'}, page_content='Model-Based Sum mari z ation After clustering the nodes using Gaussian Mixture Models, the nodes in each cluster are sent to a language model for sum mari z ation. This step allows the model to transform large chunks of text into concise, coherent summaries of the selected nodes. For our experiments, we use    $\\mathtt{g p t\\!-\\!3.5\\!-\\!t u r b o}$   to generate the summaries. The sum mari z ation step con- denses the potentially large volume of retrieved information into a manageable size. We provide statistics on the compression due to the sum mari z ation in Appendix  C  and the prompt used for sum mari z ation in Appendix  D . \nWhile the sum mari z ation model generally produces reliable summaries, a focused annotation study revealed that about   $4\\%$   of the summaries contained minor hallucinations. These did not propagate to parent nodes and had no discernible impact on question-answering tasks. For an in-depth analysis of hallucinations, refer to the appendix  E .'), Document(metadata={'page_source': '3', 'doc_source': '2401.18059v1', 'start_index': 2837, '_id': '6102f63fcbe04f91b7633ef8897e3b13', '_collection_name': 'test'}, page_content='Model-Based Sum mari z ation After clustering the nodes using Gaussian Mixture Models, the nodes in each cluster are sent to a language model for sum mari z ation. This step allows the model to transform large chunks of text into concise, coherent summaries of the selected nodes. For our experiments, we use    $\\mathtt{g p t\\!-\\!3.5\\!-\\!t u r b o}$   to generate the summaries. The sum mari z ation step con- denses the potentially large volume of retrieved information into a manageable size. We provide statistics on the compression due to the sum mari z ation in Appendix  C  and the prompt used for sum mari z ation in Appendix  D . \nWhile the sum mari z ation model generally produces reliable summaries, a focused annotation study revealed that about   $4\\%$   of the summaries contained minor hallucinations. These did not propagate to parent nodes and had no discernible impact on question-answering tasks. For an in-depth analysis of hallucinations, refer to the appendix  E .'), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 1716, '_id': 'cf2e8e2fc4254465be0fc9bdbf6157f0', '_collection_name': 'test'}, page_content='4. The final column, ""Layer 2,"" shows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - For ""3 layers,"" the value is 83.3, which is also bolded for emphasis.\n\nThe table seems to depict performance metrics or results specific to each layer query scenario across the three layers mentioned.\nThe table shows data related to layers queried or start layers across two main columns: ""Layer 0 (Leaf Nodes)"" and ""Layer 1"". It contains two rows that describe:\n\n1. For ""1 layer"" queried or started:\n   - The value in ""Layer 0 (Leaf Nodes)"" is 94.7.\n   - The value in ""Layer 1"" is 84.2.\n\n2. For ""2 layers"" queried or started:\n   - There is no value for ""Layer 0 (Leaf Nodes)"" (indicated by \'-\').\n   - The value in ""Layer 1"" is 89.4.\nThe table shows data for different layers queried or start layers, with respective values for ""Layer 0 (Leaf Nodes)"" and ""Layer 1"":'), Document(metadata={'page_source': '21', 'doc_source': '2401.18059v1', 'start_index': 1716, '_id': 'cca65c38455747648116cca01f030d32', '_collection_name': 'test'}, page_content='4. The final column, ""Layer 2,"" shows:\n   - For ""1 layer,"" the value is 61.1.\n   - For ""2 layers,"" the value is 66.6.\n   - For ""3 layers,"" the value is 83.3, which is also bolded for emphasis.\n\nThe table seems to depict performance metrics or results specific to each layer query scenario across the three layers mentioned.\nThe table shows data related to layers queried or start layers across two main columns: ""Layer 0 (Leaf Nodes)"" and ""Layer 1"". It contains two rows that describe:\n\n1. For ""1 layer"" queried or started:\n   - The value in ""Layer 0 (Leaf Nodes)"" is 94.7.\n   - The value in ""Layer 1"" is 84.2.\n\n2. For ""2 layers"" queried or started:\n   - There is no value for ""Layer 0 (Leaf Nodes)"" (indicated by \'-\').\n   - The value in ""Layer 1"" is 89.4.\nThe table shows data for different layers queried or start layers, with respective values for ""Layer 0 (Leaf Nodes)"" and ""Layer 1"":'), Document(metadata={'page_source': '17', 'doc_source': '2401.18059v1', 'start_index': 2482, '_id': '3ce13893c184400ea79009e4ac41b290', '_collection_name': 'test'}, page_content='5. **Select Top-k Nodes**:\n   - `S_layer` is assigned the top-k nodes from `sorted(top_k)[:k].nodes`, where `top_k` is sorted based on scores.\n\n6. **Update Current Layer**:\n   - `S_current` is updated to be the current set of selected top-k nodes, `S_layer`.\n\n7. **Return Statement**:\n   - The function returns the union of sets from all the layers traversed (`S_0 ∪ S_1 ∪ S_2 ... ∪ S_k`).\n\n8. **End of Function**: The function is concluded with an `end function` statement.')]"
10010,"What model is the clustering algorithm of this paper based on, and what presents a challenge to it?","[Document(metadata={'page_source': '3', 'doc_source': '2401.18059v1', 'start_index': 1463, '_id': '6149362117df47479d4f3a5279987596', '_collection_name': 'test'}, page_content='Should a local cluster’s combined context ever exceed the sum mari z ation model’s token threshold, our algorithm recursively applies clustering within the cluster, ensuring that the context remains within the token threshold. \nTo determine the optimal number of clusters, we employ the Bayesian Information Criterion (BIC) for model selection. BIC not only penalizes model complexity but also rewards goodness of fit ( Schwarz ,  1978 ). The BIC for a given GMM is    $B I C=\\ln(N)k-2\\ln(\\hat{L})$  , where    $N$   is the number of text segments (or data points),    $k$   is the number of model parameters, and  $\\hat{L}$   is the maximized value of the likelihood function of the model. In the context of GMM, the number of parameters  $k$  is a function of the dimensionality of the input vectors and the number of clusters.'), Document(metadata={'page_source': '3', 'doc_source': '2401.18059v1', 'start_index': 1463, '_id': '2038410930494ad28582bde58da2a050', '_collection_name': 'test'}, page_content='Should a local cluster’s combined context ever exceed the sum mari z ation model’s token threshold, our algorithm recursively applies clustering within the cluster, ensuring that the context remains within the token threshold. \nTo determine the optimal number of clusters, we employ the Bayesian Information Criterion (BIC) for model selection. BIC not only penalizes model complexity but also rewards goodness of fit ( Schwarz ,  1978 ). The BIC for a given GMM is    $B I C=\\ln(N)k-2\\ln(\\hat{L})$  , where    $N$   is the number of text segments (or data points),    $k$   is the number of model parameters, and  $\\hat{L}$   is the maximized value of the likelihood function of the model. In the context of GMM, the number of parameters  $k$  is a function of the dimensionality of the input vectors and the number of clusters.'), Document(metadata={'page_source': '2', 'doc_source': '2401.18059v1', 'start_index': 4309, '_id': '8ccb0429f4d44047a5db6fa55f2e7096', '_collection_name': 'test'}, page_content='Our clustering algorithm is based on Gaussian Mixture Models (GMMs), an approach that offers both flexibility and a probabilistic framework. GMMs assume that data points are generated from a mixture of several Gaussian distributions.'), Document(metadata={'page_source': '2', 'doc_source': '2401.18059v1', 'start_index': 4309, '_id': '1d172b3e278c48c3b4d4add3d8d22a9a', '_collection_name': 'test'}, page_content='Our clustering algorithm is based on Gaussian Mixture Models (GMMs), an approach that offers both flexibility and a probabilistic framework. GMMs assume that data points are generated from a mixture of several Gaussian distributions.'), Document(metadata={'page_source': '3', 'doc_source': '2401.18059v1', 'start_index': 2294, '_id': '6e398bc68e324d30bbcc657e992ee7ab', '_collection_name': 'test'}, page_content='With the optimal number of clusters determined by BIC, the Expectation-Maximization algorithm is then used to estimate the GMM parameters, namely the means, co variances, and mixture weights. \nWhile the Gaussian assumption in GMMs may not perfectly align with the nature of text data, which often exhibits a sparse and skewed distribution, our empirical observations suggest that it offers an effective model for our purpose. We run an ablation comparing GMM Clustering with summarizing contiguous chunks and provide details in Appendix  B .'), Document(metadata={'page_source': '3', 'doc_source': '2401.18059v1', 'start_index': 2294, '_id': '26f0effccc0a47a5afc15119836c9bd0', '_collection_name': 'test'}, page_content='With the optimal number of clusters determined by BIC, the Expectation-Maximization algorithm is then used to estimate the GMM parameters, namely the means, co variances, and mixture weights. \nWhile the Gaussian assumption in GMMs may not perfectly align with the nature of text data, which often exhibits a sparse and skewed distribution, our empirical observations suggest that it offers an effective model for our purpose. We run an ablation comparing GMM Clustering with summarizing contiguous chunks and provide details in Appendix  B .'), Document(metadata={'page_source': '15', 'doc_source': '2401.18059v1', 'start_index': 2203, '_id': 'f67b9a6e62e64d1bb556e06dd7e26b1a', '_collection_name': 'test'}, page_content='B.2 R ESULTS  & D ISCUSSION \nThe results of the ablation study are presented in table  9 . The results from this ablation study clearly indicate an improvement in accuracy when employing RAPTOR’s clustering mechanism over the recency-based tree approach. This finding substantiates our hypothesis that the clustering strategy in RAPTOR is more effective in capturing homogeneous content for sum mari z ation, thereby enhancing the overall retrieval performance.'), Document(metadata={'page_source': '15', 'doc_source': '2401.18059v1', 'start_index': 2203, '_id': '751183f846544e5e853d7be1ecbb3ae9', '_collection_name': 'test'}, page_content='B.2 R ESULTS  & D ISCUSSION \nThe results of the ablation study are presented in table  9 . The results from this ablation study clearly indicate an improvement in accuracy when employing RAPTOR’s clustering mechanism over the recency-based tree approach. This finding substantiates our hypothesis that the clustering strategy in RAPTOR is more effective in capturing homogeneous content for sum mari z ation, thereby enhancing the overall retrieval performance.'), Document(metadata={'page_source': '1', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '06f36d3a2bd04300bd63eac53f03789c', '_collection_name': 'test'}, page_content=""The image illustrates the tree construction process used by RAPTOR to cluster and summarize text. Here's a breakdown of its components:\n\n1. **RAPTOR Tree (Left Panel):**\n   - **Root Layer:** Contains nodes (9 and 10).\n   - **Leaf Layer:** Contains nodes (1 to 5).\n   - Clustering of nodes forms a hierarchical structure from the leaf layer up to the root.\n\n2. **Formation of One Tree Layer (Middle Panel):**\n   - **Step 1: Clustering** - Text chunks (1 through 5) are clustered based on their embeddings.\n   - **Step 2: Summarization by LLM** - Clusters (such as those formed by nodes 2 and 3) are summarized to create another layer in the tree (nodes 6, 7, 8).\n\n3. **Contents of a Node (Right Panel):**\n   - **Index:** Identifies the node (e.g., #8).\n   - **Child Nodes:** Lists nodes contained within a cluster (e.g., 2, 3).\n   - **Text Summary:** Provides a summary of the cluster's contents.\n   - **Text Embedding:** Displays the vector representation used for clustering.""), Document(metadata={'page_source': '1', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '8cb93675a3fa4506b1d10fd773541bce', '_collection_name': 'test'}, page_content=""The image illustrates the tree construction process used by RAPTOR to cluster and summarize text. Here's a breakdown of its components:\n\n1. **RAPTOR Tree (Left Panel):**\n   - **Root Layer:** Contains nodes (9 and 10).\n   - **Leaf Layer:** Contains nodes (1 to 5).\n   - Clustering of nodes forms a hierarchical structure from the leaf layer up to the root.\n\n2. **Formation of One Tree Layer (Middle Panel):**\n   - **Step 1: Clustering** - Text chunks (1 through 5) are clustered based on their embeddings.\n   - **Step 2: Summarization by LLM** - Clusters (such as those formed by nodes 2 and 3) are summarized to create another layer in the tree (nodes 6, 7, 8).\n\n3. **Contents of a Node (Right Panel):**\n   - **Index:** Identifies the node (e.g., #8).\n   - **Child Nodes:** Lists nodes contained within a cluster (e.g., 2, 3).\n   - **Text Summary:** Provides a summary of the cluster's contents.\n   - **Text Embedding:** Displays the vector representation used for clustering.""), Document(metadata={'page_source': '18', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '2df365cb9c89435685baa42b8f90838b', '_collection_name': 'test'}, page_content='The table contains pseudocode for an algorithm labeled as ""Algorithm 2: Collapsed Tree Algorithm"". The function `COLLAPSEDTREE` is designed to process a data structure, presumably a tree, based on several parameters: `tree`, `query`, `k`, and `max_tokens`. Here\'s a breakdown of the algorithm:\n\n1. **Flatten the Tree**: The tree is first flattened into a one-dimensional structure using a function `flatten(tree)`.\n\n2. **Calculate Dot Product**: For each node in the flattened tree, a dot product between the `query` and the `node` is computed and stored in a list `top_nodes` along with the node itself.\n\n3. **Sorting**: The `top_nodes` list is sorted, although it is not specified what it is sorted by.\n\n4. **Initializing Result and Token Count**: The algorithm initializes an empty list `result` and sets `total_tokens` to zero.'), Document(metadata={'page_source': '18', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '5773eb452b0843af8d5bfc7e17b5aed1', '_collection_name': 'test'}, page_content='The table contains pseudocode for an algorithm labeled as ""Algorithm 2: Collapsed Tree Algorithm"". The function `COLLAPSEDTREE` is designed to process a data structure, presumably a tree, based on several parameters: `tree`, `query`, `k`, and `max_tokens`. Here\'s a breakdown of the algorithm:\n\n1. **Flatten the Tree**: The tree is first flattened into a one-dimensional structure using a function `flatten(tree)`.\n\n2. **Calculate Dot Product**: For each node in the flattened tree, a dot product between the `query` and the `node` is computed and stored in a list `top_nodes` along with the node itself.\n\n3. **Sorting**: The `top_nodes` list is sorted, although it is not specified what it is sorted by.\n\n4. **Initializing Result and Token Count**: The algorithm initializes an empty list `result` and sets `total_tokens` to zero.'), Document(metadata={'page_source': '2', 'doc_source': '2401.18059v1', 'start_index': 2727, '_id': 'db0f0328f7b04e3ab299885072ef8c60', '_collection_name': 'test'}, page_content='To group similar text chunks, we employ a clustering algorithm. Once clustered, a Language Model is used to summarize the grouped texts. These summarized texts are then re-embedded, and the cycle of embedding, clustering, and sum mari z ation continues until further clustering becomes infeasible, resulting in a structured, multi-layered tree representation of the original documents. An important aspect of RAPTOR is its computational efficiency. The system scales linearly in terms of both build time and token expenditure, making it suitable for processing large and complex corpora. For a comprehensive discussion on RAPTOR’s s cal ability, please refer to the Appendix  A .'), Document(metadata={'page_source': '2', 'doc_source': '2401.18059v1', 'start_index': 2727, '_id': 'fc2e67f74f0848ca9d213c75668ae776', '_collection_name': 'test'}, page_content='To group similar text chunks, we employ a clustering algorithm. Once clustered, a Language Model is used to summarize the grouped texts. These summarized texts are then re-embedded, and the cycle of embedding, clustering, and sum mari z ation continues until further clustering becomes infeasible, resulting in a structured, multi-layered tree representation of the original documents. An important aspect of RAPTOR is its computational efficiency. The system scales linearly in terms of both build time and token expenditure, making it suitable for processing large and complex corpora. For a comprehensive discussion on RAPTOR’s s cal ability, please refer to the Appendix  A .'), Document(metadata={'page_source': '3', 'doc_source': '2401.18059v1', 'start_index': 608, '_id': '48279b9e9dcd42a28920fca6c4716760', '_collection_name': 'test'}, page_content='The high dimensionality of vector embeddings presents a challenge for traditional GMMs, as dis- tance metrics may behave poorly when used to measure similarity in high-dimensional spaces ( Ag- garwal et al. ,  2001 ). To mitigate this, we employ Uniform Manifold Approximation and Projection (UMAP), a manifold learning technique for dimensionality reduction ( McInnes et al. ,  2018 ). The number of nearest neighbors parameter,  n neighbors , in UMAP determines the balance between the preservation of local and global structures. Our algorithm varies  n neighbors  to create a hierar- chical clustering structure: it first identifies global clusters and then performs local clustering within these global clusters. This two-step clustering process captures a broad spectrum of relationships among the text data, from broad themes to specific details.')]"
10011,Write down the pseudo code from appendix that corresponds to step 5 of the tree traversal method,"[Document(metadata={'page_source': '18', 'doc_source': '2401.18059v1', 'start_index': 706, '_id': 'a9a3cb9961b9467fb7827135941cdb9b', '_collection_name': 'test'}, page_content=""4. **Initializing Result and Token Count**: The algorithm initializes an empty list `result` and sets `total_tokens` to zero.\n\n5. **Iterating Over Top Nodes**: It then iterates over each node in the sorted `top_nodes` list and checks if adding the current node's token size to `total_tokens` would exceed `max_tokens`.\n\n   - If not, the node is appended to the `result` list.\n   - The `total_tokens` is updated by adding the current node's token size.\n\n6. **Return Result**: The function finally returns the `result` list, which presumably contains a subset of nodes from the original tree that maximize some criteria while keeping within a token limit.\n\nThe algorithm essentially collapses a tree into a prioritized list of nodes based on their relevance to a query, constrained by a token limit.\nThe table compares RAPTOR and DPR responses to two questions about the story of Cinderella:""), Document(metadata={'page_source': '18', 'doc_source': '2401.18059v1', 'start_index': 706, '_id': '4264e246f74246fe9cb2591dcce06d99', '_collection_name': 'test'}, page_content=""4. **Initializing Result and Token Count**: The algorithm initializes an empty list `result` and sets `total_tokens` to zero.\n\n5. **Iterating Over Top Nodes**: It then iterates over each node in the sorted `top_nodes` list and checks if adding the current node's token size to `total_tokens` would exceed `max_tokens`.\n\n   - If not, the node is appended to the `result` list.\n   - The `total_tokens` is updated by adding the current node's token size.\n\n6. **Return Result**: The function finally returns the `result` list, which presumably contains a subset of nodes from the original tree that maximize some criteria while keeping within a token limit.\n\nThe algorithm essentially collapses a tree into a prioritized list of nodes based on their relevance to a query, constrained by a token limit.\nThe table compares RAPTOR and DPR responses to two questions about the story of Cinderella:""), Document(metadata={'page_source': '17', 'doc_source': '2401.18059v1', 'start_index': 1731, '_id': '9cd882e5d1cf45448b6bd9da57e6bf9b', '_collection_name': 'test'}, page_content='1. **Function Definition**: The function `TRAVERSETREE` takes three arguments: a `tree`, a `query`, and a parameter `k`.\n\n2. **Initialization**: \n   - `S_current` is initialized to the first layer of the tree using `tree.layer[0]`.\n\n3. **Loop through Tree Layers**: \n   - A `for` loop iterates over the number of layers in the tree using `range(tree.num_layers)`.\n\n4. **Processing Nodes in Current Layer**:\n   - An empty list `top_k` is initialized to keep track of the top-k nodes in each layer.\n   - Another loop iterates over each `node` in `S_current`, computing a `score` using the `dot_product` function with `query` and `node` as inputs.\n   - Each node along with its computed score is appended to the `top_k` list as a tuple `(node, score)`.\n\n5. **Select Top-k Nodes**:\n   - `S_layer` is assigned the top-k nodes from `sorted(top_k)[:k].nodes`, where `top_k` is sorted based on scores.'), Document(metadata={'page_source': '17', 'doc_source': '2401.18059v1', 'start_index': 1731, '_id': '7ef31c7782a74d5185b820a446da8914', '_collection_name': 'test'}, page_content='1. **Function Definition**: The function `TRAVERSETREE` takes three arguments: a `tree`, a `query`, and a parameter `k`.\n\n2. **Initialization**: \n   - `S_current` is initialized to the first layer of the tree using `tree.layer[0]`.\n\n3. **Loop through Tree Layers**: \n   - A `for` loop iterates over the number of layers in the tree using `range(tree.num_layers)`.\n\n4. **Processing Nodes in Current Layer**:\n   - An empty list `top_k` is initialized to keep track of the top-k nodes in each layer.\n   - Another loop iterates over each `node` in `S_current`, computing a `score` using the `dot_product` function with `query` and `node` as inputs.\n   - Each node along with its computed score is appended to the `top_k` list as a tuple `(node, score)`.\n\n5. **Select Top-k Nodes**:\n   - `S_layer` is assigned the top-k nodes from `sorted(top_k)[:k].nodes`, where `top_k` is sorted based on scores.'), Document(metadata={'page_source': '18', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '2df365cb9c89435685baa42b8f90838b', '_collection_name': 'test'}, page_content='The table contains pseudocode for an algorithm labeled as ""Algorithm 2: Collapsed Tree Algorithm"". The function `COLLAPSEDTREE` is designed to process a data structure, presumably a tree, based on several parameters: `tree`, `query`, `k`, and `max_tokens`. Here\'s a breakdown of the algorithm:\n\n1. **Flatten the Tree**: The tree is first flattened into a one-dimensional structure using a function `flatten(tree)`.\n\n2. **Calculate Dot Product**: For each node in the flattened tree, a dot product between the `query` and the `node` is computed and stored in a list `top_nodes` along with the node itself.\n\n3. **Sorting**: The `top_nodes` list is sorted, although it is not specified what it is sorted by.\n\n4. **Initializing Result and Token Count**: The algorithm initializes an empty list `result` and sets `total_tokens` to zero.'), Document(metadata={'page_source': '18', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '5773eb452b0843af8d5bfc7e17b5aed1', '_collection_name': 'test'}, page_content='The table contains pseudocode for an algorithm labeled as ""Algorithm 2: Collapsed Tree Algorithm"". The function `COLLAPSEDTREE` is designed to process a data structure, presumably a tree, based on several parameters: `tree`, `query`, `k`, and `max_tokens`. Here\'s a breakdown of the algorithm:\n\n1. **Flatten the Tree**: The tree is first flattened into a one-dimensional structure using a function `flatten(tree)`.\n\n2. **Calculate Dot Product**: For each node in the flattened tree, a dot product between the `query` and the `node` is computed and stored in a list `top_nodes` along with the node itself.\n\n3. **Sorting**: The `top_nodes` list is sorted, although it is not specified what it is sorted by.\n\n4. **Initializing Result and Token Count**: The algorithm initializes an empty list `result` and sets `total_tokens` to zero.'), Document(metadata={'page_source': '17', 'doc_source': '2401.18059v1', 'start_index': 2482, '_id': '3ce13893c184400ea79009e4ac41b290', '_collection_name': 'test'}, page_content='5. **Select Top-k Nodes**:\n   - `S_layer` is assigned the top-k nodes from `sorted(top_k)[:k].nodes`, where `top_k` is sorted based on scores.\n\n6. **Update Current Layer**:\n   - `S_current` is updated to be the current set of selected top-k nodes, `S_layer`.\n\n7. **Return Statement**:\n   - The function returns the union of sets from all the layers traversed (`S_0 ∪ S_1 ∪ S_2 ... ∪ S_k`).\n\n8. **End of Function**: The function is concluded with an `end function` statement.'), Document(metadata={'page_source': '17', 'doc_source': '2401.18059v1', 'start_index': 2482, '_id': 'ae0343d74d364b44ba2900ff483d30f2', '_collection_name': 'test'}, page_content='5. **Select Top-k Nodes**:\n   - `S_layer` is assigned the top-k nodes from `sorted(top_k)[:k].nodes`, where `top_k` is sorted based on scores.\n\n6. **Update Current Layer**:\n   - `S_current` is updated to be the current set of selected top-k nodes, `S_layer`.\n\n7. **Return Statement**:\n   - The function returns the union of sets from all the layers traversed (`S_0 ∪ S_1 ∪ S_2 ... ∪ S_k`).\n\n8. **End of Function**: The function is concluded with an `end function` statement.'), Document(metadata={'page_source': '17', 'doc_source': '2401.18059v1', 'start_index': 905, '_id': '4f74f284dead4c3fa8d836bcce39f3bb', '_collection_name': 'test'}, page_content='The hallucination here is that the summary states that Jr. Ajor and Co-Tan are sisters, but does not explicitly mention or imply this. \nUpon reviewing all parent nodes, we found that hallucinations did not propagate to higher layers. Generally, the hallucinations were minor and did not alter the thematic interpretation of the text. \nE.3 I MPACT ON  QA T ASKS \nIn our findings, hallucinations had no discernible impact on the performance of QA tasks. This sug- gests that hallucination is not a major concerns for the sum mari z ation component in our RAPTOR architecture. \nF P SEUDOCODE FOR  R ETRIEVAL  M ETHODS \nThe table contains a pseudocode listing for an algorithm titled ""Algorithm 1: Tree Traversal Algorithm."" This algorithm is designed to traverse a tree structure. Here is a brief explanation of the pseudocode:'), Document(metadata={'page_source': '17', 'doc_source': '2401.18059v1', 'start_index': 905, '_id': 'eb058587f8444b759b15050bf7e4471c', '_collection_name': 'test'}, page_content='The hallucination here is that the summary states that Jr. Ajor and Co-Tan are sisters, but does not explicitly mention or imply this. \nUpon reviewing all parent nodes, we found that hallucinations did not propagate to higher layers. Generally, the hallucinations were minor and did not alter the thematic interpretation of the text. \nE.3 I MPACT ON  QA T ASKS \nIn our findings, hallucinations had no discernible impact on the performance of QA tasks. This sug- gests that hallucination is not a major concerns for the sum mari z ation component in our RAPTOR architecture. \nF P SEUDOCODE FOR  R ETRIEVAL  M ETHODS \nThe table contains a pseudocode listing for an algorithm titled ""Algorithm 1: Tree Traversal Algorithm."" This algorithm is designed to traverse a tree structure. Here is a brief explanation of the pseudocode:'), Document(metadata={'page_source': '1', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '06f36d3a2bd04300bd63eac53f03789c', '_collection_name': 'test'}, page_content=""The image illustrates the tree construction process used by RAPTOR to cluster and summarize text. Here's a breakdown of its components:\n\n1. **RAPTOR Tree (Left Panel):**\n   - **Root Layer:** Contains nodes (9 and 10).\n   - **Leaf Layer:** Contains nodes (1 to 5).\n   - Clustering of nodes forms a hierarchical structure from the leaf layer up to the root.\n\n2. **Formation of One Tree Layer (Middle Panel):**\n   - **Step 1: Clustering** - Text chunks (1 through 5) are clustered based on their embeddings.\n   - **Step 2: Summarization by LLM** - Clusters (such as those formed by nodes 2 and 3) are summarized to create another layer in the tree (nodes 6, 7, 8).\n\n3. **Contents of a Node (Right Panel):**\n   - **Index:** Identifies the node (e.g., #8).\n   - **Child Nodes:** Lists nodes contained within a cluster (e.g., 2, 3).\n   - **Text Summary:** Provides a summary of the cluster's contents.\n   - **Text Embedding:** Displays the vector representation used for clustering.""), Document(metadata={'page_source': '1', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '8cb93675a3fa4506b1d10fd773541bce', '_collection_name': 'test'}, page_content=""The image illustrates the tree construction process used by RAPTOR to cluster and summarize text. Here's a breakdown of its components:\n\n1. **RAPTOR Tree (Left Panel):**\n   - **Root Layer:** Contains nodes (9 and 10).\n   - **Leaf Layer:** Contains nodes (1 to 5).\n   - Clustering of nodes forms a hierarchical structure from the leaf layer up to the root.\n\n2. **Formation of One Tree Layer (Middle Panel):**\n   - **Step 1: Clustering** - Text chunks (1 through 5) are clustered based on their embeddings.\n   - **Step 2: Summarization by LLM** - Clusters (such as those formed by nodes 2 and 3) are summarized to create another layer in the tree (nodes 6, 7, 8).\n\n3. **Contents of a Node (Right Panel):**\n   - **Index:** Identifies the node (e.g., #8).\n   - **Child Nodes:** Lists nodes contained within a cluster (e.g., 2, 3).\n   - **Text Summary:** Provides a summary of the cluster's contents.\n   - **Text Embedding:** Displays the vector representation used for clustering.""), Document(metadata={'page_source': '3', 'doc_source': '2401.18059v1', 'start_index': 3826, '_id': '97acd1da24a24ee8a786b079fa0c91ab', '_collection_name': 'test'}, page_content='Querying In this section, we elaborate on the two querying mechanisms employed by RAPTOR: tree traversal and collapsed tree. These methods offer unique ways of traversing the multi-layered RAPTOR tree to retrieve relevant information, each with its own advantages and trade-offs. We provide the pseudocode of both methods in Appendix  F . Note that we embed all nodes using SBERT. \nThe  tree traversal  method first selects the top-k most relevant root nodes based on their cosine similarity to the query embedding. The children of these selected nodes are considered at the next layer and the top  $\\cdot\\mathrm{k}$   nodes are selected from this pool again based on their cosine similarity to the query vector. This process is repeated until we reach the leaf nodes. Finally, the text from all selected nodes is concatenated to form the retrieved context. The algorithm’s steps are outlined below:'), Document(metadata={'page_source': '3', 'doc_source': '2401.18059v1', 'start_index': 3826, '_id': 'f246024f8f9b461e9eec4737923faeff', '_collection_name': 'test'}, page_content='Querying In this section, we elaborate on the two querying mechanisms employed by RAPTOR: tree traversal and collapsed tree. These methods offer unique ways of traversing the multi-layered RAPTOR tree to retrieve relevant information, each with its own advantages and trade-offs. We provide the pseudocode of both methods in Appendix  F . Note that we embed all nodes using SBERT. \nThe  tree traversal  method first selects the top-k most relevant root nodes based on their cosine similarity to the query embedding. The children of these selected nodes are considered at the next layer and the top  $\\cdot\\mathrm{k}$   nodes are selected from this pool again based on their cosine similarity to the query vector. This process is repeated until we reach the leaf nodes. Finally, the text from all selected nodes is concatenated to form the retrieved context. The algorithm’s steps are outlined below:'), Document(metadata={'page_source': '4', 'doc_source': '2401.18059v1', 'start_index': 0, '_id': '3d526011c1374e99ad048ee3c9961fa6', '_collection_name': 'test'}, page_content='The image illustrates two mechanisms for retrieving information using tree structures: Tree Traversal Retrieval and Collapsed Tree Retrieval.\n\n1. **Tree Traversal Retrieval (A)**:\n   - Starts with a query that is encoded into a vector.\n   - The tree structure is traversed starting from the root.\n   - At each level, it retrieves the top nodes based on cosine similarity to the query vector.\n   - Retrieves context from selected nodes which is then combined with the query to generate an answer using a language model (LLM).\n\n2. **Collapsed Tree Retrieval (B)**:\n   - Also starts with a query that is encoded.\n   - The tree is collapsed into a single layer.\n   - Nodes are retrieved based on cosine similarity until reaching a threshold number of tokens.\n   - The retrieved context is combined with the query to produce an answer using an LLM.')]"
10012,"In the figure that has a tree shape, what is the name of the branch that has the least leafs?","[Document(metadata={'page_source': '7', 'doc_source': '2312.10997v5', 'start_index': 4220, '_id': 'f7f10acb0a0f4c4abc9d698f7fa0b0e4', '_collection_name': 'test'}, page_content='and tables) and edges (indicating semantic/lexical similarity between paragraphs or relationships within the document structure), effectively addressing knowl- edge retrieval and reasoning problems in a multi-document environment.'), Document(metadata={'page_source': '7', 'doc_source': '2312.10997v5', 'start_index': 4220, '_id': '458fb8da3e0f4d0082f39a740063ad6a', '_collection_name': 'test'}, page_content='and tables) and edges (indicating semantic/lexical similarity between paragraphs or relationships within the document structure), effectively addressing knowl- edge retrieval and reasoning problems in a multi-document environment.'), Document(metadata={'page_source': '7', 'doc_source': '2312.10997v5', 'start_index': 3419, '_id': '679a13c4b7e947e9922bb9929941742d', '_collection_name': 'test'}, page_content='Knowledge Graph index . Utilize KG in constructing the hierarchical structure of documents contributes to maintaining consistency. It delineates the connections between different concepts and entities, markedly reducing the potential for illusions. Another advantage is the transformation of the information retrieval process into instructions that LLM can comprehend, thereby enhancing the accuracy of knowledge retrieval and enabling LLM to generate con textually coherent responses, thus improving the overall efficiency of the RAG system. To capture the logical relationship between document content and structure, KGP [91] proposed a method of building an index between multiple documents using KG. This KG consists of nodes (representing paragraphs or structures in the documents, such as pages and tables) and edges (indicating semantic/lexical similarity between paragraphs or relationships within the document structure), effectively addressing knowl- edge retrieval and reasoning problems'), Document(metadata={'page_source': '7', 'doc_source': '2312.10997v5', 'start_index': 3419, '_id': '26692df12d3140b7a4177979d2e718ef', '_collection_name': 'test'}, page_content='Knowledge Graph index . Utilize KG in constructing the hierarchical structure of documents contributes to maintaining consistency. It delineates the connections between different concepts and entities, markedly reducing the potential for illusions. Another advantage is the transformation of the information retrieval process into instructions that LLM can comprehend, thereby enhancing the accuracy of knowledge retrieval and enabling LLM to generate con textually coherent responses, thus improving the overall efficiency of the RAG system. To capture the logical relationship between document content and structure, KGP [91] proposed a method of building an index between multiple documents using KG. This KG consists of nodes (representing paragraphs or structures in the documents, such as pages and tables) and edges (indicating semantic/lexical similarity between paragraphs or relationships within the document structure), effectively addressing knowl- edge retrieval and reasoning problems'), Document(metadata={'page_source': '7', 'doc_source': '2312.10997v5', 'start_index': 2838, '_id': 'a3f63395050e4d8889b5da6fe40b6a41', '_collection_name': 'test'}, page_content='3) Structural Index:  One effective method for enhancing information retrieval is to establish a hierarchical structure for the documents. By constructing In structure, RAG system can expedite the retrieval and processing of pertinent data. \nHierarchical index structure . File are arranged in parent- child relationships, with chunks linked to them. Data sum- maries are stored at each node, aiding in the swift traversal of data and assisting the RAG system in determining which chunks to extract. This approach can also mitigate the illusion caused by block extraction issues.'), Document(metadata={'page_source': '7', 'doc_source': '2312.10997v5', 'start_index': 2838, '_id': '8588a88fab69463087980ec552dbdc10', '_collection_name': 'test'}, page_content='3) Structural Index:  One effective method for enhancing information retrieval is to establish a hierarchical structure for the documents. By constructing In structure, RAG system can expedite the retrieval and processing of pertinent data. \nHierarchical index structure . File are arranged in parent- child relationships, with chunks linked to them. Data sum- maries are stored at each node, aiding in the swift traversal of data and assisting the RAG system in determining which chunks to extract. This approach can also mitigate the illusion caused by block extraction issues.'), Document(metadata={'page_source': '6', 'doc_source': '2312.10997v5', 'start_index': 2319, '_id': '66b5cd0f3dfc47789d098b9e1c480102', '_collection_name': 'test'}, page_content='Structured data , such as knowledge graphs (KGs) [86] , which are typically verified and can provide more precise in- formation. KnowledGPT [15] generates KB search queries and stores knowledge in a personalized base, enhancing the RAG model’s knowledge richness. In response to the limitations of LLMs in understanding and answering questions about textual graphs, G-Retriever [84] integrates Graph Neural Networks (GNNs), LLMs and RAG, enhancing graph comprehension and question-answering capabilities through soft prompting of the LLM, and employs the Prize-Collecting Steiner Tree (PCST) optimization problem for targeted graph retrieval. On the contrary, it requires additional effort to build, validate, and maintain structured databases. On the contrary, it requires additional effort to build, validate, and maintain structured databases.'), Document(metadata={'page_source': '6', 'doc_source': '2312.10997v5', 'start_index': 2319, '_id': 'b358fa0359a64cf0bd78c28530dbea76', '_collection_name': 'test'}, page_content='Structured data , such as knowledge graphs (KGs) [86] , which are typically verified and can provide more precise in- formation. KnowledGPT [15] generates KB search queries and stores knowledge in a personalized base, enhancing the RAG model’s knowledge richness. In response to the limitations of LLMs in understanding and answering questions about textual graphs, G-Retriever [84] integrates Graph Neural Networks (GNNs), LLMs and RAG, enhancing graph comprehension and question-answering capabilities through soft prompting of the LLM, and employs the Prize-Collecting Steiner Tree (PCST) optimization problem for targeted graph retrieval. On the contrary, it requires additional effort to build, validate, and maintain structured databases. On the contrary, it requires additional effort to build, validate, and maintain structured databases.'), Document(metadata={'page_source': '1', 'doc_source': '2312.10997v5', 'start_index': 3214, '_id': '50bd1b4905fc4c779a3728993c395981', '_collection_name': 'test'}, page_content='A. Naive RAG \nThe Naive RAG research paradigm represents the earli- est methodology, which gained prominence shortly after the'), Document(metadata={'page_source': '1', 'doc_source': '2312.10997v5', 'start_index': 3214, '_id': 'f85fa07794c14a2fa926a0001e178e7d', '_collection_name': 'test'}, page_content='A. Naive RAG \nThe Naive RAG research paradigm represents the earli- est methodology, which gained prominence shortly after the'), Document(metadata={'page_source': '5', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': '83c36e68ef3143cc97aee3e86894e146', '_collection_name': 'test'}, page_content=""The table lists various methods used in retrieval tasks, with details on their retrieval sources, data types, granularity, augmentation stage, and processes. Here's a breakdown of the columns:\n\n1. **Method**: Names of the retrieval methods.\n2. **Retrieval Source**: Sources from where data is retrieved (e.g., Wikipedia, Search Engine, Dataset-base).\n3. **Retrieval Data Type**: Type of data used for retrieval, such as Text or Knowledge Graph (KG).\n4. **Retrieval Granularity**: The level at which data is retrieved, e.g., Phrase, Sentence, Chunk, Item, Doc, Sub-Graph, etc.\n5. **Augmentation Stage**: The stage at which augmentation occurs, e.g., Pre-training, Tuning, Inference.\n6. **Retrieval Process**: The processes involved, such as Iterative or Once.\n\nThe table organizes different methods based on these characteristics to show a comparative view of various retrieval techniques.""), Document(metadata={'page_source': '5', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': '516cdf2f76004a15aa4f9c6eadc012eb', '_collection_name': 'test'}, page_content=""The table lists various methods used in retrieval tasks, with details on their retrieval sources, data types, granularity, augmentation stage, and processes. Here's a breakdown of the columns:\n\n1. **Method**: Names of the retrieval methods.\n2. **Retrieval Source**: Sources from where data is retrieved (e.g., Wikipedia, Search Engine, Dataset-base).\n3. **Retrieval Data Type**: Type of data used for retrieval, such as Text or Knowledge Graph (KG).\n4. **Retrieval Granularity**: The level at which data is retrieved, e.g., Phrase, Sentence, Chunk, Item, Doc, Sub-Graph, etc.\n5. **Augmentation Stage**: The stage at which augmentation occurs, e.g., Pre-training, Tuning, Inference.\n6. **Retrieval Process**: The processes involved, such as Iterative or Once.\n\nThe table organizes different methods based on these characteristics to show a comparative view of various retrieval techniques.""), Document(metadata={'page_source': '10', 'doc_source': '2312.10997v5', 'start_index': 1959, '_id': 'e54584fa89a347418be26161b2f05a9e', '_collection_name': 'test'}, page_content='B. Recursive Retrieval \nRecursive retrieval is often used in information retrieval and NLP to improve the depth and relevance of search results. The process involves iterative ly refining search queries based on the results obtained from previous searches. Recursive Retrieval aims to enhance the search experience by gradu- ally converging on the most pertinent information through a feedback loop. IRCoT [61] uses chain-of-thought to guide the retrieval process and refines the CoT with the obtained retrieval results. ToC [57] creates a clarification tree that systematically optimizes the ambiguous parts in the Query. It can be particularly useful in complex search scenarios where the user’s needs are not entirely clear from the outset or where the information sought is highly specialized or nuanced. The recursive nature of the process allows for continuous learning and adaptation to the user’s requirements, often resulting in improved satisfaction with the search outcomes.'), Document(metadata={'page_source': '10', 'doc_source': '2312.10997v5', 'start_index': 1959, '_id': 'c5ceb3d5bc944d79a766c9c2d71ca3a5', '_collection_name': 'test'}, page_content='B. Recursive Retrieval \nRecursive retrieval is often used in information retrieval and NLP to improve the depth and relevance of search results. The process involves iterative ly refining search queries based on the results obtained from previous searches. Recursive Retrieval aims to enhance the search experience by gradu- ally converging on the most pertinent information through a feedback loop. IRCoT [61] uses chain-of-thought to guide the retrieval process and refines the CoT with the obtained retrieval results. ToC [57] creates a clarification tree that systematically optimizes the ambiguous parts in the Query. It can be particularly useful in complex search scenarios where the user’s needs are not entirely clear from the outset or where the information sought is highly specialized or nuanced. The recursive nature of the process allows for continuous learning and adaptation to the user’s requirements, often resulting in improved satisfaction with the search outcomes.'), Document(metadata={'page_source': '3', 'doc_source': '2312.10997v5', 'start_index': 627, '_id': '1f38c0d088214eb586d7f9d6c2bb79ae', '_collection_name': 'test'}, page_content='3. **Modular RAG** (Right):\n   - Enhances flexibility by introducing various specific functional modules.\n   - Includes iterative and adaptive processes like routing, searching, retrieving, reading, predicting, rewriting, reranking, demonstrating, and fusion.\n   - Facilitates non-sequential methods such as iterative retrieval and adaptive approaches, allowing for more dynamic interactions.')]"
10013,"In figure 3, how many distinct icons are used?","[Document(metadata={'page_source': '13', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': 'fa516c5a30174a5d90cd3efbf73eaffa', '_collection_name': 'test'}, page_content='The table appears to categorize different evaluation metrics or criteria (listed in the first column) based on which aspects they assess (represented by the column headers from the second to the last column). Each checkmark (✓) indicates that the corresponding metric evaluates or is relevant to the corresponding aspect.\n\nHere is a breakdown of the contents:\n\n- **Context Relevance** is assessed by Accuracy, EM, Recall, Precision, Cosine Similarity, Hit Rate, MRR, and ROUGE/ROUGE-L.\n- **Faithfulness** is assessed by Accuracy, EM, BLEU, and ROUGE/ROUGE-L.\n- **Answer Relevance** is assessed by Accuracy, EM, and R-Rate.\n- **Noise Robustness** is assessed by Accuracy, Recall, and Precision.\n- **Negative Rejection** is assessed by Accuracy and EM.\n- **Information Integration** is assessed by Accuracy, MRR, and ROUGE/ROUGE-L.\n- **Counterfactual Robustness** is assessed by Accuracy and ROUGE/ROUGE-L.'), Document(metadata={'page_source': '13', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': '01fbbf9fa3644714aa96f14b6d2fb597', '_collection_name': 'test'}, page_content='The table appears to categorize different evaluation metrics or criteria (listed in the first column) based on which aspects they assess (represented by the column headers from the second to the last column). Each checkmark (✓) indicates that the corresponding metric evaluates or is relevant to the corresponding aspect.\n\nHere is a breakdown of the contents:\n\n- **Context Relevance** is assessed by Accuracy, EM, Recall, Precision, Cosine Similarity, Hit Rate, MRR, and ROUGE/ROUGE-L.\n- **Faithfulness** is assessed by Accuracy, EM, BLEU, and ROUGE/ROUGE-L.\n- **Answer Relevance** is assessed by Accuracy, EM, and R-Rate.\n- **Noise Robustness** is assessed by Accuracy, Recall, and Precision.\n- **Negative Rejection** is assessed by Accuracy and EM.\n- **Information Integration** is assessed by Accuracy, MRR, and ROUGE/ROUGE-L.\n- **Counterfactual Robustness** is assessed by Accuracy and ROUGE/ROUGE-L.'), Document(metadata={'page_source': '14', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': 'a7aa6b752676471ca53fc28edebe36ff', '_collection_name': 'test'}, page_content=""The table provides an overview of different evaluation frameworks, their targets, aspects, and quantitative metrics used in evaluation. Here's the breakdown:\n\n1. **RGB†**\n   - **Targets**: Retrieval Quality, Generation Quality\n   - **Aspects**: Noise Robustness, Negative Rejection, Information Integration, Counterfactual Robustness\n   - **Metrics**: Accuracy, EM (Exact Match), Accuracy, Accuracy\n\n2. **RECALL†**\n   - **Targets**: Generation Quality\n   - **Aspects**: Counterfactual Robustness\n   - **Metrics**: R-Rate (Reappearance Rate)\n\n3. **RAGAS‡**\n   - **Targets**: Retrieval Quality, Generation Quality\n   - **Aspects**: Context Relevance, Faithfulness, Answer Relevance\n   - **Metrics**: *, *, Cosine Similarity\n\n4. **ARES‡**\n   - **Targets**: Retrieval Quality, Generation Quality\n   - **Aspects**: Context Relevance, Faithfulness, Answer Relevance\n   - **Metrics**: Accuracy, Accuracy, Accuracy""), Document(metadata={'page_source': '14', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': '5ded92c7dea6435098030dc2d96dac30', '_collection_name': 'test'}, page_content=""The table provides an overview of different evaluation frameworks, their targets, aspects, and quantitative metrics used in evaluation. Here's the breakdown:\n\n1. **RGB†**\n   - **Targets**: Retrieval Quality, Generation Quality\n   - **Aspects**: Noise Robustness, Negative Rejection, Information Integration, Counterfactual Robustness\n   - **Metrics**: Accuracy, EM (Exact Match), Accuracy, Accuracy\n\n2. **RECALL†**\n   - **Targets**: Generation Quality\n   - **Aspects**: Counterfactual Robustness\n   - **Metrics**: R-Rate (Reappearance Rate)\n\n3. **RAGAS‡**\n   - **Targets**: Retrieval Quality, Generation Quality\n   - **Aspects**: Context Relevance, Faithfulness, Answer Relevance\n   - **Metrics**: *, *, Cosine Similarity\n\n4. **ARES‡**\n   - **Targets**: Retrieval Quality, Generation Quality\n   - **Aspects**: Context Relevance, Faithfulness, Answer Relevance\n   - **Metrics**: Accuracy, Accuracy, Accuracy""), Document(metadata={'page_source': '6', 'doc_source': '2312.10997v5', 'start_index': 851, '_id': 'e34eec6e1d1f4936978151089763a43b', '_collection_name': 'test'}, page_content='The diagram visually represents these methods as positioned on the axes of ""External Knowledge Required"" vs. ""Model Adaptation Required"".\nUnstructured Data , such as text, is the most widely used retrieval source, which are mainly gathered from corpus. For open-domain question-answering (ODQA) tasks, the primary retrieval sources are Wikipedia Dump with the current major versions including HotpotQA   4   (1st October , 2017), DPR 5   (20 December, 2018). In addition to encyclopedic data, common unstructured data includes cross-lingual text [19] and domain- specific data (such as medical [67]and legal domains [29]).'), Document(metadata={'page_source': '6', 'doc_source': '2312.10997v5', 'start_index': 851, '_id': 'cdd75ef7c9ca449db4fabc7a8e7f411c', '_collection_name': 'test'}, page_content='The diagram visually represents these methods as positioned on the axes of ""External Knowledge Required"" vs. ""Model Adaptation Required"".\nUnstructured Data , such as text, is the most widely used retrieval source, which are mainly gathered from corpus. For open-domain question-answering (ODQA) tasks, the primary retrieval sources are Wikipedia Dump with the current major versions including HotpotQA   4   (1st October , 2017), DPR 5   (20 December, 2018). In addition to encyclopedic data, common unstructured data includes cross-lingual text [19] and domain- specific data (such as medical [67]and legal domains [29]).'), Document(metadata={'page_source': '5', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': '516cdf2f76004a15aa4f9c6eadc012eb', '_collection_name': 'test'}, page_content=""The table lists various methods used in retrieval tasks, with details on their retrieval sources, data types, granularity, augmentation stage, and processes. Here's a breakdown of the columns:\n\n1. **Method**: Names of the retrieval methods.\n2. **Retrieval Source**: Sources from where data is retrieved (e.g., Wikipedia, Search Engine, Dataset-base).\n3. **Retrieval Data Type**: Type of data used for retrieval, such as Text or Knowledge Graph (KG).\n4. **Retrieval Granularity**: The level at which data is retrieved, e.g., Phrase, Sentence, Chunk, Item, Doc, Sub-Graph, etc.\n5. **Augmentation Stage**: The stage at which augmentation occurs, e.g., Pre-training, Tuning, Inference.\n6. **Retrieval Process**: The processes involved, such as Iterative or Once.\n\nThe table organizes different methods based on these characteristics to show a comparative view of various retrieval techniques.""), Document(metadata={'page_source': '5', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': '83c36e68ef3143cc97aee3e86894e146', '_collection_name': 'test'}, page_content=""The table lists various methods used in retrieval tasks, with details on their retrieval sources, data types, granularity, augmentation stage, and processes. Here's a breakdown of the columns:\n\n1. **Method**: Names of the retrieval methods.\n2. **Retrieval Source**: Sources from where data is retrieved (e.g., Wikipedia, Search Engine, Dataset-base).\n3. **Retrieval Data Type**: Type of data used for retrieval, such as Text or Knowledge Graph (KG).\n4. **Retrieval Granularity**: The level at which data is retrieved, e.g., Phrase, Sentence, Chunk, Item, Doc, Sub-Graph, etc.\n5. **Augmentation Stage**: The stage at which augmentation occurs, e.g., Pre-training, Tuning, Inference.\n6. **Retrieval Process**: The processes involved, such as Iterative or Once.\n\nThe table organizes different methods based on these characteristics to show a comparative view of various retrieval techniques.""), Document(metadata={'page_source': '3', 'doc_source': '2312.10997v5', 'start_index': 627, '_id': '1f38c0d088214eb586d7f9d6c2bb79ae', '_collection_name': 'test'}, page_content='3. **Modular RAG** (Right):\n   - Enhances flexibility by introducing various specific functional modules.\n   - Includes iterative and adaptive processes like routing, searching, retrieving, reading, predicting, rewriting, reranking, demonstrating, and fusion.\n   - Facilitates non-sequential methods such as iterative retrieval and adaptive approaches, allowing for more dynamic interactions.'), Document(metadata={'page_source': '3', 'doc_source': '2312.10997v5', 'start_index': 627, '_id': 'a340e07d645a4fffabd7121175e02817', '_collection_name': 'test'}, page_content='3. **Modular RAG** (Right):\n   - Enhances flexibility by introducing various specific functional modules.\n   - Includes iterative and adaptive processes like routing, searching, retrieving, reading, predicting, rewriting, reranking, demonstrating, and fusion.\n   - Facilitates non-sequential methods such as iterative retrieval and adaptive approaches, allowing for more dynamic interactions.'), Document(metadata={'page_source': '3', 'doc_source': '2312.10997v5', 'start_index': 1022, '_id': 'dfd72e14b8174f3fae222e66a0b3d2ba', '_collection_name': 'test'}, page_content='The image visually represents these paradigms to illustrate their structures and components.\nPre-retrieval process . In this stage, the primary focus is on optimizing the indexing structure and the original query. The goal of optimizing indexing is to enhance the quality of the content being indexed. This involves strategies: enhancing data granularity, optimizing index structures, adding metadata, alignment optimization, and mixed retrieval. While the goal of query optimization is to make the user’s original question clearer and more suitable for the retrieval task. Common methods include query rewriting query transformation, query expansion and other techniques [7], [9]–[11].'), Document(metadata={'page_source': '3', 'doc_source': '2312.10997v5', 'start_index': 1022, '_id': '5792dd03b40e4bd0a2f9a1fafb0b7565', '_collection_name': 'test'}, page_content='The image visually represents these paradigms to illustrate their structures and components.\nPre-retrieval process . In this stage, the primary focus is on optimizing the indexing structure and the original query. The goal of optimizing indexing is to enhance the quality of the content being indexed. This involves strategies: enhancing data granularity, optimizing index structures, adding metadata, alignment optimization, and mixed retrieval. While the goal of query optimization is to make the user’s original question clearer and more suitable for the retrieval task. Common methods include query rewriting query transformation, query expansion and other techniques [7], [9]–[11].'), Document(metadata={'page_source': '13', 'doc_source': '2312.10997v5', 'start_index': 906, '_id': 'd8ee467dcf9d4e27b8ad568b8cd6e76e', '_collection_name': 'test'}, page_content='Each metric is mapped to one or more aspects to describe its relevance or functionality in evaluation.\nThe specific metrics for each evaluation aspect are sum- marized in Table III. It is essential to recognize that these metrics, derived from related work, are traditional measures and do not yet represent a mature or standardized approach for quantifying RAG evaluation aspects. Custom metrics tailored to the nuances of RAG models, though not included here, have also been developed in some evaluation studies. \nD. Evaluation Benchmarks and Tools'), Document(metadata={'page_source': '13', 'doc_source': '2312.10997v5', 'start_index': 906, '_id': 'cd2e959555a645fea25b73f406094221', '_collection_name': 'test'}, page_content='Each metric is mapped to one or more aspects to describe its relevance or functionality in evaluation.\nThe specific metrics for each evaluation aspect are sum- marized in Table III. It is essential to recognize that these metrics, derived from related work, are traditional measures and do not yet represent a mature or standardized approach for quantifying RAG evaluation aspects. Custom metrics tailored to the nuances of RAG models, though not included here, have also been developed in some evaluation studies. \nD. Evaluation Benchmarks and Tools'), Document(metadata={'page_source': '1', 'doc_source': '2312.10997v5', 'start_index': 3214, '_id': 'f85fa07794c14a2fa926a0001e178e7d', '_collection_name': 'test'}, page_content='A. Naive RAG \nThe Naive RAG research paradigm represents the earli- est methodology, which gained prominence shortly after the')]"
10014,What is the paper's full title that proposes the method that has a retrieval granularity of phrase?,"[Document(metadata={'page_source': '5', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': '83c36e68ef3143cc97aee3e86894e146', '_collection_name': 'test'}, page_content=""The table lists various methods used in retrieval tasks, with details on their retrieval sources, data types, granularity, augmentation stage, and processes. Here's a breakdown of the columns:\n\n1. **Method**: Names of the retrieval methods.\n2. **Retrieval Source**: Sources from where data is retrieved (e.g., Wikipedia, Search Engine, Dataset-base).\n3. **Retrieval Data Type**: Type of data used for retrieval, such as Text or Knowledge Graph (KG).\n4. **Retrieval Granularity**: The level at which data is retrieved, e.g., Phrase, Sentence, Chunk, Item, Doc, Sub-Graph, etc.\n5. **Augmentation Stage**: The stage at which augmentation occurs, e.g., Pre-training, Tuning, Inference.\n6. **Retrieval Process**: The processes involved, such as Iterative or Once.\n\nThe table organizes different methods based on these characteristics to show a comparative view of various retrieval techniques.""), Document(metadata={'page_source': '5', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': '516cdf2f76004a15aa4f9c6eadc012eb', '_collection_name': 'test'}, page_content=""The table lists various methods used in retrieval tasks, with details on their retrieval sources, data types, granularity, augmentation stage, and processes. Here's a breakdown of the columns:\n\n1. **Method**: Names of the retrieval methods.\n2. **Retrieval Source**: Sources from where data is retrieved (e.g., Wikipedia, Search Engine, Dataset-base).\n3. **Retrieval Data Type**: Type of data used for retrieval, such as Text or Knowledge Graph (KG).\n4. **Retrieval Granularity**: The level at which data is retrieved, e.g., Phrase, Sentence, Chunk, Item, Doc, Sub-Graph, etc.\n5. **Augmentation Stage**: The stage at which augmentation occurs, e.g., Pre-training, Tuning, Inference.\n6. **Retrieval Process**: The processes involved, such as Iterative or Once.\n\nThe table organizes different methods based on these characteristics to show a comparative view of various retrieval techniques.""), Document(metadata={'page_source': '2', 'doc_source': '2312.10997v5', 'start_index': 1377, '_id': 'a2c29f83ea764f57acf61b66ed1e45c3', '_collection_name': 'test'}, page_content='Retrieval . Upon receipt of a user query, the RAG system employs the same encoding model utilized during the indexing phase to transform the query into a vector representation. It then computes the similarity scores between the query vector and the vector of chunks within the indexed corpus. The system prioritizes and retrieves the top K chunks that demonstrate the greatest similarity to the query. These chunks are subsequently used as the expanded context in prompt.'), Document(metadata={'page_source': '2', 'doc_source': '2312.10997v5', 'start_index': 1377, '_id': 'e5010ec0b7af4fd5b19d7919cabcf9be', '_collection_name': 'test'}, page_content='Retrieval . Upon receipt of a user query, the RAG system employs the same encoding model utilized during the indexing phase to transform the query into a vector representation. It then computes the similarity scores between the query vector and the vector of chunks within the indexed corpus. The system prioritizes and retrieves the top K chunks that demonstrate the greatest similarity to the query. These chunks are subsequently used as the expanded context in prompt.'), Document(metadata={'page_source': '7', 'doc_source': '2312.10997v5', 'start_index': 1, '_id': '60f38039544a4749990be86f3c62fd07', '_collection_name': 'test'}, page_content='In text, retrieval granularity ranges from fine to coarse, including Token, Phrase, Sentence, Proposition, Chunks, Doc- ument. Among them, DenseX [30]proposed the concept of using propositions as retrieval units. Propositions are defined as atomic expressions in the text, each encapsulating a unique factual segment and presented in a concise, self-contained nat- ural language format. This approach aims to enhance retrieval precision and relevance. On the Knowledge Graph (KG), retrieval granularity includes Entity, Triplet, and sub-Graph. The granularity of retrieval can also be adapted to downstream tasks, such as retrieving Item IDs [40]in recommendation tasks and Sentence pairs [38]. Detailed information is illustrated in Table I. \nB. Indexing Optimization'), Document(metadata={'page_source': '7', 'doc_source': '2312.10997v5', 'start_index': 1, '_id': '90c55ab993284b7f96de65b77c79e371', '_collection_name': 'test'}, page_content='In text, retrieval granularity ranges from fine to coarse, including Token, Phrase, Sentence, Proposition, Chunks, Doc- ument. Among them, DenseX [30]proposed the concept of using propositions as retrieval units. Propositions are defined as atomic expressions in the text, each encapsulating a unique factual segment and presented in a concise, self-contained nat- ural language format. This approach aims to enhance retrieval precision and relevance. On the Knowledge Graph (KG), retrieval granularity includes Entity, Triplet, and sub-Graph. The granularity of retrieval can also be adapted to downstream tasks, such as retrieving Item IDs [40]in recommendation tasks and Sentence pairs [38]. Detailed information is illustrated in Table I. \nB. Indexing Optimization'), Document(metadata={'page_source': '7', 'doc_source': '2312.10997v5', 'start_index': 1017, '_id': '38a6b358a63a48ef911bc4c2f23900ca', '_collection_name': 'test'}, page_content='1) Chunking Strategy:  The most common method is to split the document into chunks on a fixed number of tokens (e.g., 100, 256, 512) [88]. Larger chunks can capture more context, but they also generate more noise, requiring longer processing time and higher costs. While smaller chunks may not fully convey the necessary context, they do have less noise. How- ever, chunks leads to truncation within sentences, prompting the optimization of a recursive splits and sliding window meth- ods, enabling layered retrieval by merging globally related information across multiple retrieval processes [89]. Never- theless, these approaches still cannot strike a balance between semantic completeness and context length. Therefore, methods like Small2Big have been proposed, where sentences (small) are used as the retrieval unit, and the preceding and following sentences are provided as (big) context to LLMs [90].'), Document(metadata={'page_source': '7', 'doc_source': '2312.10997v5', 'start_index': 1017, '_id': '40b2a8bdd416407cbcec8afcc6e49036', '_collection_name': 'test'}, page_content='1) Chunking Strategy:  The most common method is to split the document into chunks on a fixed number of tokens (e.g., 100, 256, 512) [88]. Larger chunks can capture more context, but they also generate more noise, requiring longer processing time and higher costs. While smaller chunks may not fully convey the necessary context, they do have less noise. How- ever, chunks leads to truncation within sentences, prompting the optimization of a recursive splits and sliding window meth- ods, enabling layered retrieval by merging globally related information across multiple retrieval processes [89]. Never- theless, these approaches still cannot strike a balance between semantic completeness and context length. Therefore, methods like Small2Big have been proposed, where sentences (small) are used as the retrieval unit, and the preceding and following sentences are provided as (big) context to LLMs [90].'), Document(metadata={'page_source': '3', 'doc_source': '2312.10997v5', 'start_index': 1710, '_id': '944a0be050204385b03d82c4f136c71f', '_collection_name': 'test'}, page_content='Post-Retrieval Process . Once relevant context is retrieved, it’s crucial to integrate it effectively with the query. The main methods in post-retrieval process include rerank chunks and context compressing. Re-ranking the retrieved information to relocate the most relevant content to the edges of the prompt is a key strategy. This concept has been implemented in frame- works such as LlamaIndex 2 , LangChain 3 , and HayStack [12]. Feeding all relevant documents directly into LLMs can lead to information overload, diluting the focus on key details with irrelevant content.To mitigate this, post-retrieval efforts con- centrate on selecting the essential information, emphasizing critical sections, and shortening the context to be processed. \nC. Modular RAG'), Document(metadata={'page_source': '3', 'doc_source': '2312.10997v5', 'start_index': 1710, '_id': '74d65d9b1cb240168dd7221cc6503b2b', '_collection_name': 'test'}, page_content='Post-Retrieval Process . Once relevant context is retrieved, it’s crucial to integrate it effectively with the query. The main methods in post-retrieval process include rerank chunks and context compressing. Re-ranking the retrieved information to relocate the most relevant content to the edges of the prompt is a key strategy. This concept has been implemented in frame- works such as LlamaIndex 2 , LangChain 3 , and HayStack [12]. Feeding all relevant documents directly into LLMs can lead to information overload, diluting the focus on key details with irrelevant content.To mitigate this, post-retrieval efforts con- centrate on selecting the essential information, emphasizing critical sections, and shortening the context to be processed. \nC. Modular RAG'), Document(metadata={'page_source': '8', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': 'dc818c70314d40c3a94c9ebd669d8fdc', '_collection_name': 'test'}, page_content='2) Query Transformation:  The core concept is to retrieve chunks based on a transformed query instead of the user’s original query. \nQuery Rewrite .The original queries are not always optimal for LLM retrieval, especially in real-world scenarios. There- fore, we can prompt LLM to rewrite the queries. In addition to using LLM for query rewriting, specialized smaller language models, such as RRR (Rewrite-retrieve-read) [7]. The imple- mentation of the query rewrite method in the Taobao, known as BEQUE [9] has notably enhanced recall effectiveness for long-tail queries, resulting in a rise in GMV.'), Document(metadata={'page_source': '8', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': 'fab582da0aa34644a9415506d6e9c9ed', '_collection_name': 'test'}, page_content='2) Query Transformation:  The core concept is to retrieve chunks based on a transformed query instead of the user’s original query. \nQuery Rewrite .The original queries are not always optimal for LLM retrieval, especially in real-world scenarios. There- fore, we can prompt LLM to rewrite the queries. In addition to using LLM for query rewriting, specialized smaller language models, such as RRR (Rewrite-retrieve-read) [7]. The imple- mentation of the query rewrite method in the Taobao, known as BEQUE [9] has notably enhanced recall effectiveness for long-tail queries, resulting in a rise in GMV.'), Document(metadata={'page_source': '6', 'doc_source': '2312.10997v5', 'start_index': 4028, '_id': '2565106ae1b94e8790a0a6a3999e344d', '_collection_name': 'test'}, page_content='2) Retrieval Granularity:  Another important factor besides the data format of the retrieval source is the granularity of the retrieved data. Coarse-grained retrieval units theoretically can provide more relevant information for the problem, but they may also contain redundant content, which could distract the retriever and language models in downstream tasks [50], [87]. On the other hand, fine-grained retrieval unit granularity increases the burden of retrieval and does not guarantee seman- tic integrity and meeting the required knowledge. Choosing the appropriate retrieval granularity during inference can be a simple and effective strategy to improve the retrieval and downstream task performance of dense retrievers.'), Document(metadata={'page_source': '6', 'doc_source': '2312.10997v5', 'start_index': 4028, '_id': '604556e2867f4e2db1478f46d47ba548', '_collection_name': 'test'}, page_content='2) Retrieval Granularity:  Another important factor besides the data format of the retrieval source is the granularity of the retrieved data. Coarse-grained retrieval units theoretically can provide more relevant information for the problem, but they may also contain redundant content, which could distract the retriever and language models in downstream tasks [50], [87]. On the other hand, fine-grained retrieval unit granularity increases the burden of retrieval and does not guarantee seman- tic integrity and meeting the required knowledge. Choosing the appropriate retrieval granularity during inference can be a simple and effective strategy to improve the retrieval and downstream task performance of dense retrievers.'), Document(metadata={'page_source': '4', 'doc_source': '2312.10997v5', 'start_index': 4552, '_id': '9acc813acb44464780909277537170d4', '_collection_name': 'test'}, page_content='III. R ETRIEVAL \nIn the context of RAG, it is crucial to efficiently retrieve relevant documents from the data source. There are several key issues involved, such as the retrieval source, retrieval granularity, pre-processing of the retrieval, and selection of the corresponding embedding model. \nA. Retrieval Source \nRAG relies on external knowledge to enhance LLMs, while the type of retrieval source and the granularity of retrieval units both affect the final generation results. \n1) Data Structure:  Initially, text is s the mainstream source of retrieval. Subsequently, the retrieval source expanded to in- clude semi-structured data (PDF) and structured data (Knowl- edge Graph, KG) for enhancement. In addition to retrieving from original external sources, there is also a growing trend in recent researches towards utilizing content generated by LLMs themselves for retrieval and enhancement purposes.')]"
10015,"According to table II, which are the datasets that has exactly three methods?","[Document(metadata={'page_source': '5', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': '83c36e68ef3143cc97aee3e86894e146', '_collection_name': 'test'}, page_content=""The table lists various methods used in retrieval tasks, with details on their retrieval sources, data types, granularity, augmentation stage, and processes. Here's a breakdown of the columns:\n\n1. **Method**: Names of the retrieval methods.\n2. **Retrieval Source**: Sources from where data is retrieved (e.g., Wikipedia, Search Engine, Dataset-base).\n3. **Retrieval Data Type**: Type of data used for retrieval, such as Text or Knowledge Graph (KG).\n4. **Retrieval Granularity**: The level at which data is retrieved, e.g., Phrase, Sentence, Chunk, Item, Doc, Sub-Graph, etc.\n5. **Augmentation Stage**: The stage at which augmentation occurs, e.g., Pre-training, Tuning, Inference.\n6. **Retrieval Process**: The processes involved, such as Iterative or Once.\n\nThe table organizes different methods based on these characteristics to show a comparative view of various retrieval techniques.""), Document(metadata={'page_source': '5', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': '516cdf2f76004a15aa4f9c6eadc012eb', '_collection_name': 'test'}, page_content=""The table lists various methods used in retrieval tasks, with details on their retrieval sources, data types, granularity, augmentation stage, and processes. Here's a breakdown of the columns:\n\n1. **Method**: Names of the retrieval methods.\n2. **Retrieval Source**: Sources from where data is retrieved (e.g., Wikipedia, Search Engine, Dataset-base).\n3. **Retrieval Data Type**: Type of data used for retrieval, such as Text or Knowledge Graph (KG).\n4. **Retrieval Granularity**: The level at which data is retrieved, e.g., Phrase, Sentence, Chunk, Item, Doc, Sub-Graph, etc.\n5. **Augmentation Stage**: The stage at which augmentation occurs, e.g., Pre-training, Tuning, Inference.\n6. **Retrieval Process**: The processes involved, such as Iterative or Once.\n\nThe table organizes different methods based on these characteristics to show a comparative view of various retrieval techniques.""), Document(metadata={'page_source': '13', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': '01fbbf9fa3644714aa96f14b6d2fb597', '_collection_name': 'test'}, page_content='The table appears to categorize different evaluation metrics or criteria (listed in the first column) based on which aspects they assess (represented by the column headers from the second to the last column). Each checkmark (✓) indicates that the corresponding metric evaluates or is relevant to the corresponding aspect.\n\nHere is a breakdown of the contents:\n\n- **Context Relevance** is assessed by Accuracy, EM, Recall, Precision, Cosine Similarity, Hit Rate, MRR, and ROUGE/ROUGE-L.\n- **Faithfulness** is assessed by Accuracy, EM, BLEU, and ROUGE/ROUGE-L.\n- **Answer Relevance** is assessed by Accuracy, EM, and R-Rate.\n- **Noise Robustness** is assessed by Accuracy, Recall, and Precision.\n- **Negative Rejection** is assessed by Accuracy and EM.\n- **Information Integration** is assessed by Accuracy, MRR, and ROUGE/ROUGE-L.\n- **Counterfactual Robustness** is assessed by Accuracy and ROUGE/ROUGE-L.'), Document(metadata={'page_source': '13', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': 'fa516c5a30174a5d90cd3efbf73eaffa', '_collection_name': 'test'}, page_content='The table appears to categorize different evaluation metrics or criteria (listed in the first column) based on which aspects they assess (represented by the column headers from the second to the last column). Each checkmark (✓) indicates that the corresponding metric evaluates or is relevant to the corresponding aspect.\n\nHere is a breakdown of the contents:\n\n- **Context Relevance** is assessed by Accuracy, EM, Recall, Precision, Cosine Similarity, Hit Rate, MRR, and ROUGE/ROUGE-L.\n- **Faithfulness** is assessed by Accuracy, EM, BLEU, and ROUGE/ROUGE-L.\n- **Answer Relevance** is assessed by Accuracy, EM, and R-Rate.\n- **Noise Robustness** is assessed by Accuracy, Recall, and Precision.\n- **Negative Rejection** is assessed by Accuracy and EM.\n- **Information Integration** is assessed by Accuracy, MRR, and ROUGE/ROUGE-L.\n- **Counterfactual Robustness** is assessed by Accuracy and ROUGE/ROUGE-L.'), Document(metadata={'page_source': '6', 'doc_source': '2312.10997v5', 'start_index': 851, '_id': 'e34eec6e1d1f4936978151089763a43b', '_collection_name': 'test'}, page_content='The diagram visually represents these methods as positioned on the axes of ""External Knowledge Required"" vs. ""Model Adaptation Required"".\nUnstructured Data , such as text, is the most widely used retrieval source, which are mainly gathered from corpus. For open-domain question-answering (ODQA) tasks, the primary retrieval sources are Wikipedia Dump with the current major versions including HotpotQA   4   (1st October , 2017), DPR 5   (20 December, 2018). In addition to encyclopedic data, common unstructured data includes cross-lingual text [19] and domain- specific data (such as medical [67]and legal domains [29]).'), Document(metadata={'page_source': '6', 'doc_source': '2312.10997v5', 'start_index': 851, '_id': 'cdd75ef7c9ca449db4fabc7a8e7f411c', '_collection_name': 'test'}, page_content='The diagram visually represents these methods as positioned on the axes of ""External Knowledge Required"" vs. ""Model Adaptation Required"".\nUnstructured Data , such as text, is the most widely used retrieval source, which are mainly gathered from corpus. For open-domain question-answering (ODQA) tasks, the primary retrieval sources are Wikipedia Dump with the current major versions including HotpotQA   4   (1st October , 2017), DPR 5   (20 December, 2018). In addition to encyclopedic data, common unstructured data includes cross-lingual text [19] and domain- specific data (such as medical [67]and legal domains [29]).'), Document(metadata={'page_source': '12', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': 'a1715cd70a5144b3a9735a8f0f2aaf62', '_collection_name': 'test'}, page_content='The table categorizes various tasks related to QA (Question Answering), dialog, information extraction (IE), reasoning, and other NLP applications. Each category is further divided into sub-tasks, provided with associated datasets, and lists methods (with citation numbers) used to perform these tasks. For example:\n\n1. **QA (Question Answering):**\n   - **Single-hop:** Uses datasets like Natural Question (NQ), TriviaQA (TQA), SQuAD, and others. Methods are indicated by citation numbers.\n   - **Multi-hop:** Datasets like HotpotQA and 2WikiMultiHopQA are listed along with the methods.\n   - **Long-form QA:** Includes datasets like ELI5 and NarrativeQA with relevant methods.\n\n2. **Dialog:**\n   - Tasks include dialog generation and personal dialog, with datasets like Wizard of Wikipedia (WoW) and KBP.\n\n3. **Information Extraction (IE):**\n   - Sub-tasks include event argument extraction and relation extraction, using datasets like WikiEvent and T-REx.'), Document(metadata={'page_source': '12', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': 'ca77e883fbe84ef2b02ca80a25663d11', '_collection_name': 'test'}, page_content='The table categorizes various tasks related to QA (Question Answering), dialog, information extraction (IE), reasoning, and other NLP applications. Each category is further divided into sub-tasks, provided with associated datasets, and lists methods (with citation numbers) used to perform these tasks. For example:\n\n1. **QA (Question Answering):**\n   - **Single-hop:** Uses datasets like Natural Question (NQ), TriviaQA (TQA), SQuAD, and others. Methods are indicated by citation numbers.\n   - **Multi-hop:** Datasets like HotpotQA and 2WikiMultiHopQA are listed along with the methods.\n   - **Long-form QA:** Includes datasets like ELI5 and NarrativeQA with relevant methods.\n\n2. **Dialog:**\n   - Tasks include dialog generation and personal dialog, with datasets like Wizard of Wikipedia (WoW) and KBP.\n\n3. **Information Extraction (IE):**\n   - Sub-tasks include event argument extraction and relation extraction, using datasets like WikiEvent and T-REx.'), Document(metadata={'page_source': '3', 'doc_source': '2312.10997v5', 'start_index': 627, '_id': 'a340e07d645a4fffabd7121175e02817', '_collection_name': 'test'}, page_content='3. **Modular RAG** (Right):\n   - Enhances flexibility by introducing various specific functional modules.\n   - Includes iterative and adaptive processes like routing, searching, retrieving, reading, predicting, rewriting, reranking, demonstrating, and fusion.\n   - Facilitates non-sequential methods such as iterative retrieval and adaptive approaches, allowing for more dynamic interactions.'), Document(metadata={'page_source': '3', 'doc_source': '2312.10997v5', 'start_index': 627, '_id': '1f38c0d088214eb586d7f9d6c2bb79ae', '_collection_name': 'test'}, page_content='3. **Modular RAG** (Right):\n   - Enhances flexibility by introducing various specific functional modules.\n   - Includes iterative and adaptive processes like routing, searching, retrieving, reading, predicting, rewriting, reranking, demonstrating, and fusion.\n   - Facilitates non-sequential methods such as iterative retrieval and adaptive approaches, allowing for more dynamic interactions.'), Document(metadata={'page_source': '3', 'doc_source': '2312.10997v5', 'start_index': 1022, '_id': '5792dd03b40e4bd0a2f9a1fafb0b7565', '_collection_name': 'test'}, page_content='The image visually represents these paradigms to illustrate their structures and components.\nPre-retrieval process . In this stage, the primary focus is on optimizing the indexing structure and the original query. The goal of optimizing indexing is to enhance the quality of the content being indexed. This involves strategies: enhancing data granularity, optimizing index structures, adding metadata, alignment optimization, and mixed retrieval. While the goal of query optimization is to make the user’s original question clearer and more suitable for the retrieval task. Common methods include query rewriting query transformation, query expansion and other techniques [7], [9]–[11].'), Document(metadata={'page_source': '3', 'doc_source': '2312.10997v5', 'start_index': 1022, '_id': 'dfd72e14b8174f3fae222e66a0b3d2ba', '_collection_name': 'test'}, page_content='The image visually represents these paradigms to illustrate their structures and components.\nPre-retrieval process . In this stage, the primary focus is on optimizing the indexing structure and the original query. The goal of optimizing indexing is to enhance the quality of the content being indexed. This involves strategies: enhancing data granularity, optimizing index structures, adding metadata, alignment optimization, and mixed retrieval. While the goal of query optimization is to make the user’s original question clearer and more suitable for the retrieval task. Common methods include query rewriting query transformation, query expansion and other techniques [7], [9]–[11].'), Document(metadata={'page_source': '14', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': '5ded92c7dea6435098030dc2d96dac30', '_collection_name': 'test'}, page_content=""The table provides an overview of different evaluation frameworks, their targets, aspects, and quantitative metrics used in evaluation. Here's the breakdown:\n\n1. **RGB†**\n   - **Targets**: Retrieval Quality, Generation Quality\n   - **Aspects**: Noise Robustness, Negative Rejection, Information Integration, Counterfactual Robustness\n   - **Metrics**: Accuracy, EM (Exact Match), Accuracy, Accuracy\n\n2. **RECALL†**\n   - **Targets**: Generation Quality\n   - **Aspects**: Counterfactual Robustness\n   - **Metrics**: R-Rate (Reappearance Rate)\n\n3. **RAGAS‡**\n   - **Targets**: Retrieval Quality, Generation Quality\n   - **Aspects**: Context Relevance, Faithfulness, Answer Relevance\n   - **Metrics**: *, *, Cosine Similarity\n\n4. **ARES‡**\n   - **Targets**: Retrieval Quality, Generation Quality\n   - **Aspects**: Context Relevance, Faithfulness, Answer Relevance\n   - **Metrics**: Accuracy, Accuracy, Accuracy""), Document(metadata={'page_source': '14', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': 'a7aa6b752676471ca53fc28edebe36ff', '_collection_name': 'test'}, page_content=""The table provides an overview of different evaluation frameworks, their targets, aspects, and quantitative metrics used in evaluation. Here's the breakdown:\n\n1. **RGB†**\n   - **Targets**: Retrieval Quality, Generation Quality\n   - **Aspects**: Noise Robustness, Negative Rejection, Information Integration, Counterfactual Robustness\n   - **Metrics**: Accuracy, EM (Exact Match), Accuracy, Accuracy\n\n2. **RECALL†**\n   - **Targets**: Generation Quality\n   - **Aspects**: Counterfactual Robustness\n   - **Metrics**: R-Rate (Reappearance Rate)\n\n3. **RAGAS‡**\n   - **Targets**: Retrieval Quality, Generation Quality\n   - **Aspects**: Context Relevance, Faithfulness, Answer Relevance\n   - **Metrics**: *, *, Cosine Similarity\n\n4. **ARES‡**\n   - **Targets**: Retrieval Quality, Generation Quality\n   - **Aspects**: Context Relevance, Faithfulness, Answer Relevance\n   - **Metrics**: Accuracy, Accuracy, Accuracy""), Document(metadata={'page_source': '1', 'doc_source': '2312.10997v5', 'start_index': 3214, '_id': '50bd1b4905fc4c779a3728993c395981', '_collection_name': 'test'}, page_content='A. Naive RAG \nThe Naive RAG research paradigm represents the earli- est methodology, which gained prominence shortly after the')]"
10016,"Which subsection does the section ""AUGMENTATION PROCESS IN RAG"" include?","[Document(metadata={'page_source': '0', 'doc_source': '2312.10997v5', 'start_index': 3596, '_id': '74aadb6df7304fbb8df6ed01bfea1801', '_collection_name': 'test'}, page_content='during the inference stage, leading to rapid development in RAG studies. As research progressed, the enhancement of RAG was no longer limited to the inference stage but began to incorporate more with LLM fine-tuning techniques.'), Document(metadata={'page_source': '0', 'doc_source': '2312.10997v5', 'start_index': 3596, '_id': '536d2b14b004494a8c53b2cb80041c25', '_collection_name': 'test'}, page_content='during the inference stage, leading to rapid development in RAG studies. As research progressed, the enhancement of RAG was no longer limited to the inference stage but began to incorporate more with LLM fine-tuning techniques.'), Document(metadata={'page_source': '3', 'doc_source': '2312.10997v5', 'start_index': 627, '_id': 'a340e07d645a4fffabd7121175e02817', '_collection_name': 'test'}, page_content='3. **Modular RAG** (Right):\n   - Enhances flexibility by introducing various specific functional modules.\n   - Includes iterative and adaptive processes like routing, searching, retrieving, reading, predicting, rewriting, reranking, demonstrating, and fusion.\n   - Facilitates non-sequential methods such as iterative retrieval and adaptive approaches, allowing for more dynamic interactions.'), Document(metadata={'page_source': '3', 'doc_source': '2312.10997v5', 'start_index': 627, '_id': '1f38c0d088214eb586d7f9d6c2bb79ae', '_collection_name': 'test'}, page_content='3. **Modular RAG** (Right):\n   - Enhances flexibility by introducing various specific functional modules.\n   - Includes iterative and adaptive processes like routing, searching, retrieving, reading, predicting, rewriting, reranking, demonstrating, and fusion.\n   - Facilitates non-sequential methods such as iterative retrieval and adaptive approaches, allowing for more dynamic interactions.'), Document(metadata={'page_source': '9', 'doc_source': '2312.10997v5', 'start_index': 1316, '_id': 'aed5641a4d324ffeb2c527d48de3544a', '_collection_name': 'test'}, page_content='2) Context Selection/Compression:  A common misconcep- tion in the RAG process is the belief that retrieving as many relevant documents as possible and concatenating them to form a lengthy retrieval prompt is beneficial. However, excessive context can introduce more noise, diminishing the LLM’s perception of key information .'), Document(metadata={'page_source': '9', 'doc_source': '2312.10997v5', 'start_index': 1316, '_id': '707695b0fccc43ea9b2c7a28e7a86119', '_collection_name': 'test'}, page_content='2) Context Selection/Compression:  A common misconcep- tion in the RAG process is the belief that retrieving as many relevant documents as possible and concatenating them to form a lengthy retrieval prompt is beneficial. However, excessive context can introduce more noise, diminishing the LLM’s perception of key information .'), Document(metadata={'page_source': '0', 'doc_source': '2312.10997v5', 'start_index': 1312, '_id': '374edf1534794e47862fcff8784dbf48', '_collection_name': 'test'}, page_content='RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the- art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evalua- tion framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development   1 .'), Document(metadata={'page_source': '0', 'doc_source': '2312.10997v5', 'start_index': 1312, '_id': '2d6a912ffb084708b26194e3c77b2fa3', '_collection_name': 'test'}, page_content='RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the- art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evalua- tion framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development   1 .'), Document(metadata={'page_source': '2', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': '8ec4223e41694a358adcddaee861ccf7', '_collection_name': 'test'}, page_content='The image illustrates the RAG (Retrieval-Augmented Generation) process used in question answering systems. It consists of three main steps:\n\n1. **Indexing**: Documents are divided into chunks, encoded into vectors, and stored in a vector database.\n2. **Retrieval**: The system retrieves the top K chunks most relevant to the query based on semantic similarity.\n3. **Generation**: Combines the original question and retrieved chunks as inputs into a large language model (LLM) to generate the final answer.'), Document(metadata={'page_source': '2', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': '7a9a8521ed07430ba94de2cb98bd7e2c', '_collection_name': 'test'}, page_content='The image illustrates the RAG (Retrieval-Augmented Generation) process used in question answering systems. It consists of three main steps:\n\n1. **Indexing**: Documents are divided into chunks, encoded into vectors, and stored in a vector database.\n2. **Retrieval**: The system retrieves the top K chunks most relevant to the query based on semantic similarity.\n3. **Generation**: Combines the original question and retrieved chunks as inputs into a large language model (LLM) to generate the final answer.'), Document(metadata={'page_source': '13', 'doc_source': '2312.10997v5', 'start_index': 4716, '_id': '016a888806e448ba86adc1fa56c1e853', '_collection_name': 'test'}, page_content='C. Hybrid Approaches \nCombining RAG with fine-tuning is emerging as a leading strategy. Determining the optimal integration of RAG and fine-tuning whether sequential, alternating, or through end-to- end joint training—and how to harness both parameterized'), Document(metadata={'page_source': '13', 'doc_source': '2312.10997v5', 'start_index': 4716, '_id': 'f36f443089ab4fc19f8fd3b110450332', '_collection_name': 'test'}, page_content='C. Hybrid Approaches \nCombining RAG with fine-tuning is emerging as a leading strategy. Determining the optimal integration of RAG and fine-tuning whether sequential, alternating, or through end-to- end joint training—and how to harness both parameterized'), Document(metadata={'page_source': '2', 'doc_source': '2312.10997v5', 'start_index': 2973, '_id': '7638a50adfeb49eaa71b6f44c6bc1a9b', '_collection_name': 'test'}, page_content='Augmentation Hurdles . Integrating retrieved information with the different task can be challenging, sometimes resulting in disjointed or incoherent outputs. The process may also encounter redundancy when similar information is retrieved from multiple sources, leading to repetitive responses. Deter- mining the significance and relevance of various passages and ensuring stylistic and tonal consistency add further complexity. Facing complex issues, a single retrieval based on the original query may not suffice to acquire adequate context information. \nMoreover, there’s a concern that generation models might overly rely on augmented information, leading to outputs that simply echo retrieved content without adding insightful or synthesized information. \nB. Advanced RAG'), Document(metadata={'page_source': '2', 'doc_source': '2312.10997v5', 'start_index': 2973, '_id': '87e884dc25484ce0ab4a254eef3ba4a0', '_collection_name': 'test'}, page_content='Augmentation Hurdles . Integrating retrieved information with the different task can be challenging, sometimes resulting in disjointed or incoherent outputs. The process may also encounter redundancy when similar information is retrieved from multiple sources, leading to repetitive responses. Deter- mining the significance and relevance of various passages and ensuring stylistic and tonal consistency add further complexity. Facing complex issues, a single retrieval based on the original query may not suffice to acquire adequate context information. \nMoreover, there’s a concern that generation models might overly rely on augmented information, leading to outputs that simply echo retrieved content without adding insightful or synthesized information. \nB. Advanced RAG'), Document(metadata={'page_source': '6', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': 'e16480d4c068490b8565bb6ee8fee374', '_collection_name': 'test'}, page_content='The image is a diagram comparing different model optimization methods in terms of ""External Knowledge Required"" and ""Model Adaptation Required"". \n\nKey elements:\n\n1. **RAG (Retrieval-Augmented Generation)**: Shown as evolving from Naive RAG to Advanced and Modular RAG.\n   - **Naive RAG**: Involves adding contextual paragraphs with low model modifications.\n   - **Advanced RAG**: Includes index and retrieval optimizations.\n   - **Modular RAG**: Combines multiple modules organically.\n\n2. **Prompt Engineering**: \n   - Requires low modifications and external knowledge, using the capabilities of large language models.\n   - Includes Standard Prompt, Few-shot Prompt, and XoT Prompt (e.g., CoT, ToT).\n\n3. **Fine-tuning**: \n   - Requires high model adaptation.\n   - Includes Retriever Fine-tuning, Collaborative Fine-tuning, and Generator Fine-tuning.')]"
10017,Which method integrates knowledge into white-box models via directive fine-tuning?,"[Document(metadata={'page_source': '8', 'doc_source': '2312.10997v5', 'start_index': 6257, '_id': 'af5061316bfd45689b09dd59bb475bf4', '_collection_name': 'test'}, page_content='Furthermore, PKG introduces an innovative method for integrating knowledge into white-box models via directive fine-tuning [75]. In this approach, the retriever module is directly substituted to gen- erate relevant documents according to a query. This method assists in addressing the difficulties encountered during the fine-tuning process and enhances model performance.'), Document(metadata={'page_source': '8', 'doc_source': '2312.10997v5', 'start_index': 6257, '_id': 'fb9c056ec45d4e228c7d1ae3f637a36d', '_collection_name': 'test'}, page_content='Furthermore, PKG introduces an innovative method for integrating knowledge into white-box models via directive fine-tuning [75]. In this approach, the retriever module is directly substituted to gen- erate relevant documents according to a query. This method assists in addressing the difficulties encountered during the fine-tuning process and enhances model performance.'), Document(metadata={'page_source': '4', 'doc_source': '2312.10997v5', 'start_index': 2212, '_id': '4392401b121e4fb982f1ff21c1c80566', '_collection_name': 'test'}, page_content='D. RAG vs Fine-tuning \nThe augmentation of LLMs has attracted considerable atten- tion due to their growing prevalence. Among the optimization methods for LLMs, RAG is often compared with Fine-tuning (FT) and prompt engineering. Each method has distinct charac- teristics as illustrated in Figure 4. We used a quadrant chart to illustrate the differences among three methods in two dimen- sions: external knowledge requirements and model adaption requirements. Prompt engineering leverages a model’s inherent capabilities with minimum necessity for external knowledge and model adaption. RAG can be likened to providing a model with a tailored textbook for information retrieval, ideal for pre- cise information retrieval tasks. In contrast, FT is comparable to a student internalizing knowledge over time, suitable for scenarios requiring replication of specific structures, styles, or formats.'), Document(metadata={'page_source': '4', 'doc_source': '2312.10997v5', 'start_index': 2212, '_id': 'a833281365664752b77fe9a94481c457', '_collection_name': 'test'}, page_content='D. RAG vs Fine-tuning \nThe augmentation of LLMs has attracted considerable atten- tion due to their growing prevalence. Among the optimization methods for LLMs, RAG is often compared with Fine-tuning (FT) and prompt engineering. Each method has distinct charac- teristics as illustrated in Figure 4. We used a quadrant chart to illustrate the differences among three methods in two dimen- sions: external knowledge requirements and model adaption requirements. Prompt engineering leverages a model’s inherent capabilities with minimum necessity for external knowledge and model adaption. RAG can be likened to providing a model with a tailored textbook for information retrieval, ideal for pre- cise information retrieval tasks. In contrast, FT is comparable to a student internalizing knowledge over time, suitable for scenarios requiring replication of specific structures, styles, or formats.'), Document(metadata={'page_source': '4', 'doc_source': '2312.10997v5', 'start_index': 3673, '_id': '02ce121071164369a95238b74acfa175', '_collection_name': 'test'}, page_content='In multiple evaluations of their performance on various knowledge-intensive tasks across different topics, [28] re- vealed that while unsupervised fine-tuning shows some im- provement, RAG consistently outperforms it, for both exist- ing knowledge encountered during training and entirely new knowledge. Additionally, it was found that LLMs struggle to learn new factual information through unsupervised fine- tuning. The choice between RAG and FT depends on the specific needs for data dynamics, customization, and com- putational capabilities in the application context. RAG and FT are not mutually exclusive and can complement each other, enhancing a model’s capabilities at different levels. In some instances, their combined use may lead to optimal performance. The optimization process involving RAG and FT may require multiple iterations to achieve satisfactory results. \nIII. R ETRIEVAL'), Document(metadata={'page_source': '4', 'doc_source': '2312.10997v5', 'start_index': 3673, '_id': '5dfd954f791a49bbb783517a5b4c87c8', '_collection_name': 'test'}, page_content='In multiple evaluations of their performance on various knowledge-intensive tasks across different topics, [28] re- vealed that while unsupervised fine-tuning shows some im- provement, RAG consistently outperforms it, for both exist- ing knowledge encountered during training and entirely new knowledge. Additionally, it was found that LLMs struggle to learn new factual information through unsupervised fine- tuning. The choice between RAG and FT depends on the specific needs for data dynamics, customization, and com- putational capabilities in the application context. RAG and FT are not mutually exclusive and can complement each other, enhancing a model’s capabilities at different levels. In some instances, their combined use may lead to optimal performance. The optimization process involving RAG and FT may require multiple iterations to achieve satisfactory results. \nIII. R ETRIEVAL'), Document(metadata={'page_source': '6', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': 'e16480d4c068490b8565bb6ee8fee374', '_collection_name': 'test'}, page_content='The image is a diagram comparing different model optimization methods in terms of ""External Knowledge Required"" and ""Model Adaptation Required"". \n\nKey elements:\n\n1. **RAG (Retrieval-Augmented Generation)**: Shown as evolving from Naive RAG to Advanced and Modular RAG.\n   - **Naive RAG**: Involves adding contextual paragraphs with low model modifications.\n   - **Advanced RAG**: Includes index and retrieval optimizations.\n   - **Modular RAG**: Combines multiple modules organically.\n\n2. **Prompt Engineering**: \n   - Requires low modifications and external knowledge, using the capabilities of large language models.\n   - Includes Standard Prompt, Few-shot Prompt, and XoT Prompt (e.g., CoT, ToT).\n\n3. **Fine-tuning**: \n   - Requires high model adaptation.\n   - Includes Retriever Fine-tuning, Collaborative Fine-tuning, and Generator Fine-tuning.'), Document(metadata={'page_source': '6', 'doc_source': '2312.10997v5', 'start_index': 0, '_id': '7c2b6b098b4e4d299c8f889a0c917ca2', '_collection_name': 'test'}, page_content='The image is a diagram comparing different model optimization methods in terms of ""External Knowledge Required"" and ""Model Adaptation Required"". \n\nKey elements:\n\n1. **RAG (Retrieval-Augmented Generation)**: Shown as evolving from Naive RAG to Advanced and Modular RAG.\n   - **Naive RAG**: Involves adding contextual paragraphs with low model modifications.\n   - **Advanced RAG**: Includes index and retrieval optimizations.\n   - **Modular RAG**: Combines multiple modules organically.\n\n2. **Prompt Engineering**: \n   - Requires low modifications and external knowledge, using the capabilities of large language models.\n   - Includes Standard Prompt, Few-shot Prompt, and XoT Prompt (e.g., CoT, ToT).\n\n3. **Fine-tuning**: \n   - Requires high model adaptation.\n   - Includes Retriever Fine-tuning, Collaborative Fine-tuning, and Generator Fine-tuning.'), Document(metadata={'page_source': '0', 'doc_source': '2312.10997v5', 'start_index': 4627, '_id': '35049f2de7094b8498f920d17294ff35', '_collection_name': 'test'}, page_content='methods applicable to RAG. Overall, this paper sets out to meticulously compile and categorize the foundational technical concepts, historical progression, and the spectrum of RAG methodologies and applications that have emerged post-LLMs. It is designed to equip readers and professionals with a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and speculate on upcoming trends and innovations.'), Document(metadata={'page_source': '0', 'doc_source': '2312.10997v5', 'start_index': 4627, '_id': '7617bb62857244639627246f05fd67e5', '_collection_name': 'test'}, page_content='methods applicable to RAG. Overall, this paper sets out to meticulously compile and categorize the foundational technical concepts, historical progression, and the spectrum of RAG methodologies and applications that have emerged post-LLMs. It is designed to equip readers and professionals with a detailed and structured understanding of both large models and RAG. It aims to illuminate the evolution of retrieval augmentation techniques, assess the strengths and weaknesses of various approaches in their respective contexts, and speculate on upcoming trends and innovations.'), Document(metadata={'page_source': '8', 'doc_source': '2312.10997v5', 'start_index': 5176, '_id': '376780bc6e7a419295c97faeac87d04d', '_collection_name': 'test'}, page_content='E. Adapter \nFine-tuning models may present challenges, such as in- tegrating functionality through an API or addressing con- straints arising from limited local computational resources. Consequently, some approaches opt to incorporate an external adapter to aid in alignment.'), Document(metadata={'page_source': '8', 'doc_source': '2312.10997v5', 'start_index': 5176, '_id': '3fce95f8c0204ec699506c1bd495379a', '_collection_name': 'test'}, page_content='E. Adapter \nFine-tuning models may present challenges, such as in- tegrating functionality through an API or addressing con- straints arising from limited local computational resources. Consequently, some approaches opt to incorporate an external adapter to aid in alignment.'), Document(metadata={'page_source': '4', 'doc_source': '2312.10997v5', 'start_index': 1, '_id': '9ed85e7a10aa443083dbfe113e8e86da', '_collection_name': 'test'}, page_content='2) New Patterns:  Modular RAG offers remarkable adapt- ability by allowing module substitution or reconfiguration to address specific challenges. This goes beyond the fixed structures of Naive and Advanced RAG, characterized by a simple “Retrieve” and “Read” mechanism. Moreover, Modular RAG expands this flexibility by integrating new modules or adjusting interaction flow among existing ones, enhancing its applicability across different tasks.'), Document(metadata={'page_source': '4', 'doc_source': '2312.10997v5', 'start_index': 1, '_id': '701861892f3b477bbfabee0a566bb07d', '_collection_name': 'test'}, page_content='2) New Patterns:  Modular RAG offers remarkable adapt- ability by allowing module substitution or reconfiguration to address specific challenges. This goes beyond the fixed structures of Naive and Advanced RAG, characterized by a simple “Retrieve” and “Read” mechanism. Moreover, Modular RAG expands this flexibility by integrating new modules or adjusting interaction flow among existing ones, enhancing its applicability across different tasks.'), Document(metadata={'page_source': '14', 'doc_source': '2312.10997v5', 'start_index': 2760, '_id': 'a967851368fe48e5b63bcafc7cd28970', '_collection_name': 'test'}, page_content='The development of the RAG ecosystem is greatly impacted by the progression of its technical stack. Key tools like LangChain and LLamaIndex have quickly gained popularity with the emergence of ChatGPT, providing extensive RAG- related APIs and becoming essential in the realm of LLMs.The emerging technology stack, while not as rich in features as LangChain and LLamaIndex, stands out through its specialized products. For example, Flowise AI prioritizes a low-code approach, allowing users to deploy AI applications, including RAG, through a user-friendly drag-and-drop interface. Other technologies like HayStack, Meltano, and Cohere Coral are also gaining attention for their unique contributions to the field.')]"
10018,What degree does LEBOUR have?,"[Document(metadata={'page_source': '15', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 2811, '_id': '82667cb952b34aadbad2c06025dac282', '_collection_name': 'test'}, page_content=""Mr. Parrington. There was no reason, without giving up an inch of his own theory, why he should not adopt Mr. Parrington's. If mining engineers said that the lowering of the level of the water by the Water Company or others pumping was liable to make thewater-blasts,he(Professor Lebour) was willing to accept that; and that might account for the great noises heard in connection with tl he earth-shakes in Sunderland.But this did not in the slightest degree militate against the explanation he had brought fur ward.""), Document(metadata={'page_source': '15', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 2811, '_id': '99c026747ed843ecafaf98502bd21be0', '_collection_name': 'test'}, page_content=""Mr. Parrington. There was no reason, without giving up an inch of his own theory, why he should not adopt Mr. Parrington's. If mining engineers said that the lowering of the level of the water by the Water Company or others pumping was liable to make thewater-blasts,he(Professor Lebour) was willing to accept that; and that might account for the great noises heard in connection with tl he earth-shakes in Sunderland.But this did not in the slightest degree militate against the explanation he had brought fur ward.""), Document(metadata={'page_source': '0', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 165, '_id': '41163f91c0a54f5ca1660bb1e9f40408', '_collection_name': 'test'}, page_content='THE town of Sunderland is built upon the Permian Magnesian Lime- stone. The latter (with its subordinate marl-slate) rests, the very irregular and sometimes absent yellow sands of the Permian alone inter- vening, upon a denuded surface of Coal-Measures. The total thickness of the Magnesian Limestone at its maximum may be estimated as being not much over 60o feet,but at Sunderland the uppermost portion of the deposit is absent.The amount of that rock present there is between 300 and 400 feet. There is very little southerly dip in the limestone, but there is some, and the best way to ascertain the nature of those portions which underlie the town is therefore to study the beds as they crop out in the beautiful cliff sections to the north, between the Wear and the Tyne. These rocks are so strange in structure, and so striking by reason of the variety of their forms, that they have been described in many raluable papers, by the late Professor Sedgwick, Mr. R. Howse, Mr. J. W. Kirkby, and'), Document(metadata={'page_source': '0', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 165, '_id': 'df783b4fee3344dfa3ac89bdb44a307f', '_collection_name': 'test'}, page_content='THE town of Sunderland is built upon the Permian Magnesian Lime- stone. The latter (with its subordinate marl-slate) rests, the very irregular and sometimes absent yellow sands of the Permian alone inter- vening, upon a denuded surface of Coal-Measures. The total thickness of the Magnesian Limestone at its maximum may be estimated as being not much over 60o feet,but at Sunderland the uppermost portion of the deposit is absent.The amount of that rock present there is between 300 and 400 feet. There is very little southerly dip in the limestone, but there is some, and the best way to ascertain the nature of those portions which underlie the town is therefore to study the beds as they crop out in the beautiful cliff sections to the north, between the Wear and the Tyne. These rocks are so strange in structure, and so striking by reason of the variety of their forms, that they have been described in many raluable papers, by the late Professor Sedgwick, Mr. R. Howse, Mr. J. W. Kirkby, and'), Document(metadata={'page_source': '8', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 1666, '_id': 'b716aa0ff71f4e0fb91cddffa0bcd1a1', '_collection_name': 'test'}, page_content='Sir Charles Lyell, in a letter to Leonard Horner, dated September 1st, 1838,thus refers to the coast at Marsden which he had just visited for the first time:""The coast scenery was very grand,and the breccia ted \n\\* There is also some less definite evidence of shocks of much the\'same character having taken place in or near Sunderland about eleven years ago. Information as to any occurrences of thiskind prior to 1883 would bevery thankfully received by the writer. 十“ Geology of Northumberland and Durham,"" Trans. Geol. Soc., Vol. IV., p. 6, 1816. \n↑“On the Geological Relations and Internal Structure of the Magnesian Lime stone,Trans,Geol,Soc.,Series2,Vol.11I.,p.92(1835)'), Document(metadata={'page_source': '8', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 1666, '_id': 'b48b068280314204a91bd38b85f96ddf', '_collection_name': 'test'}, page_content='Sir Charles Lyell, in a letter to Leonard Horner, dated September 1st, 1838,thus refers to the coast at Marsden which he had just visited for the first time:""The coast scenery was very grand,and the breccia ted \n\\* There is also some less definite evidence of shocks of much the\'same character having taken place in or near Sunderland about eleven years ago. Information as to any occurrences of thiskind prior to 1883 would bevery thankfully received by the writer. 十“ Geology of Northumberland and Durham,"" Trans. Geol. Soc., Vol. IV., p. 6, 1816. \n↑“On the Geological Relations and Internal Structure of the Magnesian Lime stone,Trans,Geol,Soc.,Series2,Vol.11I.,p.92(1835)'), Document(metadata={'page_source': '16', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 978, '_id': 'b7d2398053274a4ea81d38e695de863b', '_collection_name': 'test'}, page_content='The PrEsIDENT said, the paper would be discussed at a future meet- ing. He proposed a vote of thanks to Professor Iebour for the interesting information he had given the members in the paper. This was a subject in which Mr. Daglish and himself took considerable interest twenty years ago ; but their experience was now old,and perhaps was superseded by the information of thepresent time.It was important that information on this subject should be gathered, and embodied in the Transactions of the Institute. \nThe vote of thanks was agreed to \nMr. M. WALTon BRowN read the following paper “On the Observation of Earth-shakes or Tremors, in order to foretell the issue of sudden Out- bursts of Fire-damp :""-'), Document(metadata={'page_source': '16', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 978, '_id': 'e9c3516734764fb59f2a34029dd7e753', '_collection_name': 'test'}, page_content='The PrEsIDENT said, the paper would be discussed at a future meet- ing. He proposed a vote of thanks to Professor Iebour for the interesting information he had given the members in the paper. This was a subject in which Mr. Daglish and himself took considerable interest twenty years ago ; but their experience was now old,and perhaps was superseded by the information of thepresent time.It was important that information on this subject should be gathered, and embodied in the Transactions of the Institute. \nThe vote of thanks was agreed to \nMr. M. WALTon BRowN read the following paper “On the Observation of Earth-shakes or Tremors, in order to foretell the issue of sudden Out- bursts of Fire-damp :""-'), Document(metadata={'page_source': '6', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 961, '_id': 'd1ccbc2fd4d544a19a4ac36f95902dc2', '_collection_name': 'test'}, page_content='Road neighbourhood of Sunder- land,and,it would appear,to certain linear directions within that district. For some months the writer has been kept informed of the successive “ shocks"" through the kindness of several gentlemcn, among whom Professor G. S. Brady, F.R.S., Mr. J. B. Atkinson, Mr. W. S. Harrison, B.A., A.Sc., Mr. C. L. Cummings, and Mr. G. Shaw must be specially mentioned.The results of the information thus gathered from various and independent quarters arebriefy asfollows:'), Document(metadata={'page_source': '6', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 961, '_id': '45c7563492cd44dc9de4849554d3d4af', '_collection_name': 'test'}, page_content='Road neighbourhood of Sunder- land,and,it would appear,to certain linear directions within that district. For some months the writer has been kept informed of the successive “ shocks"" through the kindness of several gentlemcn, among whom Professor G. S. Brady, F.R.S., Mr. J. B. Atkinson, Mr. W. S. Harrison, B.A., A.Sc., Mr. C. L. Cummings, and Mr. G. Shaw must be specially mentioned.The results of the information thus gathered from various and independent quarters arebriefy asfollows:'), Document(metadata={'page_source': '16', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 1, '_id': '13ff87c42a9d42cfa1fe91dcd89f7f24', '_collection_name': 'test'}, page_content=""Mr. PARRINGToN said, he saw at page 58 in De Rance's “ Water Supply ofEngland and Wales,”that noless than 5,0oo,0i0 gallons a-day were pumped from the magnesia n limestone without in the least altering the permanent level of the water in the district. He (Mr. Parrington) quite agreed that the permanent level of thewater at Sunderland was not altered;but he would ask Mr.Forster if towards the outcrop,thelevel of this water was not permanently lowered? \nMr. R. ForsTER said, he understood that this paper would come up for discussion at a future meeting, and he proposed to answer Mr. Parring ton's question by putting in the diagram to which he had already alluded. He wished to ask, however, if the head of water were lowered, would not that have a tendency to cause, in the underground river or lake, the fow of water to bc more rapid, and so, taking away the foundation of these Breccia-gashes, cause the falls to be more frequent, and carry off more limestone with it?""), Document(metadata={'page_source': '16', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 1, '_id': '1d02e72537e94e4da4c96aa0b51c47e9', '_collection_name': 'test'}, page_content=""Mr. PARRINGToN said, he saw at page 58 in De Rance's “ Water Supply ofEngland and Wales,”that noless than 5,0oo,0i0 gallons a-day were pumped from the magnesia n limestone without in the least altering the permanent level of the water in the district. He (Mr. Parrington) quite agreed that the permanent level of thewater at Sunderland was not altered;but he would ask Mr.Forster if towards the outcrop,thelevel of this water was not permanently lowered? \nMr. R. ForsTER said, he understood that this paper would come up for discussion at a future meeting, and he proposed to answer Mr. Parring ton's question by putting in the diagram to which he had already alluded. He wished to ask, however, if the head of water were lowered, would not that have a tendency to cause, in the underground river or lake, the fow of water to bc more rapid, and so, taking away the foundation of these Breccia-gashes, cause the falls to be more frequent, and carry off more limestone with it?""), Document(metadata={'page_source': '2', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 0, '_id': 'afd8cc791c2c498f963bbc9314a5dea7', '_collection_name': 'test'}, page_content='To illustrate Prt. Lcbour\'s paper ""On the Breccia Gushes of Durham? \nThe image appears to be a drawing or illustration depicting a stone wall with a section in the middle showing rubble or irregularly shaped stones. The caption text mentions ""FiG. I,"" indicating that this might be Figure 1 in a larger set of illustrations. The words ""evil & cypress"" are written at the top left corner of the image, but without additional context, their relevance to the image is unclear. The style suggests it may be from an older publication on construction, archaeology, or geology, possibly illustrating a type of masonry or wall construction method involving rough-hewn stones used for the inner layer of a wall.'), Document(metadata={'page_source': '2', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 0, '_id': '6193213492714233aafe28cbd3e1db64', '_collection_name': 'test'}, page_content='To illustrate Prt. Lcbour\'s paper ""On the Breccia Gushes of Durham? \nThe image appears to be a drawing or illustration depicting a stone wall with a section in the middle showing rubble or irregularly shaped stones. The caption text mentions ""FiG. I,"" indicating that this might be Figure 1 in a larger set of illustrations. The words ""evil & cypress"" are written at the top left corner of the image, but without additional context, their relevance to the image is unclear. The style suggests it may be from an older publication on construction, archaeology, or geology, possibly illustrating a type of masonry or wall construction method involving rough-hewn stones used for the inner layer of a wall.'), Document(metadata={'page_source': '10', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 4007, '_id': '0133bd4803814cc9bf998cf328b079ee', '_collection_name': 'test'}, page_content=""How great this action really is may perhaps be best understood when it is stated that in every thousand gallons of Sunderland water there is nearly one pound of lime and magnesia; or, in other words, every thousand gallons of that water pumped up represents a pound weight of rock abstracted.t In the course of 8 year the amount of hard compact Mag- nesian Limestone carried away by the Water Company's works would not fall much short of forty cubic yards. If to this be added the amount of water similarly charged with lime and magnesia, which runs off to the sea from springs, streams, and rivers, the enormous amount of stone annually lost by the Permian series in East Durham can be better imagined than represented by figures.A cubic foot of Magnesian Lime- stone of the less crystalline ra rie ties when saturated holds from  $\\pmb{\\mathfrak{s}}\\mathbf{\\cdot}\\pmb{\\mathfrak{s}}\\mathfrak{$  1b8. to 17 lbs.of water; the crystalline forms hold very little.f This bears out the etatement made"")]"
10019,What is the title of the of the Figure 2?,"[Document(metadata={'page_source': '2', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 703, '_id': '7ae63d8c27dc471f8b08c7ef87a5ffea', '_collection_name': 'test'}, page_content='The image is a diagram displaying ""Breccia Gashes,"" which are formations within a cliff. The top part of these gashes is shown as being eroded or ""denuded,"" while the lower portion is visible within the cliff face. ""Breccia"" refers to a rock composed of broken fragments of minerals or rock cemented together by a fine-grained matrix, which is depicted in the diagram as a pattern of angular fragments, representing the texture of the breccia within these formations.\nDia g rum of Breccia Gash in Cli fr with top denuded of fund Bottom concealed by the beach. \nThe image is a line drawing labeled ""FiG. 3,"" depicting a geological cross-section with features that resemble vertical geological formations or structures. These structures could represent elements like veins or intrusions within layered rock formations. The drawing uses different textures and lines to distinguish between various layers and features, suggesting a focus on geological processes or rock formations.'), Document(metadata={'page_source': '2', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 703, '_id': 'ea756a38564642c2800dfc4958134130', '_collection_name': 'test'}, page_content='The image is a diagram displaying ""Breccia Gashes,"" which are formations within a cliff. The top part of these gashes is shown as being eroded or ""denuded,"" while the lower portion is visible within the cliff face. ""Breccia"" refers to a rock composed of broken fragments of minerals or rock cemented together by a fine-grained matrix, which is depicted in the diagram as a pattern of angular fragments, representing the texture of the breccia within these formations.\nDia g rum of Breccia Gash in Cli fr with top denuded of fund Bottom concealed by the beach. \nThe image is a line drawing labeled ""FiG. 3,"" depicting a geological cross-section with features that resemble vertical geological formations or structures. These structures could represent elements like veins or intrusions within layered rock formations. The drawing uses different textures and lines to distinguish between various layers and features, suggesting a focus on geological processes or rock formations.'), Document(metadata={'page_source': '2', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 1681, '_id': 'a540bf910ba94ca19ea25e0234a929be', '_collection_name': 'test'}, page_content='The image appears to be a geological diagram illustrating a specific formation process. It depicts layers of sedimentary rock with a visible deformation feature, which resembles a fault or crack filled with fragmented material, indicative of a fracture or a fault zone in the rock layers. This may represent the formation process of a particular geological structure, potentially related to a gas formation, although the text accompanying the image contains typographical errors and is difficult to interpret accurately.\nN.B. In.the above sketches the Cementing matter is represented by diagonal shading.'), Document(metadata={'page_source': '2', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 1681, '_id': 'ad284ff5cb594a51915816b2271111d2', '_collection_name': 'test'}, page_content='The image appears to be a geological diagram illustrating a specific formation process. It depicts layers of sedimentary rock with a visible deformation feature, which resembles a fault or crack filled with fragmented material, indicative of a fracture or a fault zone in the rock layers. This may represent the formation process of a particular geological structure, potentially related to a gas formation, although the text accompanying the image contains typographical errors and is difficult to interpret accurately.\nN.B. In.the above sketches the Cementing matter is represented by diagonal shading.'), Document(metadata={'page_source': '2', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 0, '_id': 'afd8cc791c2c498f963bbc9314a5dea7', '_collection_name': 'test'}, page_content='To illustrate Prt. Lcbour\'s paper ""On the Breccia Gushes of Durham? \nThe image appears to be a drawing or illustration depicting a stone wall with a section in the middle showing rubble or irregularly shaped stones. The caption text mentions ""FiG. I,"" indicating that this might be Figure 1 in a larger set of illustrations. The words ""evil & cypress"" are written at the top left corner of the image, but without additional context, their relevance to the image is unclear. The style suggests it may be from an older publication on construction, archaeology, or geology, possibly illustrating a type of masonry or wall construction method involving rough-hewn stones used for the inner layer of a wall.'), Document(metadata={'page_source': '2', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 0, '_id': '6193213492714233aafe28cbd3e1db64', '_collection_name': 'test'}, page_content='To illustrate Prt. Lcbour\'s paper ""On the Breccia Gushes of Durham? \nThe image appears to be a drawing or illustration depicting a stone wall with a section in the middle showing rubble or irregularly shaped stones. The caption text mentions ""FiG. I,"" indicating that this might be Figure 1 in a larger set of illustrations. The words ""evil & cypress"" are written at the top left corner of the image, but without additional context, their relevance to the image is unclear. The style suggests it may be from an older publication on construction, archaeology, or geology, possibly illustrating a type of masonry or wall construction method involving rough-hewn stones used for the inner layer of a wall.'), Document(metadata={'page_source': '4', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 0, '_id': '58d9b81171cb4bc19933d23c18e9f779', '_collection_name': 'test'}, page_content='The image appears to be a sketch or drawing depicting a textured, rocky landscape or a cliffside. It uses a monochromatic palette with detailed shading to create depth and texture.'), Document(metadata={'page_source': '4', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 0, '_id': 'fffb2f0c7c0648aeb80bab75221d44dc', '_collection_name': 'test'}, page_content='The image appears to be a sketch or drawing depicting a textured, rocky landscape or a cliffside. It uses a monochromatic palette with detailed shading to create depth and texture.'), Document(metadata={'page_source': '0', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 969, '_id': '33362d23e72948f5aafc5a675879ec4f', '_collection_name': 'test'}, page_content='in structure, and so striking by reason of the variety of their forms, that they have been described in many raluable papers, by the late Professor Sedgwick, Mr. R. Howse, Mr. J. W. Kirkby, and others.\\* It is not intended in the present paper to repeat what has been so well and so often·said before, but simply to draw special atten- tion to one of the strangest and most striking of the developments of the Magnesian Iimestone as displayed in Marsden Bay.'), Document(metadata={'page_source': '0', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 969, '_id': '29830190cfc74c9eaf68be00844f4755', '_collection_name': 'test'}, page_content='in structure, and so striking by reason of the variety of their forms, that they have been described in many raluable papers, by the late Professor Sedgwick, Mr. R. Howse, Mr. J. W. Kirkby, and others.\\* It is not intended in the present paper to repeat what has been so well and so often·said before, but simply to draw special atten- tion to one of the strangest and most striking of the developments of the Magnesian Iimestone as displayed in Marsden Bay.'), Document(metadata={'page_source': '1', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 1901, '_id': '25691d9d50f44364a9d2af8cd16f433a', '_collection_name': 'test'}, page_content='The varieties of breccia-gashes\\*enumerated above are illustrated by diagrammatic sketches in Plate XII., Figs..1, 2, 3, and 4, whilst the nature of the breccia itself is shown in Plate XI lI.,which has been drawn from B photograph of one of the largest gashes near the north end of Marsden Bay, kindly taken for the writer by Mr. W. G. Laws, jun., A.Sc. \n\\* The word gash is a convenient one used oca sion ally by lead-miners to express a fissure unaccompanied by dislocation. See N. Winch\'s “ Geology of Northumberland and Durham,"" Trans. Geol. Soc., Vol. IV., p. 30, (1816).'), Document(metadata={'page_source': '1', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 1901, '_id': '532dcea68b854fff8ac61ee1445563c0', '_collection_name': 'test'}, page_content='The varieties of breccia-gashes\\*enumerated above are illustrated by diagrammatic sketches in Plate XII., Figs..1, 2, 3, and 4, whilst the nature of the breccia itself is shown in Plate XI lI.,which has been drawn from B photograph of one of the largest gashes near the north end of Marsden Bay, kindly taken for the writer by Mr. W. G. Laws, jun., A.Sc. \n\\* The word gash is a convenient one used oca sion ally by lead-miners to express a fissure unaccompanied by dislocation. See N. Winch\'s “ Geology of Northumberland and Durham,"" Trans. Geol. Soc., Vol. IV., p. 30, (1816).'), Document(metadata={'page_source': '6', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 0, '_id': '5dad70ec92fd43739af40439e49071c2', '_collection_name': 'test'}, page_content='The fragments constituting thebreccia are of all shapes and sizes. from blocks 8 yard or more in diameter to the smallest grains,butallare angular. \n11.'), Document(metadata={'page_source': '6', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 0, '_id': '80014ca8518d4a888307fb6002f39b9c', '_collection_name': 'test'}, page_content='The fragments constituting thebreccia are of all shapes and sizes. from blocks 8 yard or more in diameter to the smallest grains,butallare angular. \n11.'), Document(metadata={'page_source': '12', 'doc_source': 'fd76bbefe469561966e5387aa709c482', 'start_index': 696, '_id': 'c9e4b6b6afce4a5d9c8b92b971d87027', '_collection_name': 'test'}, page_content='Here then are the conditions to which it is desired that attention should be directed:——A mass of stone, mostly hard and compact, but cellular in places and earthy and friable in others; often cavernous on a large scale; full of water, and through its action continually parting with its substance, and thus enlarging the cavities within it.')]"
